{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21596,"status":"ok","timestamp":1707935766687,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"IqglcpHIMctv","outputId":"ff5e9501-3219-459d-e958-01ac542b338d"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3695,"status":"ok","timestamp":1707935770378,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"FQXHpgZnNNTM"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","# Define the file paths\n","\n","file_path3 = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_training_data.csv'\n","file_path4  = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_testing_data.csv'\n","# Use pandas to read the CSV files and then convert them to NumPy arrays\n","JPL_train = pd.read_csv(file_path3).values\n","JPL_test=pd.read_csv(file_path4).values"]},{"cell_type":"markdown","metadata":{"id":"fDhHTxx6S2sq"},"source":["#Training phase\n","-Create blue matrix \\\\\n","-SVD decomposition -> find orange matrix (non-stationary part) \\\\\n","-Subtract blue from orange -> get green matrix -> stationary part/residual \\\\\n","-Calculate beta_hat (will be used in testing phase for out of sample forecasting of \"SAMoSSA\" part"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":513,"status":"ok","timestamp":1707935771883,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"ro286i0SNO8n"},"outputs":[],"source":["#Remove row number (in 1st column)\n","JPL_train=JPL_train[:,1:]\n","JPL_test=JPL_test[:,1:]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1707935771883,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"az3HhS6aOOBD"},"outputs":[],"source":["import pandas as pd\n","\n","def process_dataframe(df):\n","    # Select only columns 0 and 3\n","    df = pd.DataFrame(df)\n","    selected_df = df.iloc[:, [0, 3]]\n","\n","    # Splitting the date and time in column 0\n","    df_split = selected_df[0].str.split(' ', expand=True)\n","\n","    # Renaming the columns for clarity\n","    df_split.columns = ['Date', 'Time']\n","\n","    # Including the second column from the original data\n","    processed_df = pd.concat([df_split, selected_df.iloc[:, 1]], axis=1)\n","\n","    # Rename the UserID column for clarity\n","    processed_df.rename(columns={3: 'UserID'}, inplace=True)\n","\n","    return processed_df\n","\n","# Usage example\n","JPL_train = process_dataframe(JPL_train)\n","JPL_test = process_dataframe(JPL_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1707935771883,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"EgRzfPzGOQaD"},"outputs":[],"source":["def convert_time_to_decimal(time_str):\n","    # Splitting the time into hours, minutes, and seconds\n","    hours, minutes, seconds = map(int, time_str.split(':'))\n","\n","    # Converting time to decimal format\n","    decimal_hours = hours + minutes / 60 + seconds / 3600\n","\n","    return decimal_hours\n","\n","# Applying the conversion to the 'Time' column\n","JPL_train['Time'] = JPL_train['Time'].apply(convert_time_to_decimal)\n","JPL_test['Time'] = JPL_test['Time'].apply(convert_time_to_decimal)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1707935771883,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"YdIXQ6DPOTtt"},"outputs":[],"source":["import pandas as pd\n","\n","JPL_train = pd.DataFrame(JPL_train, columns=['Date', 'Time', 'UserID'])\n","JPL_train['Date'] = pd.to_datetime(JPL_train['Date'])\n","sorted_JPL_train = JPL_train.sort_values(by=['UserID', 'Date', 'Time'])\n","\n","\n","JPL_test = pd.DataFrame(JPL_test, columns=['Date', 'Time', 'UserID'])\n","JPL_test['Date'] = pd.to_datetime(JPL_test['Date'])\n","sorted_JPL_test = JPL_test.sort_values(by=['UserID', 'Date', 'Time'])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1707935771884,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"_zerE1D8OWNg"},"outputs":[],"source":["#Keep only one session (earliest) per day for each user\n","# Assuming your DataFrame is named df and is structured as shown\n","sorted_JPL_train['Date'] = pd.to_datetime(sorted_JPL_train['Date'])\n","\n","# Group by 'UserID' and 'Date', then find the index of the earliest 'Time' for each group\n","idx = sorted_JPL_train.groupby(['UserID', 'Date'])['Time'].idxmin()\n","\n","# Use these indices to filter the original DataFrame\n","sorted_JPL_train = sorted_JPL_train.loc[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1707935771884,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"nvw5FVONOYys"},"outputs":[],"source":["#Keep only one session (earliest) per day for each user\n","# Assuming your DataFrame is named df and is structured as shown\n","sorted_JPL_test['Date'] = pd.to_datetime(sorted_JPL_test['Date'])\n","\n","# Group by 'UserID' and 'Date', then find the index of the earliest 'Time' for each group\n","idx = sorted_JPL_test.groupby(['UserID', 'Date'])['Time'].idxmin()\n","\n","# Use these indices to filter the original DataFrame\n","sorted_JPL_test = sorted_JPL_test.loc[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1707935771884,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"VdJwxABQNpV5"},"outputs":[],"source":["# Find common UserIDs\n","common_user_ids = set(sorted_JPL_train['UserID']).intersection(set(sorted_JPL_test['UserID']))\n","# # Filter both datasets to include only common UserIDs\n","sorted_JPL_train = sorted_JPL_train[sorted_JPL_train['UserID'].isin(common_user_ids)]\n","sorted_JPL_test = sorted_JPL_test[sorted_JPL_test['UserID'].isin(common_user_ids)]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":599,"status":"ok","timestamp":1707935772479,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"ImbsCoohPHLR","outputId":"b4f08f3f-95ba-41db-bf1b-6b4d0dec4245"},"outputs":[],"source":["sorted_JPL_train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1707935772479,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"BTeyPbhpPwp6","outputId":"f5bc3a8a-e78e-4933-f4b3-1512fd30bb99"},"outputs":[],"source":["user_counts_train = sorted_JPL_train['UserID'].value_counts()\n","min_sessions_train=min(user_counts_train)\n","min_sessions_train"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1707935772480,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"iR1onXBIO9Pk"},"outputs":[],"source":["# Selecting only the last 20 rows for each user ID\n","\n","last_19_sessions = sorted_JPL_train.groupby('UserID').apply(lambda x: x.tail(19))\n","\n","# Resetting the index\n","last_19_sessions = last_19_sessions.reset_index(drop=True)\n","last_19_sessions=last_19_sessions[['Time', 'UserID']]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1707935772480,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"QX1C4ETUPJch","outputId":"ebe125a4-4e42-4c0b-94d6-bee1f7c8401b"},"outputs":[],"source":["last_19_sessions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1707935772824,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"fNZnknp-SKyV","outputId":"91d6c920-2f20-41e8-b2b3-87741f6c3ac5"},"outputs":[],"source":["arrival_times_per_user = last_19_sessions.groupby('UserID')['Time'].apply(list)\n","arrival_times_per_user"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707935776787,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"AdDEPbzvSXb3"},"outputs":[],"source":["def list_to_matrix_columnwise_corrected(lst, rows=19, columns=1):\n","    # Initialize a matrix of zeros\n","    matrix = np.zeros((rows, columns))\n","\n","    # Fill the matrix column-wise\n","    for i, val in enumerate(lst):\n","        row = i % rows\n","        col = i // rows\n","        if col < columns:\n","            matrix[row, col] = val\n","\n","    return matrix\n","\n","page_matrices = {user_id: list_to_matrix_columnwise_corrected(times) for user_id, times in arrival_times_per_user.items()}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1707935777793,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"Oof9yeywSlhF","outputId":"4b78f9ff-86f3-4be6-e7dd-eb9b07c6a69c"},"outputs":[],"source":["matrices = list(page_matrices.values())\n","\n","# Stacking the matrices horizontally\n","stacked_page_matrix = np.hstack(matrices)\n","stacked_page_matrix.shape ##blue matrix created"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13580,"status":"ok","timestamp":1707935797411,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"l57Vaq5EnhWX","outputId":"5a16399c-a2bc-458c-b21f-927a7dc0af69"},"outputs":[],"source":["!pip install git+https://github.com/ShunChi100/RobustPCA\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8372,"status":"ok","timestamp":1707935805773,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"qE8y5zfgn0h-","outputId":"597b7567-af59-4436-b5b2-5c6ac4a58a54"},"outputs":[],"source":["!pip install fbpca"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":323,"status":"ok","timestamp":1707939436524,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"TXNZOx10CWyz"},"outputs":[],"source":["import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","\n","# Standardize the data (mean=0, variance=1)\n","scaler = StandardScaler()\n","data_standardized = scaler.fit_transform(stacked_page_matrix)  # Transpose to standardize across users, not time points\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":321,"status":"ok","timestamp":1707939440958,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"ZUvcQR3cnsrn","outputId":"93fcb6c6-7715-46bf-cf05-122584db07cc"},"outputs":[],"source":["from RobustPCA.rpca import RobustPCA\n","\n","rpca = RobustPCA(max_iter=2100)\n","\n","rpca.fit(data_standardized)\n","L = rpca.get_low_rank()\n","S = rpca.get_sparse()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":404,"status":"ok","timestamp":1707939444940,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"DhNB3t-WCkyp"},"outputs":[],"source":["L_inverse_scaled = scaler.inverse_transform(L)"]},{"cell_type":"markdown","metadata":{"id":"S9FfDiL_V2Ht"},"source":["##SVD decomposition\n","-Use a hard margin of k=5 (same as the one used in the paper)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1244,"status":"ok","timestamp":1707935920592,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"wZCi9uNY7BeY"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","import numpy as np\n","\n","# Assuming 'stacked_page_matrix' is your data matrix\n","\n","# Step 1: PCA automatically centers the data, so you don't need to manually subtract the mean\n","\n","# Step 2: Perform PCA with the desired number of components\n","num_principal_components = 5  # This is akin to your num_singular_values\n","pca = PCA(n_components=num_principal_components)\n","\n","# Fit PCA to the data and transform the data onto the principal components\n","pca.fit(L)\n","transformed_data = pca.transform(L)\n","\n","# Step 3: Reconstruct the data from the principal components\n","non_stationary_component = pca.inverse_transform(transformed_data)\n","\n","# The 'reconstructed_data' matrix now acts as your non-stationary component\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"elapsed":921,"status":"ok","timestamp":1707935924892,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"SbTSjigGHbfy","outputId":"40607c04-538c-4039-b6e9-9ab8f5660859"},"outputs":[],"source":["pca = PCA().fit(L)\n","\n","#% matplotlib inline\n","import matplotlib.pyplot as plt\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","fig, ax = plt.subplots()\n","xi = np.arange(1, 20, step=1)\n","y = np.cumsum(pca.explained_variance_ratio_)\n","\n","plt.ylim(0.0,1.1)\n","plt.plot(xi, y, marker='o', linestyle='--', color='b')\n","\n","plt.xlabel('Number of Components')\n","plt.xticks(np.arange(0, 20, step=1)) #change from 0-based array index to 1-based human-readable label\n","plt.ylabel('Cumulative variance (%)')\n","plt.title('The number of components needed to explain variance')\n","\n","plt.axhline(y=0.99, color='r', linestyle='-')\n","plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n","\n","ax.grid(axis='x')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":420,"status":"ok","timestamp":1707935930712,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"JaZUqwO_Mucv","outputId":"9398ba57-c276-44c3-9156-b8c9366b5702"},"outputs":[],"source":["np.cumsum(pca.explained_variance_ratio_)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":340,"status":"ok","timestamp":1707939462243,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"uIINSa2nVwX0"},"outputs":[],"source":["from scipy.linalg import svd\n","\n","# The singular values are in the vector 's'\n","# U and VT are the left and right singular vectors, respectively\n","U, s, VT = svd(L_inverse_scaled)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":428,"status":"ok","timestamp":1707938371570,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"WtAThjQSDI2z","outputId":"a3f6b9ed-1505-4bac-93f8-11c427e54787"},"outputs":[],"source":["L_inverse_scaled.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":619,"status":"ok","timestamp":1707939476282,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"_QGqUQa5V00p"},"outputs":[],"source":["num_singular_values = 5\n","s_reduced = np.zeros(L_inverse_scaled.shape)\n","np.fill_diagonal(s_reduced, s[:num_singular_values])\n","\n","non_stationary_component = U @ s_reduced @ VT ##orange matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1707939478336,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"MZ5pq3zuDL2V","outputId":"7e47995d-e47d-49d6-9708-c620d956f265"},"outputs":[],"source":["non_stationary_component.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1707939483008,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"qkafvFu7a8DV","outputId":"3751d5c0-ff87-4110-db52-f7df2111b82b"},"outputs":[],"source":["F_hat_mat = non_stationary_component[:18, :] #L-1 rows of orange matrix\n","y_vector = stacked_page_matrix[-1, :] #last row of blue matrix\n","\n","import numpy as np\n","\n","Y = y_vector.reshape(-1, 1)  # Reshape Y to be a column vector if it's a 1D array\n","\n","# Solve for beta_hat using the least squares method\n","beta_hat, residuals, rank, s = np.linalg.lstsq(F_hat_mat.T, Y, rcond=None)\n","\n","# # beta_hat now contains the estimated beta parameters, should be 18x1 dimensions\n","beta_hat"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707939486211,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"EOa-ZrnZbazT"},"outputs":[],"source":["array=beta_hat\n","desired_length = 19\n","last_element = array[-1, :]\n","while len(array) < desired_length:\n","    array = np.vstack([array, last_element])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707939487444,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"q2xXO3KGWFlV"},"outputs":[],"source":["residual=stacked_page_matrix-array.T@non_stationary_component ##green matrix\n"]},{"cell_type":"markdown","metadata":{"id":"ElvqvhDPWuD4"},"source":["##Train LSTM with residual (one LSTM model per user)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707939488564,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"z-1IkfdKWM8t"},"outputs":[],"source":["userIDs = list(arrival_times_per_user.keys())\n","residuals_dict = {userID: residual[:, i] for i, userID in enumerate(userIDs)}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1707939489662,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"rNBFzGH7V64a","outputId":"89847b27-d2e3-4d76-a852-9d1ad8aac16b"},"outputs":[],"source":["residuals_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":718,"status":"ok","timestamp":1707939494304,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"bPYJs8Jy7N5l"},"outputs":[],"source":["normalized_residuals_dict = {}\n","means_dict = {}\n","stds_dict = {}\n","\n","for key, values in residuals_dict.items():\n","    # Calculate mean and standard deviation\n","    mean = values.mean()\n","    std = values.std()\n","\n","    # Normalize values\n","    normalized_values = (values - mean) / std\n","\n","    # Store normalized values and statistics\n","    normalized_residuals_dict[key] = normalized_values\n","    means_dict[key] = mean\n","    stds_dict[key] = std"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":800816,"status":"ok","timestamp":1707940298445,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"r3IE6qNRYl5x","outputId":"f503c191-3c60-4c26-beac-087904f312d8"},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import LSTM, Dense, Dropout\n","import numpy as np\n","from keras import backend as K\n","\n","def smape_loss(y_true, y_pred):\n","    denominator = K.maximum(K.abs(y_true) + K.abs(y_pred), K.epsilon())\n","    diff = K.abs(y_pred - y_true) / denominator\n","    return 100 * K.mean(diff, axis=-1)\n","\n","def create_lstm_model(input_shape):\n","    model = Sequential()\n","    model.add(LSTM(units=32, return_sequences=True, input_shape=input_shape))\n","    model.add(Dropout(0.2))  # Dropout layer after the first LSTM layer\n","    model.add(LSTM(units=16))  # Second LSTM layer with 32 units\n","    model.add(Dropout(0.2))  # Dropout layer after the second LSTM layer\n","    model.add(Dense(units=1))\n","    model.compile(optimizer='adam', loss=smape_loss)\n","    return model\n","\n","def prepare_data(residuals, n_steps):\n","    X, y = [], []\n","    for i in range(len(residuals)):\n","        end_ix = i + n_steps\n","        if end_ix > len(residuals)-1:\n","            break\n","        seq_x, seq_y = residuals[i:end_ix], residuals[end_ix]\n","        X.append(seq_x)\n","        y.append(seq_y)\n","    return np.array(X), np.array(y)\n","\n","user_ids = normalized_residuals_dict.keys()\n","n_steps = 1  # Number of time steps for LSTM. Adjust as needed.\n","\n","lstm_models = {}\n","\n","for user_id in user_ids:\n","    residuals = normalized_residuals_dict[user_id]\n","\n","    # Check if the user has sufficient data\n","    if len(residuals) > n_steps:\n","        X, y = prepare_data(residuals, n_steps)\n","        if X.size > 0 and y.size > 0:\n","            X = X.reshape((X.shape[0], X.shape[1], 1))\n","            lstm_model = create_lstm_model((X.shape[1], 1))\n","            lstm_model.fit(X, y, epochs=100, batch_size=1)\n","            lstm_models[user_id] = lstm_model\n","        else:\n","            print(f\"Insufficient data for user {user_id}\")\n","    else:\n","        print(f\"Not enough data points for user {user_id} for n_steps = {n_steps}\")\n"]},{"cell_type":"markdown","metadata":{"id":"RZzXWInOcFQD"},"source":["#Inference phase"]},{"cell_type":"markdown","metadata":{"id":"Vr3sF1IicIG9"},"source":["##Estimate b_hat (parameters of non-stationary component)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":344,"status":"ok","timestamp":1707940304014,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"7pI2mh35cLLL","outputId":"9d9dc8a8-b506-48a9-8db4-f8b3c955fc35"},"outputs":[],"source":["F_hat_mat = non_stationary_component[:18, :] #L-1 rows of orange matrix\n","y_vector = stacked_page_matrix[-1, :] #last row of blue matrix\n","\n","import numpy as np\n","\n","Y = y_vector.reshape(-1, 1)  # Reshape Y to be a column vector if it's a 1D array\n","\n","# Solve for beta_hat using the least squares method\n","beta_hat, residuals, rank, s = np.linalg.lstsq(F_hat_mat.T, Y, rcond=None)\n","\n","# # beta_hat now contains the estimated beta parameters, should be 18x1 dimensions\n","beta_hat"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":402,"status":"ok","timestamp":1707940307477,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"XPLQ5hDAc0vY","outputId":"971d4783-cf90-49a7-9454-29021a15e01b"},"outputs":[],"source":["sorted_JPL_test"]},{"cell_type":"markdown","metadata":{"id":"MO8SEh3lui0g"},"source":["##Prediction of non-stationary part (using SAMoSSA method)\n","-Use method outlined in part V of figure (to predict value at time interval t use previous L points)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":429,"status":"ok","timestamp":1707940313224,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"-qUoCHoWzo_6","outputId":"69078883-edfb-42b5-aa0d-c1f95a4d081e"},"outputs":[],"source":["import pandas as pd\n","\n","# Assuming last_19_sessions is a pandas DataFrame\n","# last_19_sessions = pd.read_csv('your_dataset.csv')  # or however you load your dataset\n","\n","# Initialize an empty DataFrame to store the first 18 sessions for each user\n","last_18_sessions_per_user = pd.DataFrame()\n","\n","# Group the data by userID and then take the first 18 rows for each group\n","for user_id, group in last_19_sessions.groupby('UserID'):\n","    last_18_sessions = group.tail(18)\n","    last_18_sessions_per_user = pd.concat([last_18_sessions_per_user, last_18_sessions])\n","\n","# Reset index of the new DataFrame\n","last_18_sessions_per_user.reset_index(drop=True, inplace=True)\n","last_18_sessions_per_user"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":755,"status":"ok","timestamp":1707940329810,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"dR26VP69wX8a"},"outputs":[],"source":["import pandas as pd\n","\n","# Initialize an empty dictionary to store time values for each user\n","user_times = {}\n","\n","# Process the train dataset\n","for index, row in last_18_sessions_per_user.iterrows():\n","    user_id = row['UserID']\n","    time = row['Time']\n","    if user_id not in user_times:\n","        user_times[user_id] = []\n","    user_times[user_id].append(time)\n","\n","# Process the test dataset\n","for index, row in sorted_JPL_test.iterrows():\n","    user_id = row['UserID']\n","    time = row['Time']\n","    if user_id not in user_times:\n","        user_times[user_id] = []\n","    user_times[user_id].append(time)\n","#Create a dictionary where for each user store the last 18 values of the train dataset and the values\n","#of the test dataset for each user"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707940331562,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"giKI_s-X11Vt"},"outputs":[],"source":["import numpy as np\n","\n","# Initialize a dictionary to store predictions for each user\n","user_predictions_non_stationary = {}\n","\n","for user_id, times in user_times.items():\n","    # We can only make a prediction if there are at least 18 values\n","    if len(times) >= 18:\n","        predictions = []\n","        # Slide the window and predict\n","        for i in range(len(times) - 18):\n","            window = np.array(times[i:i+18])\n","            prediction = np.dot(window, beta_hat).item()\n","            predictions.append(prediction)\n","        user_predictions_non_stationary[user_id] = predictions\n","\n","# user_predictions now contains the predicted values for each user\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707940334276,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"324iQusM3-yk"},"outputs":[],"source":["# Group by 'userID' and aggregate the 'time' values into lists\n","user_actual_time = sorted_JPL_test.groupby('UserID')['Time'].apply(list).to_dict()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1707940340775,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"Hsnlk6RkLVzi","outputId":"906a9f94-957f-499c-efe7-35f2e47eb9cf"},"outputs":[],"source":["user_actual_time"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":355,"status":"ok","timestamp":1707940369183,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"d5_enra64huA"},"outputs":[],"source":["# Assuming user_actual_time and user_predictions_non_stationary are dictionaries with lists as values\n","residual_test = {key: [a - b for a, b in zip(user_actual_time[key], user_predictions_non_stationary[key])]\n","                 for key in user_actual_time\n","                 if key in user_predictions_non_stationary}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1707940371934,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"O1QLUbyBaize","outputId":"d3379e5e-39de-4283-f647-8402c31022b2"},"outputs":[],"source":["residual_test"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":495,"status":"ok","timestamp":1707940401370,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"cqJadPFV-8B7"},"outputs":[],"source":["# Adjusted dictionary to store the results\n","adjusted_residual_test = {}\n","\n","for key, values in residual_test.items():\n","    # Retrieve the mean and standard deviation for the current key\n","    mean = means_dict[key]\n","    std = stds_dict[key]\n","\n","    # Adjust the values by subtracting the mean and dividing by the standard deviation\n","    adjusted_values = (values - mean) / std\n","\n","    # Store the adjusted values\n","    adjusted_residual_test[key] = adjusted_values"]},{"cell_type":"markdown","metadata":{"id":"aq7c7gsR6Xt3"},"source":["##Make predictions using LSTM on stationary/residual part"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34738,"status":"ok","timestamp":1707940438655,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"XRORsiCv_FmM","outputId":"50fe665e-fca6-43ed-a080-282b2e8d83c5"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","def prepare_lstm_input(residuals, n_steps):\n","    X = []\n","    for i in range(len(residuals) - n_steps + 1):\n","        X.append(residuals[i:i + n_steps])\n","    return np.array(X)\n","\n","def reverse_normalize_data(normalized_data, mean, std):\n","    return (normalized_data * std) + mean\n","\n","# Create a DataFrame for storing final predictions\n","final_predictions_df = pd.DataFrame(columns=['UserID', 'Final_Prediction'])\n","\n","# Iterate over each user and their residuals\n","for user_id, residuals in adjusted_residual_test.items():\n","    if user_id in lstm_models:\n","        mean = means_dict[user_id]\n","        std = stds_dict[user_id]\n","        # Prepare LSTM input from residuals\n","        if len(residuals) >= n_steps:\n","            lstm_input = prepare_lstm_input(residuals, n_steps)\n","            lstm_input = lstm_input.reshape((-1, n_steps, 1))  # Reshape for LSTM\n","\n","            # Make prediction with LSTM\n","            lstm_pred_normalized = lstm_models[user_id].predict(lstm_input)\n","\n","            # Reverse normalization on LSTM predictions to bring them back to original scale\n","            lstm_pred = reverse_normalize_data(lstm_pred_normalized, mean, std)\n","\n","            # Retrieve the corresponding non-stationary model predictions\n","            non_stationary_pred = user_predictions_non_stationary[user_id]\n","\n","            # Add LSTM predictions to non-stationary model predictions\n","            # Assuming non_stationary_pred is aligned with the last lstm_pred\n","            #combined_pred = non_stationary_pred[-len(lstm_pred):] + (beta_hat[-len(lstm_pred):].T@lstm_pred).flatten()\n","            combined_pred = non_stationary_pred[-len(lstm_pred):] + lstm_pred.flatten()\n","\n","        else:\n","            combined_pred = residuals\n","    else:\n","        # If no LSTM model, use non-stationary model predictions as is\n","        combined_pred = user_predictions_non_stationary[user_id]\n","\n","    # Add combined predictions to the DataFrame\n","    final_predictions_df = final_predictions_df.append(pd.DataFrame({\n","        'UserID': user_id,\n","        'Final_Prediction': combined_pred\n","    }), ignore_index=True)\n","\n","# final_predictions_df now contains combined predictions for each user\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3864,"status":"ok","timestamp":1707932219870,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"yY5dBHHz6gRp","outputId":"c01046f8-4d45-486e-d955-4be44320394a"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","def prepare_lstm_input(residuals, n_steps):\n","    X = []\n","    for i in range(len(residuals) - n_steps + 1):\n","        X.append(residuals[i:i + n_steps])\n","    return np.array(X)\n","\n","# Create a DataFrame for storing final predictions\n","final_predictions_df = pd.DataFrame(columns=['UserID', 'Final_Prediction'])\n","\n","# Iterate over each user and their residuals\n","for user_id, residuals in residual_test.items():\n","    if user_id in lstm_models:\n","        # Prepare LSTM input from residuals\n","        if len(residuals) >= n_steps:\n","            lstm_input = prepare_lstm_input(residuals, n_steps)\n","            lstm_input = lstm_input.reshape((-1, n_steps, 1))  # Reshape for LSTM\n","\n","            # Make prediction with LSTM\n","            lstm_pred = lstm_models[user_id].predict(lstm_input)\n","            # Retrieve the corresponding non-stationary model predictions\n","            non_stationary_pred = user_predictions_non_stationary[user_id]\n","\n","            # Add LSTM predictions to non-stationary model predictions\n","            # Assuming non_stationary_pred is aligned with the last lstm_pred\n","            #combined_pred = non_stationary_pred[-len(lstm_pred):] + (beta_hat[-len(lstm_pred):].T@lstm_pred).flatten()\n","            combined_pred = non_stationary_pred[-len(lstm_pred):] + lstm_pred.flatten()\n","\n","        else:\n","            combined_pred = residuals\n","    else:\n","        # If no LSTM model, use non-stationary model predictions as is\n","        combined_pred = user_predictions_non_stationary[user_id]\n","\n","    # Add combined predictions to the DataFrame\n","    final_predictions_df = final_predictions_df.append(pd.DataFrame({\n","        'UserID': user_id,\n","        'Final_Prediction': combined_pred\n","    }), ignore_index=True)\n","\n","# final_predictions_df now contains combined predictions for each user\n"]},{"cell_type":"markdown","metadata":{"id":"EXJBTz359xrB"},"source":["#SMAPE of hybrid model (Rank Decomposition+LSTM hybrid)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":592,"status":"ok","timestamp":1707940441074,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"d8m8ahvV9xLH"},"outputs":[],"source":["df = pd.DataFrame(sorted_JPL_test)\n","\n","# Convert DataFrame to dictionary with UserID as key and Time values as list\n","true_values = df.groupby('UserID')['Time'].apply(list).to_dict()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707940441685,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"dBbeEB5m9zgi"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","def calculate_smape(actual, predicted):\n","    \"\"\"Calculate SMAPE between two series.\"\"\"\n","    denominator = (np.abs(actual) + np.abs(predicted))\n","    diff = np.abs(actual - predicted) / denominator\n","    diff[denominator == 0] = 0.0  # handle division by zero\n","    return 100 * np.mean(diff)\n","\n","# Dictionary to store SMAPE for each user\n","smape_values_non_stationary = {}\n","\n","# Iterate over each user\n","for user in user_predictions_non_stationary:\n","    # Retrieve the predicted values for the user and convert to a Pandas Series if not already\n","    predicted = pd.Series(user_predictions_non_stationary[user])\n","\n","    # Retrieve the true values for the user and convert to a Pandas Series\n","    actual = pd.Series(true_values[user],index=predicted.index)\n","\n","    # Calculate SMAPE\n","    smape = calculate_smape(actual, predicted)\n","\n","    # Store the SMAPE value\n","    smape_values_non_stationary[user] = smape\n","\n","# smape_values dictionary now contains the SMAPE for each user\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":517,"status":"ok","timestamp":1707940445477,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"5FIyBeQ39_3Y"},"outputs":[],"source":["final_predictions_df = final_predictions_df.reset_index(drop=True)\n","sorted_JPL_test = sorted_JPL_test.reset_index(drop=True)\n","combined_df = pd.concat([final_predictions_df, sorted_JPL_test], axis=1)\n","combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":601,"status":"ok","timestamp":1707940447227,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"D5TTFcB1-CiR"},"outputs":[],"source":["import pandas as pd\n","df = pd.DataFrame(combined_df)\n","\n","# Function to calculate SMAPE\n","def calculate_smape(df):\n","    def smape(y_true, y_pred):\n","        denominator = (abs(y_true) + abs(y_pred))\n","        diff = abs(y_true - y_pred) / denominator\n","        return 100 * diff.mean()\n","\n","    smape_values = df.groupby('UserID').apply(lambda x: smape(x['Time'], x['Final_Prediction']))\n","    return smape_values\n","\n","smape_results_combined = calculate_smape(df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1707940448226,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"T7E0XVac-LzB","outputId":"dfab2f80-e122-4fce-ed9a-a6ceb5ce5477"},"outputs":[],"source":["# Assuming smape_results1 and smape_results2 are the SMAPE results from two different DataFrames\n","final_smape_results = pd.DataFrame({\n","    'SMAPE1': smape_values_non_stationary,\n","    'SMAPE2': smape_results_combined\n","})\n","\n","# Select the better SMAPE for each UserID\n","final_smape_results['Best_SMAPE'] = final_smape_results.min(axis=1)\n","\n","# Display the final SMAPE results\n","print(final_smape_results)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":427,"status":"ok","timestamp":1707940453678,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"2NkAB2yM-Y8v","outputId":"29aaf759-e4c0-49d1-c93e-674ddc8a2c79"},"outputs":[],"source":["mean_best_smape = final_smape_results['Best_SMAPE'].mean()\n","print(\"SMAPE for hybrid SAMoSSA and LSTM model (%):\", mean_best_smape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707940455586,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"Vk-too4u_1Gr","outputId":"53e42acb-d80f-42ce-f251-91371790045d"},"outputs":[],"source":["mean_best_smape = final_smape_results['SMAPE2'].mean()\n","print(\"SMAPE for hybrid SAMoSSA and LSTM model (%):\", mean_best_smape)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNQ/1+Qt+aEk2mIl4gcyZxG","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
