{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5918,"status":"ok","timestamp":1707960545673,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"IqglcpHIMctv","outputId":"5d3b6973-0539-45c2-ac00-bcd7532e379f"},"outputs":[],"source":["!git clone https://github.com/AbdullahO/SAMoSSA.git\n","import numpy as np\n","dataset = np.load('/content/SAMoSSA/datasets/electricity/electricity.npy', encoding='bytes')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1707960545673,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"FQXHpgZnNNTM"},"outputs":[],"source":["training_set = dataset[25800:25848]      # Arrays 1 to 25824\n","validation_set = dataset[25848:25872] # Arrays 25825 to 25872"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1707960545673,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"4nrB52O55d7K"},"outputs":[],"source":["sampled_user_ids=[ 58,  53,  84, 274, 164, 365, 340, 225, 281,  48,  42, 298, 334,\n","        63,   3, 229, 262, 104,  64,  27, 133,  61, 245,   2,  67, 337,\n","       127, 248, 218, 217, 317, 280, 243,  76, 219, 250, 305,  75, 350,\n","        49,  95, 224, 162, 367,  73, 161, 238, 324,  29, 154]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8954,"status":"ok","timestamp":1707960554623,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"pw0GCaaC18HT","outputId":"08518b6f-ff6f-42ca-b6d5-190a7b53d865"},"outputs":[],"source":["!pip install tensorly"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":533,"status":"ok","timestamp":1707960555152,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"HHstdKnt1wrI"},"outputs":[],"source":["from tensorly.decomposition import robust_pca"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nvI7pSAi2Zuq"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"fDhHTxx6S2sq"},"source":["#Training phase\n","-Create blue matrix \\\\\n","-SVD decomposition -> find orange matrix (non-stationary part) \\\\\n","-Subtract blue from orange -> get green matrix -> stationary part/residual \\\\\n","-Calculate beta_hat (will be used in testing phase for out of sample forecasting of \"SAMoSSA\" part"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1583,"status":"ok","timestamp":1707960559330,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"BqXXXMta3pgY"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","number_of_hours, num_users = training_set.shape\n","\n","# Generate date range\n","date_range = pd.date_range(start='01/01/2011 00:00', periods=number_of_hours, freq='H')\n","\n","# Reshape and create pairs of values and user IDs\n","data = []\n","for user_id in range(1, num_users + 1):\n","    for hour, value in enumerate(training_set[:, user_id - 1]):\n","        data.append([date_range[hour], value, user_id])\n","\n","# Create DataFrame\n","df_train = pd.DataFrame(data, columns=['Date', 'Load', 'UserID'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":442,"status":"ok","timestamp":1707960559768,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"xL3CzmBR3vN2"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","number_of_hours, num_users = validation_set.shape\n","\n","# Generate date range\n","date_range = pd.date_range(start='17/01/2011 00:00', periods=number_of_hours, freq='H')\n","\n","# Reshape and create pairs of values and user IDs\n","data = []\n","for user_id in range(1, num_users + 1):\n","    for hour, value in enumerate(validation_set[:, user_id - 1]):\n","        data.append([date_range[hour], value, user_id])\n","\n","# Create DataFrame\n","df_valid = pd.DataFrame(data, columns=['Date', 'Load', 'UserID'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707960559769,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"PaO3bF2N5fz3"},"outputs":[],"source":["# Filter the original DataFrame to include only the sampled user IDs\n","df_train = df_train[df_train['UserID'].isin(sampled_user_ids)]\n","df_valid = df_valid[df_valid['UserID'].isin(sampled_user_ids)]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1707960560194,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"BTeyPbhpPwp6"},"outputs":[],"source":["# Resetting the index\n","df_train = df_train.reset_index(drop=True)\n","df_train=df_train[['Load', 'UserID']]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":156,"status":"ok","timestamp":1707960560838,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"fNZnknp-SKyV","outputId":"f4c2d928-7aef-4441-d844-dca97d35300b"},"outputs":[],"source":["arrival_times_per_user = df_train.groupby('UserID')['Load'].apply(list)\n","arrival_times_per_user"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":291,"status":"ok","timestamp":1707960563003,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"AdDEPbzvSXb3"},"outputs":[],"source":["def list_to_matrix_columnwise_corrected(lst, rows=48, columns=1):\n","    # Initialize a matrix of zeros\n","    matrix = np.zeros((rows, columns))\n","\n","    # Fill the matrix column-wise\n","    for i, val in enumerate(lst):\n","        row = i % rows\n","        col = i // rows\n","        if col < columns:\n","            matrix[row, col] = val\n","\n","    return matrix\n","\n","page_matrices = {user_id: list_to_matrix_columnwise_corrected(times) for user_id, times in arrival_times_per_user.items()}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":125,"status":"ok","timestamp":1707960564273,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"Oof9yeywSlhF","outputId":"81713edb-e38b-49c5-e26a-7a1788599569"},"outputs":[],"source":["matrices = list(page_matrices.values())\n","\n","# Stacking the matrices horizontally\n","stacked_page_matrix = np.hstack(matrices)\n","stacked_page_matrix.shape ##blue matrix created"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1707960565499,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"TjnbnsQLL37H","outputId":"b86a3a92-7b9a-4063-fe10-55a83e7c6195"},"outputs":[],"source":["stacked_page_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":817,"status":"ok","timestamp":1707960789848,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"2e39ZRWnZMY6"},"outputs":[],"source":["import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","\n","# Standardize the data (mean=0, variance=1)\n","scaler = StandardScaler()\n","data_standardized = scaler.fit_transform(stacked_page_matrix)  # Transpose to standardize across users, not time points\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":416,"status":"ok","timestamp":1707960804282,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"cLGoLx2q3cwf","outputId":"d9354811-13e1-429e-aa63-5d3c4da1f1e4"},"outputs":[],"source":["low_rank_part, sparse_part = robust_pca(data_standardized, reg_E=0.04, learning_rate=1.2, n_iter_max=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":124,"status":"ok","timestamp":1707960828226,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"LKz0I3Wj3fB-"},"outputs":[],"source":["L_inverse_scaled = scaler.inverse_transform(low_rank_part)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":113,"status":"ok","timestamp":1707960831694,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"kWmtoX2yZgrU","outputId":"2e92c4ee-a016-4bd9-d09f-ec8711f3a661"},"outputs":[],"source":["L_inverse_scaled"]},{"cell_type":"markdown","metadata":{"id":"S9FfDiL_V2Ht"},"source":["##SVD decomposition\n","-Use a hard margin of k=5 (same as the one used in the paper)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":116,"status":"ok","timestamp":1707960853545,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"312fAg5R6xFi"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","import numpy as np\n","\n","# Assuming 'stacked_page_matrix' is your data matrix\n","\n","# Step 1: PCA automatically centers the data, so you don't need to manually subtract the mean\n","\n","# Step 2: Perform PCA with the desired number of components\n","num_principal_components = 10  # This is akin to your num_singular_values\n","pca = PCA(n_components=num_principal_components)\n","\n","# Fit PCA to the data and transform the data onto the principal components\n","pca.fit(L_inverse_scaled)\n","transformed_data = pca.transform(L_inverse_scaled)\n","\n","# Step 3: Reconstruct the data from the principal components\n","non_stationary_component = pca.inverse_transform(transformed_data)\n","\n","# The 'reconstructed_data' matrix now acts as your non-stationary component\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":133,"status":"ok","timestamp":1707960854988,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"6NYCjyBwGZMU","outputId":"f7974ac2-5d3f-4840-fccb-e2b4be8d2317"},"outputs":[],"source":["pca.explained_variance_ratio_"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":138,"status":"ok","timestamp":1707952254838,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"uIINSa2nVwX0"},"outputs":[],"source":["from scipy.linalg import svd\n","\n","# The singular values are in the vector 's'\n","# U and VT are the left and right singular vectors, respectively\n","U, s, VT = svd(low_rank_part)\n","num_singular_values = 10\n","s_reduced = np.zeros(low_rank_part.shape)\n","np.fill_diagonal(s_reduced, s[:num_singular_values])\n","\n","non_stationary_component = U @ s_reduced @ VT ##orange matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":123,"status":"ok","timestamp":1707952237991,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"_QGqUQa5V00p"},"outputs":[],"source":["num_singular_values = 10\n","s_reduced = np.zeros(stacked_page_matrix.shape)\n","np.fill_diagonal(s_reduced, s[:num_singular_values])\n","\n","non_stationary_component = U @ s_reduced @ VT ##orange matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x7rTpB3p-e92"},"outputs":[],"source":["from scipy.linalg import svd\n","# Mean centering the data\n","stacked_page_matrix_centered = stacked_page_matrix - np.mean(stacked_page_matrix, axis=0)\n","\n","# Applying SVD to the mean-centered data\n","U, s, VT = svd(stacked_page_matrix_centered)\n","\n","num_singular_values = 10\n","s_reduced = np.zeros(stacked_page_matrix.shape)\n","np.fill_diagonal(s_reduced, s[:num_singular_values])\n","\n","# Reconstruction from the reduced number of singular values\n","non_stationary_component_centered = U @ s_reduced @ VT\n","\n","# Adding back the mean to the centered reconstruction\n","non_stationary_component = non_stationary_component_centered + np.mean(stacked_page_matrix, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1707960872167,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"qkafvFu7a8DV","outputId":"cc84d465-d8d8-4ef1-f289-c4d18a6a3b89"},"outputs":[],"source":["F_hat_mat = non_stationary_component[:47, :] #L-1 rows of orange matrix\n","y_vector = stacked_page_matrix[-1, :] #last row of blue matrix\n","\n","import numpy as np\n","\n","Y = y_vector.reshape(-1, 1)  # Reshape Y to be a column vector if it's a 1D array\n","\n","# Solve for beta_hat using the least squares method\n","beta_hat, residuals, rank, s = np.linalg.lstsq(F_hat_mat.T, Y, rcond=None)\n","\n","# # beta_hat now contains the estimated beta parameters, should be 18x1 dimensions\n","beta_hat"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":137,"status":"ok","timestamp":1707952200531,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"EOa-ZrnZbazT"},"outputs":[],"source":["array=beta_hat\n","desired_length = 48\n","last_element = array[-1, :]\n","while len(array) < desired_length:\n","    array = np.vstack([array, last_element])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":131,"status":"ok","timestamp":1707952201408,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"q2xXO3KGWFlV"},"outputs":[],"source":["residual=stacked_page_matrix-array.T@non_stationary_component ##green matrix\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1707952202303,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"Yd5q1ali5FKa","outputId":"155de03d-b9d0-42b5-83b0-8281d1be3c6f"},"outputs":[],"source":["residual"]},{"cell_type":"markdown","metadata":{"id":"ElvqvhDPWuD4"},"source":["##Train LSTM with residual (one LSTM model per user)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z-1IkfdKWM8t"},"outputs":[],"source":["userIDs = list(arrival_times_per_user.keys())\n","residuals_dict = {userID: residual[:, i] for i, userID in enumerate(userIDs)}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":188,"status":"ok","timestamp":1707886372291,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"rNBFzGH7V64a","outputId":"a4ed5d3a-4a99-486e-faa4-83ccc574a9ae"},"outputs":[],"source":["residuals_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZCFuqG0Pqci"},"outputs":[],"source":["import numpy as np\n","\n","# Function to scale data to [-1, 1] range\n","def scale_to_neg_one_to_one(X, X_min, X_max):\n","    return 2 * ((X - X_min) / (X_max - X_min)) - 1\n","\n","scaled_residuals_dict = {}\n","mins_dict = {}\n","maxs_dict = {}\n","\n","# Iterate over each user's data in the dictionary\n","for key, values in residuals_dict.items():\n","    # Calculate the min and max from the current user's data\n","    X_min = np.min(values)\n","    X_max = np.max(values)\n","\n","    # Scale the current user's data\n","    scaled_values = scale_to_neg_one_to_one(values, X_min, X_max)\n","\n","    # Store the scaled values in the scaled dictionary\n","    scaled_residuals_dict[key] = scaled_values\n","\n","    # Store the min and max values separately for later use (e.g., scaling test data)\n","    mins_dict[key] = X_min\n","    maxs_dict[key] = X_max\n","\n","# scaled_data_dict now contains the scaled datasets for each user\n","# min_max_values_dict contains the min and max values for each user's data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7N1ZSOv-5a6o"},"outputs":[],"source":["normalized_residuals_dict = {}\n","means_dict = {}\n","stds_dict = {}\n","\n","for key, values in residuals_dict.items():\n","    # Calculate mean and standard deviation\n","    mean = values.mean()\n","    std = values.std()\n","\n","    # Normalize values\n","    normalized_values = (values - mean) / std\n","\n","    # Store normalized values and statistics\n","    normalized_residuals_dict[key] = normalized_values\n","    means_dict[key] = mean\n","    stds_dict[key] = std"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1934825,"status":"ok","timestamp":1707888318125,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"r3IE6qNRYl5x","outputId":"43dee121-5b57-415b-b83a-9a214e2d0bd4"},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import LSTM, Dense, Dropout\n","import numpy as np\n","from keras import backend as K\n","\n","def smape_loss(y_true, y_pred):\n","    denominator = K.maximum(K.abs(y_true) + K.abs(y_pred), K.epsilon())\n","    diff = K.abs(y_pred - y_true) / denominator\n","    return 100 * K.mean(diff, axis=-1)\n","\n","def create_lstm_model(input_shape):\n","    model = Sequential()\n","    model.add(LSTM(units=32, return_sequences=True, input_shape=input_shape))\n","    #model.add(Dropout(0.2))  # Dropout layer after the first LSTM layer\n","    model.add(LSTM(units=16, return_sequences=True))  # Second LSTM layer with 32 units\n","    #model.add(Dropout(0.2))  # Dropout layer after the second LSTM layer\n","    model.add(Dense(units=1))\n","    model.compile(optimizer='adam', loss=smape_loss)\n","    return model\n","\n","def prepare_data(residuals, n_steps):\n","    X, y = [], []\n","    for i in range(len(residuals)):\n","        end_ix = i + n_steps\n","        if end_ix > len(residuals)-1:\n","            break\n","        seq_x, seq_y = residuals[i:end_ix], residuals[end_ix]\n","        X.append(seq_x)\n","        y.append(seq_y)\n","    return np.array(X), np.array(y)\n","\n","user_ids = normalized_residuals_dict.keys()\n","n_steps = 1  # Number of time steps for LSTM. Adjust as needed.\n","\n","lstm_models = {}\n","\n","for user_id in user_ids:\n","    residuals = normalized_residuals_dict[user_id]\n","\n","    # Check if the user has sufficient data\n","    if len(residuals) > n_steps:\n","        X, y = prepare_data(residuals, n_steps)\n","        if X.size > 0 and y.size > 0:\n","            X = X.reshape((X.shape[0], X.shape[1], 1))\n","            lstm_model = create_lstm_model((X.shape[1], 1))\n","            lstm_model.fit(X, y, epochs=100, batch_size=1)\n","            lstm_models[user_id] = lstm_model\n","        else:\n","            print(f\"Insufficient data for user {user_id}\")\n","    else:\n","        print(f\"Not enough data points for user {user_id} for n_steps = {n_steps}\")\n"]},{"cell_type":"markdown","metadata":{"id":"RZzXWInOcFQD"},"source":["#Inference phase"]},{"cell_type":"markdown","metadata":{"id":"Vr3sF1IicIG9"},"source":["##Estimate b_hat (parameters of non-stationary component)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7pI2mh35cLLL"},"outputs":[],"source":["F_hat_mat = non_stationary_component[:48, :] #L-1 rows of orange matrix\n","y_vector = stacked_page_matrix[-1, :] #last row of blue matrix\n","\n","import numpy as np\n","\n","Y = y_vector.reshape(-1, 1)  # Reshape Y to be a column vector if it's a 1D array\n","\n","# Solve for beta_hat using the least squares method\n","beta_hat, residuals, rank, s = np.linalg.lstsq(F_hat_mat.T, Y, rcond=None)\n","\n","# # beta_hat now contains the estimated beta parameters, should be 18x1 dimensions\n"]},{"cell_type":"markdown","metadata":{"id":"MO8SEh3lui0g"},"source":["##Prediction of non-stationary part (using SAMoSSA method)\n","-Use method outlined in part V of figure (to predict value at time interval t use previous L points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dR26VP69wX8a"},"outputs":[],"source":["import pandas as pd\n","\n","# Initialize an empty dictionary to store time values for each user\n","user_times = {}\n","\n","# Process the train dataset\n","for index, row in df_train.iterrows():\n","    user_id = row['UserID']\n","    load = row['Load']\n","    if user_id not in user_times:\n","        user_times[user_id] = []\n","    user_times[user_id].append(load)\n","\n","# Process the test dataset\n","for index, row in df_valid.iterrows():\n","    user_id = row['UserID']\n","    load = row['Load']\n","    if user_id not in user_times:\n","        user_times[user_id] = []\n","    user_times[user_id].append(load)\n","#Create a dictionary where for each user store the last 18 values of the train dataset and the values\n","#of the test dataset for each user"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":186,"status":"ok","timestamp":1707323097021,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"f0xZTi2d-pU0","outputId":"a2e9084b-333a-43d4-9e1b-f5a9dbef4132"},"outputs":[],"source":["len(user_times[367.0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"giKI_s-X11Vt"},"outputs":[],"source":["import numpy as np\n","\n","# Initialize a dictionary to store predictions for each user\n","user_predictions_non_stationary = {}\n","\n","for user_id, times in user_times.items():\n","    # We can only make a prediction if there are at least 18 values\n","    if len(times) >= 48:\n","        predictions = []\n","        # Slide the window and predict\n","        for i in range(len(times) - 48):\n","            window = np.array(times[i:i+48])\n","            prediction = np.dot(window, beta_hat).item()\n","            predictions.append(prediction)\n","        user_predictions_non_stationary[user_id] = predictions\n","\n","# user_predictions now contains the predicted values for each user\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"324iQusM3-yk"},"outputs":[],"source":["# Group by 'userID' and aggregate the 'time' values into lists\n","user_actual_time = df_valid.groupby('UserID')['Load'].apply(list).to_dict()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d5_enra64huA"},"outputs":[],"source":["# Assuming user_actual_time and user_predictions_non_stationary are dictionaries with lists as values\n","residual_test = {key: [a - b for a, b in zip(user_actual_time[key], user_predictions_non_stationary[key])]\n","                 for key in user_actual_time\n","                 if key in user_predictions_non_stationary}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":155,"status":"ok","timestamp":1707323263444,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"xKyZgpPUZYVj","outputId":"1a975b3e-9a0e-4d4f-a2ef-c175af2db36e"},"outputs":[],"source":["mins_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TpBhoxgkY5nC"},"outputs":[],"source":["import numpy as np\n","\n","scaled_residuals_test={}\n","\n","# Function to scale data to [-1, 1] range\n","def scale_to_neg_one_to_one(X, X_min, X_max):\n","    return 2 * ((X - X_min) / (X_max - X_min)) - 1\n","\n","\n","# Iterate over each user's data in the dictionary\n","for key, values in residual_test.items():\n","\n","    X_min = mins_dict[key]\n","    X_max = maxs_dict[key]\n","\n","    # Scale the current user's data\n","    scaled_values = scale_to_neg_one_to_one(values, X_min, X_max)\n","\n","    # Store the scaled values in the scaled dictionary\n","    scaled_residuals_test[key] = scaled_values\n","\n","# scaled_data_dict now contains the scaled datasets for each user\n","# min_max_values_dict contains the min and max values for each user's data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M6cPPcDUCdps"},"outputs":[],"source":["# Adjusted dictionary to store the results\n","adjusted_residual_test = {}\n","\n","for key, values in residual_test.items():\n","    # Retrieve the mean and standard deviation for the current key\n","    mean = means_dict[key]\n","    std = stds_dict[key]\n","\n","    # Adjust the values by subtracting the mean and dividing by the standard deviation\n","    adjusted_values = (values - mean) / std\n","\n","    # Store the adjusted values\n","    adjusted_residual_test[key] = adjusted_values"]},{"cell_type":"markdown","metadata":{"id":"aq7c7gsR6Xt3"},"source":["##Make predictions using LSTM on stationary/residual part"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41006,"status":"ok","timestamp":1707500501971,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"hk09hqVMcanJ","outputId":"b82ab351-7928-4184-d2a9-4851894a435d"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","def prepare_lstm_input(residuals, n_steps):\n","    X = []\n","    for i in range(len(residuals) - n_steps + 1):\n","        X.append(residuals[i:i + n_steps])\n","    return np.array(X)\n","\n","def reverse_scale_to_original(X_scaled, X_min, X_max):\n","    return ((X_scaled + 1) / 2) * (X_max - X_min) + X_min\n","\n","\n","# Create a DataFrame for storing final predictions\n","final_predictions_df = pd.DataFrame(columns=['UserID', 'Final_Prediction'])\n","\n","# Iterate over each user and their residuals\n","for user_id, residuals in scaled_residuals_test.items():\n","    if user_id in lstm_models:\n","        X_min = mins_dict[user_id]\n","        X_max = maxs_dict[user_id]\n","        # Prepare LSTM input from residuals\n","        if len(residuals) >= n_steps:\n","            lstm_input = prepare_lstm_input(residuals, n_steps)\n","            lstm_input = lstm_input.reshape((-1, n_steps, 1))  # Reshape for LSTM\n","\n","            # Make prediction with LSTM\n","            lstm_pred_normalized = lstm_models[user_id].predict(lstm_input)\n","\n","            # Reverse normalization on LSTM predictions to bring them back to original scale\n","            lstm_pred = reverse_scale_to_original(lstm_pred_normalized, X_min, X_max)\n","\n","            # Retrieve the corresponding non-stationary model predictions\n","            non_stationary_pred = user_predictions_non_stationary[user_id]\n","\n","            # Add LSTM predictions to non-stationary model predictions\n","            # Assuming non_stationary_pred is aligned with the last lstm_pred\n","            #combined_pred = non_stationary_pred[-len(lstm_pred):] + (beta_hat[-len(lstm_pred):].T@lstm_pred).flatten()\n","            combined_pred = non_stationary_pred[-len(lstm_pred):] + lstm_pred.flatten()\n","\n","        else:\n","            combined_pred = residuals\n","    else:\n","        # If no LSTM model, use non-stationary model predictions as is\n","        combined_pred = user_predictions_non_stationary[user_id]\n","\n","    # Add combined predictions to the DataFrame\n","    final_predictions_df = final_predictions_df.append(pd.DataFrame({\n","        'UserID': user_id,\n","        'Final_Prediction': combined_pred\n","    }), ignore_index=True)\n","\n","# final_predictions_df now contains combined predictions for each user\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41383,"status":"ok","timestamp":1707888439859,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"yY5dBHHz6gRp","outputId":"af526280-627c-450d-958f-387d0db6db02"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","def prepare_lstm_input(residuals, n_steps):\n","    X = []\n","    for i in range(len(residuals) - n_steps + 1):\n","        X.append(residuals[i:i + n_steps])\n","    return np.array(X)\n","\n","def reverse_normalize_data(normalized_data, mean, std):\n","    return (normalized_data * std) + mean\n","\n","# Create a DataFrame for storing final predictions\n","final_predictions_df = pd.DataFrame(columns=['UserID', 'Final_Prediction'])\n","\n","# Iterate over each user and their residuals\n","for user_id, residuals in adjusted_residual_test.items():\n","    if user_id in lstm_models:\n","        mean = means_dict[user_id]\n","        std = stds_dict[user_id]\n","        # Prepare LSTM input from residuals\n","        if len(residuals) >= n_steps:\n","            lstm_input = prepare_lstm_input(residuals, n_steps)\n","            lstm_input = lstm_input.reshape((-1, n_steps, 1))  # Reshape for LSTM\n","\n","            # Make prediction with LSTM\n","            lstm_pred_normalized = lstm_models[user_id].predict(lstm_input)\n","\n","            # Reverse normalization on LSTM predictions to bring them back to original scale\n","            lstm_pred = reverse_normalize_data(lstm_pred_normalized, mean, std)\n","\n","            # Retrieve the corresponding non-stationary model predictions\n","            non_stationary_pred = user_predictions_non_stationary[user_id]\n","\n","            # Add LSTM predictions to non-stationary model predictions\n","            # Assuming non_stationary_pred is aligned with the last lstm_pred\n","            #combined_pred = non_stationary_pred[-len(lstm_pred):] + (beta_hat[-len(lstm_pred):].T@lstm_pred).flatten()\n","            combined_pred = non_stationary_pred[-len(lstm_pred):] + lstm_pred.flatten()\n","\n","        else:\n","            combined_pred = residuals\n","    else:\n","        # If no LSTM model, use non-stationary model predictions as is\n","        combined_pred = user_predictions_non_stationary[user_id]\n","\n","    # Add combined predictions to the DataFrame\n","    final_predictions_df = final_predictions_df.append(pd.DataFrame({\n","        'UserID': user_id,\n","        'Final_Prediction': combined_pred\n","    }), ignore_index=True)\n","\n","# final_predictions_df now contains combined predictions for each user\n"]},{"cell_type":"markdown","metadata":{"id":"EXJBTz359xrB"},"source":["#SMAPE of hybrid model (Rank Decomposition+LSTM hybrid)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d8m8ahvV9xLH"},"outputs":[],"source":["df = pd.DataFrame(df_valid)\n","\n","# Convert DataFrame to dictionary with UserID as key and Time values as list\n","true_values = df.groupby('UserID')['Load'].apply(list).to_dict()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dBbeEB5m9zgi"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","def calculate_smape(actual, predicted):\n","    \"\"\"Calculate SMAPE between two series.\"\"\"\n","    denominator = (np.abs(actual) + np.abs(predicted))\n","    diff = np.abs(actual - predicted) / denominator\n","    diff[denominator == 0] = 0.0  # handle division by zero\n","    return 100 * np.mean(diff)\n","\n","# Dictionary to store SMAPE for each user\n","smape_values_non_stationary = {}\n","\n","# Iterate over each user\n","for user in user_predictions_non_stationary:\n","    # Retrieve the predicted values for the user and convert to a Pandas Series if not already\n","    predicted = pd.Series(user_predictions_non_stationary[user])\n","\n","    # Retrieve the true values for the user and convert to a Pandas Series\n","    actual = pd.Series(true_values[user],index=predicted.index)\n","\n","    # Calculate SMAPE\n","    smape = calculate_smape(actual, predicted)\n","\n","    # Store the SMAPE value\n","    smape_values_non_stationary[user] = smape\n","\n","# smape_values dictionary now contains the SMAPE for each user\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FIyBeQ39_3Y"},"outputs":[],"source":["final_predictions_df = final_predictions_df.reset_index(drop=True)\n","sorted_JPL_test = df_valid.reset_index(drop=True)\n","combined_df = pd.concat([final_predictions_df, sorted_JPL_test], axis=1)\n","combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D5TTFcB1-CiR"},"outputs":[],"source":["import pandas as pd\n","df = pd.DataFrame(combined_df)\n","\n","# Function to calculate SMAPE\n","def calculate_smape(df):\n","    def smape(y_true, y_pred):\n","        denominator = (abs(y_true) + abs(y_pred))\n","        diff = abs(y_true - y_pred) / denominator\n","        return 100 * diff.mean()\n","\n","    smape_values = df.groupby('UserID').apply(lambda x: smape(x['Load'], x['Final_Prediction']))\n","    return smape_values\n","\n","smape_results_combined = calculate_smape(df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":118,"status":"ok","timestamp":1707888451820,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"T7E0XVac-LzB","outputId":"65cdf96f-ac37-4d7a-9805-7249f9362895"},"outputs":[],"source":["# Assuming smape_results1 and smape_results2 are the SMAPE results from two different DataFrames\n","final_smape_results = pd.DataFrame({\n","    'SMAPE1': smape_values_non_stationary,\n","    'SMAPE2': smape_results_combined\n","})\n","\n","# Select the better SMAPE for each UserID\n","final_smape_results['Best_SMAPE'] = final_smape_results.min(axis=1)\n","\n","# Display the final SMAPE results\n","print(final_smape_results)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":102,"status":"ok","timestamp":1707888455166,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"2NkAB2yM-Y8v","outputId":"463840ef-e32f-4c3b-dff2-aebab0c09bb5"},"outputs":[],"source":["mean_best_smape = final_smape_results['Best_SMAPE'].mean()\n","print(\"SMAPE for hybrid SAMoSSA and LSTM model (%):\", mean_best_smape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":133,"status":"ok","timestamp":1707888469706,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"BT5dsgi_FdPh","outputId":"8c7d0e2c-f87c-422d-f5e1-674f20e7abf8"},"outputs":[],"source":["mean_best_smape = final_smape_results['SMAPE2'].mean()\n","print(\"SMAPE for hybrid SAMoSSA and LSTM model (%):\", mean_best_smape)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMG68WyCCn9wnWtj5tZjRkN","provenance":[{"file_id":"1Lgq4H6nF9VJzG1kL_9MVulbudsmUaH4g","timestamp":1707263992253}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
