{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15724,"status":"ok","timestamp":1707276151708,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"vhfUoHGlGLgn","outputId":"db2f3bd3-37a4-4417-ad86-6fb0a7b34c7e"},"outputs":[],"source":["!pip install iTransformer==0.4.4"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12170,"status":"ok","timestamp":1707276163872,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"xOXgh901GVlr"},"outputs":[],"source":["import torch\n","from iTransformer import iTransformer"]},{"cell_type":"markdown","metadata":{"id":"6-ISMtnM2w1u"},"source":["#Load dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4397,"status":"ok","timestamp":1707276168260,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"Hf3IrHmb2yLT","outputId":"edc1cbfb-c787-4e01-8486-d2f4229f449c"},"outputs":[],"source":["!git clone https://github.com/AbdullahO/SAMoSSA.git"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1707276168261,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"mLvgeMz6g90v"},"outputs":[],"source":["import numpy as np\n","dataset = np.load('/content/SAMoSSA/datasets/electricity/electricity.npy', encoding='bytes')"]},{"cell_type":"markdown","metadata":{"id":"SD56LRGqhwGK"},"source":["#Training and Validation Stage"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":400,"status":"ok","timestamp":1707276274621,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"PhY4bBVXg8um"},"outputs":[],"source":["training_set = dataset[25824:25848]      # Arrays 1 to 25824\n","validation_set = dataset[25848:25872] # Arrays 25825 to 25872"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":122,"status":"ok","timestamp":1707276277670,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"mfwAurZNllM_"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","number_of_hours, num_users = training_set.shape\n","\n","# Generate date range\n","date_range = pd.date_range(start='01/01/2011 00:00', periods=number_of_hours, freq='H')\n","\n","# Reshape and create pairs of values and user IDs\n","data = []\n","for user_id in range(1, num_users + 1):\n","    for hour, value in enumerate(training_set[:, user_id - 1]):\n","        data.append([date_range[hour], value, user_id])\n","\n","# Create DataFrame\n","df_train = pd.DataFrame(data, columns=['Date', 'Load', 'UserID'])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":126,"status":"ok","timestamp":1707276279707,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"XDVDcgjPht8H"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","number_of_hours, num_users = validation_set.shape\n","\n","# Generate date range\n","date_range = pd.date_range(start='17/01/2011 00:00', periods=number_of_hours, freq='H')\n","\n","# Reshape and create pairs of values and user IDs\n","data = []\n","for user_id in range(1, num_users + 1):\n","    for hour, value in enumerate(validation_set[:, user_id - 1]):\n","        data.append([date_range[hour], value, user_id])\n","\n","# Create DataFrame\n","df_valid = pd.DataFrame(data, columns=['Date', 'Load', 'UserID'])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":127,"status":"ok","timestamp":1707276280701,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"r-lTt3rUl1Ef"},"outputs":[],"source":["sampled_user_ids=[ 58,  53,  84, 274, 164, 365, 340, 225, 281,  48,  42, 298, 334,\n","        63,   3, 229, 262, 104,  64,  27, 133,  61, 245,   2,  67, 337,\n","       127, 248, 218, 217, 317, 280, 243,  76, 219, 250, 305,  75, 350,\n","        49,  95, 224, 162, 367,  73, 161, 238, 324,  29, 154]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1707276281176,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"-z6ejDRkl3Bg"},"outputs":[],"source":["# Filter the original DataFrame to include only the sampled user IDs\n","df_train = df_train[df_train['UserID'].isin(sampled_user_ids)]\n","df_valid = df_valid[df_valid['UserID'].isin(sampled_user_ids)]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":131,"status":"ok","timestamp":1707276282181,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"mrF2-dp-hnX2","outputId":"14ce2f0a-9a6f-4b6c-8dc2-f3df313335e5"},"outputs":[],"source":["load_per_user_train = df_train.groupby('UserID')['Load'].apply(list)\n","load_per_user_train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":117,"status":"ok","timestamp":1707276284365,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"96wH8Sv67nzf","outputId":"4d18654b-f8c2-4a09-a657-dae92aa4d9b0"},"outputs":[],"source":["load_per_user_valid = df_valid.groupby('UserID')['Load'].apply(list)\n","load_per_user_valid"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":123,"status":"ok","timestamp":1707276286915,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"YwNLfd5d9DK2","outputId":"d39be775-8dc5-4b1b-add6-ee9055ff6268"},"outputs":[],"source":["time_series = torch.tensor(load_per_user_train.tolist()).unsqueeze(0)\n","\n","time_series.shape, time_series.dtype"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":116,"status":"ok","timestamp":1707276287840,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"tlSHLYvk-KfS","outputId":"757999c1-4388-452d-80bf-ae533ddcd27f"},"outputs":[],"source":["transposed_time_series = time_series.transpose(1, 2)  # (batch, lookback len, variates)\n","\n","transposed_time_series.shape, transposed_time_series.dtype"]},{"cell_type":"markdown","metadata":{"id":"qIxuTh1i-Rq3"},"source":["#Train iTransformer model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":134,"status":"ok","timestamp":1707276289710,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"uuRNKfTPnKtw","outputId":"ac684e96-704f-4c67-a00b-3081de17d126"},"outputs":[],"source":["array_list_valid = [np.array(lst) for lst in load_per_user_valid]\n","# Combine the arrays into a single 2D array and convert to a tensor\n","combined_array = torch.tensor(array_list_valid).T\n","\n","validation_tensor = combined_array.unsqueeze(0)\n","\n","# Verify the shape\n","print(validation_tensor.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":115,"status":"ok","timestamp":1707276291607,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"P_KnjwzFvilV","outputId":"e450ce5a-1231-464c-8242-09baa32f0a43"},"outputs":[],"source":["transposed_time_series.shape,transposed_time_series.dtype"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":151,"status":"ok","timestamp":1707276814303,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"u10LuJgBoLT1"},"outputs":[],"source":["mean = transposed_time_series.mean(dim=(0, 1), keepdim=True)\n","std = transposed_time_series.std(dim=(0, 1), keepdim=True)\n","\n","# Normalize training data\n","normalized_train_tensor = (transposed_time_series - mean) / std\n","\n","# Normalize validation data using training mean and std\n","validation_tensor_normalized = (validation_tensor - mean) / std"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":487},"executionInfo":{"elapsed":115414,"status":"error","timestamp":1707277256975,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"0i8aAquFmlwI","outputId":"9cd39bc5-ea86-4c50-8382-f265ed2ceddb"},"outputs":[],"source":["import torch\n","import torch.optim as optim\n","from iTransformer import iTransformer\n","from torch.utils.data import DataLoader\n","import copy\n","import torch.nn as nn\n","import torch.nn.init as init\n","import random\n","\n","# Function to calculate SMAPE\n","def calculate_smape(predictions, ground_truth):\n","    predictions = predictions.float()\n","    ground_truth = ground_truth.float()\n","    mask = ground_truth != 0\n","    masked_predictions = predictions[mask]\n","    masked_ground_truth = ground_truth[mask]\n","    numerator = torch.abs(masked_predictions - masked_ground_truth)\n","    denominator = torch.abs(masked_predictions) + torch.abs(masked_ground_truth)\n","    smape = torch.mean(numerator / denominator)\n","    return smape.item()\n","\n","# Initialization methods\n","def xavier_init(m):\n","    if isinstance(m, nn.Linear):\n","        init.xavier_uniform_(m.weight)\n","        if m.bias is not None:\n","            init.zeros_(m.bias)\n","\n","def kaiming_init(m):\n","    if isinstance(m, nn.Linear):\n","        init.kaiming_uniform_(m.weight, nonlinearity='relu')\n","        if m.bias is not None:\n","            init.zeros_(m.bias)\n","\n","def uniform_init(m):\n","    if isinstance(m, nn.Linear):\n","        init.uniform_(m.weight, -0.1, 0.1)\n","        if m.bias is not None:\n","            init.zeros_(m.bias)\n","\n","initialization_methods = [uniform_init]\n","\n","# Constants\n","lookback_len = 23\n","num_variates = 50\n","pred_length = 23\n","num_iterations = 30\n","\n","best_smape_error = float('inf')\n","best_model_state = None\n","best_init_method = None\n","\n","for iteration in range(num_iterations):\n","    # Randomly select an initialization method\n","    init_method = random.choice(initialization_methods)\n","\n","    # Model initialization\n","    model = iTransformer(\n","        num_variates=num_variates,\n","        lookback_len=lookback_len,\n","        dim=32,\n","        depth=4,\n","        heads=8,\n","        dim_head=32,\n","        pred_length=pred_length,\n","        num_tokens_per_variate=2,\n","    )\n","\n","    # Apply selected initialization\n","    model.apply(init_method)\n","\n","    # Loss function and optimizer\n","    loss_function = torch.nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)\n","\n","    # DataLoader for training data\n","    train_loader = DataLoader(normalized_train_tensor, batch_size=2, shuffle=False)\n","\n","    # Training loop\n","    model.train()\n","    for epoch in range(1000):\n","        total_loss = 0\n","        for inputs in train_loader:\n","            x = inputs[:, :lookback_len, :]\n","            y = inputs[:, 1:lookback_len+1, :]\n","            optimizer.zero_grad()\n","            outputs = model(x)\n","            loss = loss_function(outputs[pred_length], y)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","        scheduler.step()\n","    # Evaluation\n","    test_tensor_adjusted = normalized_train_tensor[:, :23, :]\n","    test_tensor_adjusted_float = test_tensor_adjusted.float()\n","    model.eval()\n","\n","    with torch.no_grad():\n","      model_output = model(test_tensor_adjusted_float)  # Make sure this tensor is normalized\n","      if pred_length in model_output:\n","        predictions_normalized = model_output[pred_length]\n","        # Transform predictions back to original scale\n","        predictions_original_scale = (predictions_normalized * std) + mean\n","        smape_error = calculate_smape(predictions_original_scale, validation_tensor[:, :23, :])\n","        if smape_error < best_smape_error:\n","          best_smape_error = smape_error\n","          best_model_state = copy.deepcopy(model.state_dict())\n","          best_init_method = init_method.__name__\n","\n","  # Print the best initialization method and SMAPE error\n","          print(f\"Best Initialization Method: {best_init_method}\")\n","          print(f\"Best SMAPE Error(%): {best_smape_error*100}\")\n","\n","  # Optionally, save the best model state\n","  # if best_model_state is not None:\n","  #     torch.save(best_model_state, 'best_model.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":439},"executionInfo":{"elapsed":310,"status":"error","timestamp":1707276299340,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"MfEc3Zxpp1Cr","outputId":"f02365ec-e6fa-430b-8ceb-57b2530b1113"},"outputs":[],"source":["import torch\n","import torch.optim as optim\n","from iTransformer import iTransformer\n","from torch.utils.data import DataLoader\n","import copy\n","import itertools\n","\n","# Function to calculate SMAPE\n","def calculate_smape(predictions, ground_truth):\n","    predictions = predictions\n","    ground_truth = ground_truth\n","    mask = ground_truth != 0\n","    masked_predictions = predictions[mask]\n","    masked_ground_truth = ground_truth[mask]\n","    numerator = torch.abs(masked_predictions - masked_ground_truth)\n","    denominator = torch.abs(masked_predictions+masked_ground_truth)\n","    smape = torch.mean(numerator / denominator)\n","    return smape.item()\n","\n","# Constants\n","lookback_len = 24\n","num_variates = 50\n","pred_length = 24\n","\n","# Define the hyperparameter grid\n","# learning_rates = [0.001, 0.01, 0.1]\n","# depths = [2, 4, 6]\n","# heads = [4, 8, 12]\n","learning_rates = [0.001]\n","depths = [4]\n","heads = [8]\n","\n","# Create a list of all possible combinations of hyperparameters\n","hyperparameter_grid = list(itertools.product(learning_rates, depths, heads))\n","\n","# Initialize variables to store the best hyperparameters and corresponding SMAPE\n","best_smape_error = float('inf')\n","best_hyperparameters = None\n","best_model_state = None\n","\n","# DataLoader for training and validation data\n","train_loader = DataLoader(transposed_time_series, batch_size=2, shuffle=False)\n","validation_loader = DataLoader(validation_tensor, batch_size=2, shuffle=False)\n","\n","for lr, depth, head in hyperparameter_grid:\n","\n","    # Initialize the model with the current set of hyperparameters\n","    model = iTransformer(\n","        num_variates=num_variates,\n","        lookback_len=lookback_len,\n","        dim=32,\n","        depth=depth,\n","        heads=head,\n","        dim_head=32,\n","        pred_length=pred_length,\n","        num_tokens_per_variate=2,\n","    )\n","\n","    # Loss function and optimizer\n","    loss_function = torch.nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)\n","\n","    # Training loop\n","    model.train()\n","\n","    for epoch in range(1000):\n","        total_loss = 0\n","        for inputs in train_loader:\n","            # Modify these lines to correctly slice the inputs and targets\n","            x = inputs[:, :lookback_len, :]  # Input sequence with lookback length\n","            y = inputs[:, 1:lookback_len + 1, :]  # Target sequence with prediction length\n","            optimizer.zero_grad()\n","            outputs = model(x)\n","            loss = loss_function(outputs[pred_length],y)  # Ensure outputs and y have compatible shapes\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","    # Evaluation\n","    val_tensor_adjusted = validation_tensor[:, :47, :]\n","    val_tensor_adjusted_float = val_tensor_adjusted.float()\n","    model.eval()\n","\n","    with torch.no_grad():\n","        model_output = model(val_tensor_adjusted_float)\n","        if pred_length in model_output:\n","            predictions = model_output[pred_length]\n","            smape_error = calculate_smape(predictions, val_tensor_adjusted_float)\n","            if smape_error < best_smape_error:\n","                best_smape_error = smape_error\n","                best_hyperparameters = (lr, depth, head)\n","                best_model_state = copy.deepcopy(model.state_dict())\n","\n","                # Print the best hyperparameters and corresponding SMAPE\n","                print(f\"Best Hyperparameters: Learning Rate: {best_hyperparameters[0]}, Depth: {best_hyperparameters[1]}, Heads: {best_hyperparameters[2]}\")\n","                print(f\"Best SMAPE Error: {best_smape_error}\")\n"]},{"cell_type":"markdown","metadata":{"id":"XsIiBzT3zyLV"},"source":["#Testing Stage"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"twLqh4JV0RfB"},"outputs":[],"source":["train_val_set = dataset[25440:25872]\n","testing_set = dataset[25872:25920]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VVmtGej50p5w"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","number_of_hours, num_users = train_val_set.shape\n","\n","# Generate date range\n","date_range = pd.date_range(start='01/01/2011 00:00', periods=number_of_hours, freq='H')\n","\n","# Reshape and create pairs of values and user IDs\n","data = []\n","for user_id in range(1, num_users + 1):\n","    for hour, value in enumerate(train_val_set[:, user_id - 1]):\n","        data.append([date_range[hour], value, user_id])\n","\n","# Create DataFrame\n","df_train_val = pd.DataFrame(data, columns=['Date', 'Load', 'UserID'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZGjiHV500Ng"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","number_of_hours, num_users = testing_set.shape\n","\n","# Generate date range\n","date_range = pd.date_range(start='19/01/2011 00:00', periods=number_of_hours, freq='H')\n","\n","# Reshape and create pairs of values and user IDs\n","data = []\n","for user_id in range(1, num_users + 1):\n","    for hour, value in enumerate(testing_set[:, user_id - 1]):\n","        data.append([date_range[hour], value, user_id])\n","\n","# Create DataFrame\n","df_test = pd.DataFrame(data, columns=['Date', 'Load', 'UserID'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":412,"status":"ok","timestamp":1706543911666,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":0},"id":"CtV47IV41G-L","outputId":"d100637e-9612-4711-dc30-32c7cb46f5f8"},"outputs":[],"source":["load_per_user_train = df_train_val.groupby('UserID')['Load'].apply(list)\n","load_per_user_train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706543912890,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":0},"id":"-mVRlaYA1PSR","outputId":"f66af063-0aac-4f70-ec11-a80adf2d4e08"},"outputs":[],"source":["load_per_user_test = df_test.groupby('UserID')['Load'].apply(list)\n","load_per_user_test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":238,"status":"ok","timestamp":1706543915389,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":0},"id":"9Ro2nizu2_6S","outputId":"6f3429d7-e907-4799-b5b7-cab9c4b57647"},"outputs":[],"source":["time_series = torch.tensor(load_per_user_train.tolist()).unsqueeze(0)\n","\n","time_series.shape, time_series.dtype"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706543916187,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":0},"id":"bW_xBlm43FFS","outputId":"448c8dc0-5cb7-46ec-c120-03ff40e76dc3"},"outputs":[],"source":["transposed_time_series = time_series.transpose(1, 2)  # (batch, lookback len, variates)\n","\n","transposed_time_series.shape, transposed_time_series.dtype"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":211,"status":"ok","timestamp":1706543930663,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":0},"id":"NnvuiA3k3KEB","outputId":"8d1e8879-613d-450d-efd9-2603c5154c26"},"outputs":[],"source":["array_list_valid = [np.array(lst) for lst in load_per_user_test]\n","# Combine the arrays into a single 2D array and convert to a tensor\n","combined_array = torch.tensor(array_list_valid).T\n","\n","test_tensor = combined_array.unsqueeze(0)\n","\n","# Verify the shape\n","print(test_tensor.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1706546113661,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":0},"id":"RfSQU1NKEhVg","outputId":"e241aff8-c7af-4973-f222-0eadefb18fd0"},"outputs":[],"source":["for inputs in train_loader:\n","  print(inputs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":402},"executionInfo":{"elapsed":2159,"status":"error","timestamp":1706547453419,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":0},"id":"XPh4-K97zwBd","outputId":"17720961-cca0-41ae-d3dd-eb2432c466be"},"outputs":[],"source":["import torch\n","import torch.optim as optim\n","from iTransformer import iTransformer\n","from torch.utils.data import DataLoader\n","import copy\n","import torch.nn as nn\n","import torch.nn.init as init\n","import random\n","\n","# Function to calculate SMAPE\n","def calculate_smape(predictions, ground_truth):\n","    predictions = predictions.float()\n","    ground_truth = ground_truth.float()\n","    mask = ground_truth != 0\n","    masked_predictions = predictions[mask]\n","    masked_ground_truth = ground_truth[mask]\n","    numerator = torch.abs(masked_predictions - masked_ground_truth)\n","    denominator = torch.abs(masked_predictions+masked_ground_truth)\n","    smape = torch.mean(numerator / denominator)\n","    return smape.item()\n","\n","def uniform_init(m):\n","    if isinstance(m, nn.Linear):\n","        init.uniform_(m.weight, -0.1, 0.1)\n","        if m.bias is not None:\n","            init.zeros_(m.bias)\n","\n","# Constants\n","lookback_len = 47\n","num_variates = 370\n","pred_length = 47\n","\n","\n","best_smape_error = float('inf')\n","best_model_state = None\n","best_init_method = None\n","\n","\n","# Model initialization\n","model = iTransformer(\n","    num_variates=num_variates,\n","    lookback_len=lookback_len,\n","    dim=256,\n","    depth=6,\n","    heads=8,\n","    dim_head=256,\n","    pred_length=pred_length,\n","    num_tokens_per_variate=2,\n",")\n","\n","# Apply selected initialization\n","model.apply(uniform_init)\n","\n","# Loss function and optimizer\n","loss_function = torch.nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)\n","\n","# DataLoader for training data\n","train_loader = DataLoader(transposed_time_series, batch_size=32, shuffle=False)\n","\n","# Training loop\n","model.train()\n","for epoch in range(100):\n","    print(epoch)\n","    total_loss = 0\n","    for inputs in train_loader:\n","        x = inputs[:, :lookback_len, :]\n","        y = inputs[:, lookback_len:lookback_len+pred_length, :]\n","        optimizer.zero_grad()\n","        outputs = model(x)\n","        loss = loss_function(outputs[pred_length], y)\n","        optimizer.step()\n","        total_loss += loss.item()\n","        print(loss)\n","    scheduler.step()\n","\n","# Evaluation\n","test_tensor_adjusted = test_tensor[:, :47, :]\n","test_tensor_adjusted_float = test_tensor_adjusted.float()\n","model.eval()\n","\n","with torch.no_grad():\n","  model_output = model(test_tensor_adjusted_float)\n","  if pred_length in model_output:\n","      predictions = model_output[pred_length]\n","      smape_error = calculate_smape(predictions, test_tensor_adjusted)\n","      if smape_error < best_smape_error:\n","          best_smape_error = smape_error\n","          best_model_state = copy.deepcopy(model.state_dict())\n","\n","print(f\"Best SMAPE Error(%): {best_smape_error*100}\")\n","\n","  # Optionally, save the best model state\n","  # if best_model_state is not None:\n","  #     torch.save(best_model_state, 'best_model.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"xp2VksS7KSfP","outputId":"10b7e6a2-6add-468d-c5ce-c18f1ed44f7c"},"outputs":[],"source":["import torch\n","import torch.optim as optim\n","from iTransformer import iTransformer\n","from torch.utils.data import DataLoader\n","import copy\n","import torch.nn as nn\n","import torch.nn.init as init\n","import random\n","\n","# Function to calculate SMAPE\n","def calculate_smape(predictions, ground_truth):\n","    predictions = predictions.float()\n","    ground_truth = ground_truth.float()\n","    mask = ground_truth != 0\n","    masked_predictions = predictions[mask]\n","    masked_ground_truth = ground_truth[mask]\n","    numerator = torch.abs(masked_predictions - masked_ground_truth)\n","    denominator = torch.abs(masked_predictions) + torch.abs(masked_ground_truth)\n","    smape = torch.mean(numerator / denominator)\n","    return smape.item()\n","\n","# Initialization methods\n","def xavier_init(m):\n","    if isinstance(m, nn.Linear):\n","        init.xavier_uniform_(m.weight)\n","        if m.bias is not None:\n","            init.zeros_(m.bias)\n","\n","def kaiming_init(m):\n","    if isinstance(m, nn.Linear):\n","        init.kaiming_uniform_(m.weight, nonlinearity='relu')\n","        if m.bias is not None:\n","            init.zeros_(m.bias)\n","\n","def uniform_init(m):\n","    if isinstance(m, nn.Linear):\n","        init.uniform_(m.weight, -0.1, 0.1)\n","        if m.bias is not None:\n","            init.zeros_(m.bias)\n","\n","initialization_methods = [xavier_init, kaiming_init, uniform_init]\n","\n","# Constants\n","lookback_len = 47\n","num_variates = 370\n","pred_length = 47\n","num_iterations = 5\n","\n","best_smape_error = float('inf')\n","best_model_state = None\n","best_init_method = None\n","\n","for iteration in range(num_iterations):\n","    # Randomly select an initialization method\n","    init_method = random.choice(initialization_methods)\n","\n","    # Model initialization\n","    model = iTransformer(\n","        num_variates=num_variates,\n","        lookback_len=lookback_len,\n","        dim=512,\n","        depth=4,\n","        heads=8,\n","        dim_head=512,\n","        pred_length=pred_length,\n","        num_tokens_per_variate=2,\n","    )\n","\n","    # Apply selected initialization\n","    model.apply(init_method)\n","\n","    # Loss function and optimizer\n","    loss_function = torch.nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)\n","\n","    # DataLoader for training data\n","    train_loader = DataLoader(transposed_time_series, batch_size=32, shuffle=False)\n","\n","    # Training loop\n","    model.train()\n","    for epoch in range(1000):\n","        total_loss = 0\n","        for inputs in train_loader:\n","            x = inputs[:, :lookback_len, :]\n","            y = inputs[:, 1:lookback_len+1, :]\n","            optimizer.zero_grad()\n","            outputs = model(x)\n","            loss = loss_function(outputs[pred_length], y)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","        scheduler.step()\n","    # Evaluation\n","    test_tensor_adjusted = transposed_time_series[:, :47, :]\n","    test_tensor_adjusted_float = test_tensor_adjusted.float()\n","    model.eval()\n","\n","    with torch.no_grad():\n","        model_output = model(test_tensor_adjusted_float)\n","        if pred_length in model_output:\n","            predictions = model_output[pred_length]\n","            smape_error = calculate_smape(predictions, test_tensor_adjusted_float)\n","            if smape_error < best_smape_error:\n","                best_smape_error = smape_error\n","                best_model_state = copy.deepcopy(model.state_dict())\n","                best_init_method = init_method.__name__\n","\n","  # Print the best initialization method and SMAPE error\n","                print(f\"Best Initialization Method: {best_init_method}\")\n","                print(f\"Best SMAPE Error(%): {best_smape_error*100}\")\n","\n","  # Optionally, save the best model state\n","  # if best_model_state is not None:\n","  #     torch.save(best_model_state, 'best_model.pth')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPh5Kpuiz8ziGIEN8EW2AIO","provenance":[{"file_id":"1NeAUL1-_0TwkU1yUJRLaN7mOYgaIrE2d","timestamp":1706267936062}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
