{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26341,"status":"ok","timestamp":1715572728334,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"DL0k10RoUPJW","outputId":"188bd46c-d7d5-48c1-f26d-a78a6401cc9f"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M1Ot-FcdUzts"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","# Define the file paths\n","file_path1 = '/content/drive/MyDrive/SuperUROP /Data Analysis/caltech_training_data.csv'\n","file_path2 = '/content/drive/MyDrive/SuperUROP /Data Analysis/caltech_testing_data.csv'\n","file_path3 = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_training_data.csv'\n","file_path4  = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_testing_data.csv'\n","# Use pandas to read the CSV files and then convert them to NumPy arrays\n","caltech_train = pd.read_csv(file_path1).values\n","caltech_test = pd.read_csv(file_path2).values\n","\n","JPL_train = pd.read_csv(file_path3).values\n","JPL_test=pd.read_csv(file_path4).values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":199,"status":"ok","timestamp":1715523921800,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"ZNc-Ob3N2rFS","outputId":"ff459b3c-af6c-4ee1-c50a-41335c2559a2"},"outputs":[],"source":["JPL_test[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":222,"status":"ok","timestamp":1715572736988,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"lXml4U-O2fzl","outputId":"4d0f86e6-88ad-4318-fce6-3d0c763bed24"},"outputs":[],"source":["filtered_rows = JPL_test[JPL_test[:, 4] == 356]\n","\n","# Printing the filtered rows\n","print(filtered_rows)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27--VKb_k94F"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","# Define the file paths\n","file_path1 = '/content/drive/MyDrive/SuperUROP /Data Analysis/caltech_training_data_full.csv'\n","file_path3 = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_training_data.csv'\n","file_path4 = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_testing_data.csv'\n","\n","# Use pandas to read the CSV files\n","caltech_full_df = pd.read_csv(file_path1)\n","\n","JPL_train_df = pd.read_csv(file_path3)\n","JPL_test_df = pd.read_csv(file_path4)\n","\n","# Filter the caltech_full DataFrame\n","caltech_full_df = caltech_full_df[caltech_full_df['duration'] <= 24]\n","\n","caltech_train_df = caltech_full_df[(caltech_full_df['connectionTime'] >= '2021-03-01') & (caltech_full_df['connectionTime'] <= '2021-05-31')]\n","caltech_train_df= caltech_train_df[caltech_train_df['no_sessions'] >= 50]\n","caltech_test_df = caltech_full_df[(caltech_full_df['connectionTime'] >= '2021-06-01') & (caltech_full_df['connectionTime'] <= '2021-06-30')]\n","\n","JPL_train_df=JPL_train_df[JPL_train_df['no_sessions'] >= 30]\n","\n","# Convert to NumPy arrays if necessary\n","caltech_train = caltech_train_df.values\n","caltech_test = caltech_test_df.values\n","JPL_train = JPL_train_df.values\n","JPL_test = JPL_test_df.values\n"]},{"cell_type":"markdown","metadata":{"id":"kmxS_FXBWZ-S"},"source":["#Data Processing for mode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xf1aCXzmWZNH"},"outputs":[],"source":["JPL_train=JPL_train[:,1:]\n","JPL_test=JPL_test[:,1:]\n","\n","sorted_JPL_training = sorted(JPL_train, key=lambda x: x[3])\n","sorted_JPL_training=np.delete(sorted_JPL_training, [1], axis=1)\n","sorted_JPL_testing = sorted(JPL_test, key=lambda x: x[3])\n","sorted_JPL_testing=np.delete(sorted_JPL_testing, [1], axis=1)\n","\n","def convert_to_interval(datetime_str):\n","    # Extract time string from the datetime string\n","    time_str = datetime_str.split()[1]\n","\n","    # Convert time string into hours, minutes, and seconds\n","    hours, minutes, seconds = map(int, time_str.split(':'))\n","\n","    # Calculate total minutes\n","    total_minutes = hours * 60 + minutes\n","\n","    # Find the nearest lower multiple of 15 for the minutes\n","    interval_minutes = (total_minutes // 15) * 15\n","\n","    # Convert back to hour and minute\n","    interval_hour, interval_minute = divmod(interval_minutes, 60)\n","\n","    # Return as a formatted string\n","    return \"{:02d}:{:02d}\".format(interval_hour, interval_minute)\n","\n","sorted_JPL_training[:, 0] = np.vectorize(convert_to_interval)(sorted_JPL_training[:, 0])\n","sorted_JPL_testing[:, 0] = np.vectorize(convert_to_interval)(sorted_JPL_testing[:, 0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"neAuDRxPXxWc"},"outputs":[],"source":["sorted_JPL_training[:, 3] = sorted_JPL_training[:, 3].astype(float) * 60\n","sorted_JPL_training[:, 3] = np.round(sorted_JPL_training[:, 3].astype(float) / 10) * 10\n","sorted_JPL_testing[:, 3] = sorted_JPL_testing[:, 3].astype(float) * 60\n","sorted_JPL_testing[:, 3] = np.round(sorted_JPL_testing[:, 3].astype(float) / 10) * 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_tc39X4SXzq4"},"outputs":[],"source":["#Make training and testing set have the same user IDs\n","users_from_training = set(sorted_JPL_training[:, 2])\n","mask = np.isin(sorted_JPL_testing[:, 2], list(users_from_training))\n","sorted_JPL_testing = sorted_JPL_testing[mask]\n","users_from_testing = set(sorted_JPL_testing[:, 2])\n","mask = np.isin(sorted_JPL_training[:, 2], list(users_from_testing))\n","sorted_JPL_training = sorted_JPL_training[mask]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i7rFUFoGX6tX"},"outputs":[],"source":["# Convert time strings to total minutes for sorting\n","total_minutes_JPL_training = np.array([int(time.split(':')[0])*60 + int(time.split(':')[1]) for time in sorted_JPL_training[:, 0]])\n","total_minutes_JPL_testing = np.array([int(time.split(':')[0])*60 + int(time.split(':')[1]) for time in sorted_JPL_testing[:, 0]])\n","# Argsort first by the 3rd column and then by total_minutes\n","indices_JPL_training = np.lexsort((total_minutes_JPL_training, sorted_JPL_training[:, 2].astype(int)))\n","indices_JPL_testing = np.lexsort((total_minutes_JPL_testing, sorted_JPL_testing[:, 2].astype(int)))\n","# Use the sorted indices to reorder the array\n","sorted_data_JPL_training = sorted_JPL_training[indices_JPL_training]\n","sorted_data_JPL_testing = sorted_JPL_testing[indices_JPL_testing]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":164,"status":"ok","timestamp":1715572747034,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"7-eBUpEcKGb3","outputId":"00e447a2-d854-474d-afa0-c35212e6c65d"},"outputs":[],"source":["sorted_data_JPL_testing"]},{"cell_type":"markdown","metadata":{"id":"T-LH42jEdjoD"},"source":["#Data Processing for MLR & DKDE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8P5wsz9EVgXL"},"outputs":[],"source":["#Remove departure time (2nd column)\n","# Convert arrival date to hour and find day of the week\n","from datetime import datetime\n","\n","def convert_time_and_day(data_array):\n","    \"\"\"\n","    Converts the time from HH:MM to HH.XX format and appends the day of the week to it.\n","    Also, removes the second column.\n","    \"\"\"\n","    transformed_data = []\n","    for row in data_array:\n","        # Convert the arrival time to HH.XX format\n","        time_obj = datetime.strptime(row[0], '%Y-%m-%d %H:%M:%S')\n","        new_time = time_obj.hour + (time_obj.minute / 60.0)\n","\n","        # Convert the date to a day of the week\n","        day_of_week = time_obj.strftime('%A')\n","        new_time = str(new_time) + \" \" + day_of_week\n","\n","        # Create a new row excluding the second column\n","        new_row = [new_time] + list(row[2:])\n","        transformed_data.append(new_row)\n","\n","    return np.array(transformed_data)\n","\n","JPL_train=convert_time_and_day(JPL_train)\n","JPL_test=convert_time_and_day(JPL_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFJTpBVqd-jm"},"outputs":[],"source":["def day_to_number(day):\n","    \"\"\"Converts a day of the week to its corresponding discrete value.\"\"\"\n","    days = {\n","        'Monday': 1,\n","        'Tuesday': 2,\n","        'Wednesday': 3,\n","        'Thursday': 4,\n","        'Friday': 5,\n","        'Saturday': 6,\n","        'Sunday': 7\n","    }\n","    return days[day]\n","\n","def separate_time_and_day(data_array):\n","    \"\"\"\n","    Separates the time and day in the given column,\n","    and converts the day into a discrete value between 1 and 7.\n","\n","    \"\"\"\n","    transformed_data = []\n","    for row in data_array:\n","        time_day_str = row[0]\n","        time, day = time_day_str.split()\n","        time = float(time)\n","        day_num = day_to_number(day)\n","\n","        # Create a new row with separated time and day number\n","        new_row = [time, day_num] + list(row[1:])\n","        transformed_data.append(new_row)\n","\n","    return np.array(transformed_data)\n","\n","JPL_train=separate_time_and_day(JPL_train)\n","JPL_test=separate_time_and_day(JPL_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OYcBnwTisZ8n"},"outputs":[],"source":["#Make training and testing set have the same user IDs\n","users_from_training_caltech = set(caltech_train[:, 3])\n","mask_caltech = np.isin(caltech_test[:, 3], list(users_from_training_caltech))\n","caltech_test = caltech_test[mask_caltech]\n","users_from_testing_caltech = set(caltech_test[:, 3])\n","mask_caltech = np.isin(caltech_train[:, 3], list(users_from_testing_caltech))\n","caltech_train = caltech_train[mask_caltech]\n","\n","users_from_training = set(JPL_train[:, 3])\n","mask = np.isin(JPL_test[:, 3], list(users_from_training))\n","JPL_test = JPL_test[mask]\n","users_from_testing = set(JPL_test[:, 3])\n","mask = np.isin(JPL_train[:, 3], list(users_from_testing))\n","JPL_train = JPL_train[mask]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eORUVi8wemPe"},"outputs":[],"source":["JPL_train = np.array(JPL_train, dtype='float')\n","JPL_test = np.array(JPL_test, dtype='float')"]},{"cell_type":"markdown","metadata":{"id":"yELKBYaHSvPO"},"source":["#Data Processing for correlation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hBlAwEPNSyjh"},"outputs":[],"source":["from collections import defaultdict\n","\n","# Organize the arrival times by user ID for the entire dataset\n","user_times_JPL = defaultdict(list)\n","\n","\n","# Accumulate all arrival times for each user in JPL_train\n","for row in JPL_train:\n","    arrival_time = float(row[0])\n","    user_id = row[3]\n","    user_times_JPL[user_id].append(arrival_time)\n","\n","# Define the functions for hourly and half-hourly representations\n","def create_hourly_representation(times):\n","    hourly_vector = [0] * 24\n","    for time in times:\n","        hour = int(float(time))\n","        hourly_vector[hour] += 1\n","    return hourly_vector\n","\n","def create_half_hourly_representation(times):\n","    half_hourly_vector = [0] * 48\n","    for time in times:\n","        interval = int(float(time) * 2)  # Multiply by 2 for half-hourly intervals\n","        half_hourly_vector[interval] += 1\n","    return half_hourly_vector\n","\n","# Compute the hourly and half-hourly representations for each user\n","hourly_representations_JPL = {}\n","half_hourly_representations_JPL = {}\n","\n","for user_id, times in user_times_JPL.items():\n","    hourly_representations_JPL[user_id] = create_hourly_representation(times)\n","    half_hourly_representations_JPL[user_id] = create_half_hourly_representation(times)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ORxUMNMwTKb6"},"outputs":[],"source":["import numpy as np\n","from collections import defaultdict\n","\n","# Organize the energy consumption durations by user ID for the entire dataset\n","user_durations_JPL = defaultdict(list)\n","\n","\n","# Accumulate all energy consumption durations for each user in JPL_train\n","for row in JPL_train:\n","    stay_duration = float(row[4])\n","    user_id = row[3]\n","    user_durations_JPL[user_id].append(stay_duration)\n","\n","# Define functions for hourly and half-hourly representations\n","def create_hourly_representation_duration(durations):\n","    hourly_vector = [0] * 24\n","    for duration in durations:\n","        index = int(float(duration))\n","        hourly_vector[index] += 1\n","    return hourly_vector\n","\n","def create_half_hourly_representation_duration(durations):\n","    half_hourly_vector = [0] * 48\n","    for duration in durations:\n","        interval = int(float(duration) * 2)\n","        half_hourly_vector[interval] += 1\n","    return half_hourly_vector\n","\n","# Compute the hourly and half-hourly representations for each user\n","hourly_representations_duration_JPL = {}\n","half_hourly_representations_duration_JPL = {}\n","\n","for user_id, durations in user_durations_JPL.items():\n","    hourly_representations_duration_JPL[user_id] = create_hourly_representation_duration(durations)\n","    half_hourly_representations_duration_JPL[user_id] = create_half_hourly_representation_duration(durations)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ea-OFUnwTpG-"},"outputs":[],"source":["def concatenate_representations(dict1, dict2):\n","    concatenated = {}\n","    for key in set(dict1.keys()) | set(dict2.keys()):  # union of keys from both dicts\n","        list1 = dict1.get(key, [])\n","        list2 = dict2.get(key, [])\n","\n","        # Concatenate the lists\n","        concatenated_list = list1 + list2\n","\n","        concatenated[key] = concatenated_list\n","\n","    return concatenated\n","\n","# Example usage\n","\n","concatenated_hourly_JPL = concatenate_representations(hourly_representations_JPL, hourly_representations_duration_JPL)\n","concatenated_half_hourly_JPL = concatenate_representations(half_hourly_representations_JPL, half_hourly_representations_duration_JPL)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NT77SWE7N5xN"},"source":["#Calculate correlations - choose one of the two\n","Run one of the two cells as the results have the same name"]},{"cell_type":"markdown","metadata":{"id":"KFbalDPVN7vE"},"source":["##Calculate cosine similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tkQQql47Ultv"},"outputs":[],"source":["from sklearn.metrics import pairwise_distances\n","import pandas as pd\n","\n","def data_to_dataframe(data):\n","    return pd.DataFrame.from_dict(data, orient='index')\n","\n","def compute_cosine_similarity(df):\n","    cosine_distance = pairwise_distances(df, metric='cosine')\n","    cosine_similarity = 1 - cosine_distance\n","\n","    # Set the user IDs as row and column names to preserve them\n","    cosine_similarity_df = pd.DataFrame(cosine_similarity, index=df.index, columns=df.index)\n","    return cosine_similarity_df\n","\n","# Compute cosine similarity for each concatenated dataset\n","df_hourly_JPL = data_to_dataframe(concatenated_hourly_JPL)\n","similarities_hourly_JPL = compute_cosine_similarity(df_hourly_JPL)\n","\n","df_half_hourly_JPL = data_to_dataframe(concatenated_half_hourly_JPL)\n","similarities_half_hourly_JPL = compute_cosine_similarity(df_half_hourly_JPL)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_4dA-332OAB1"},"source":["##Calculate Pearson correlation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ZUpqfjarhtj"},"outputs":[],"source":["import numpy as np\n","\n","def compute_pearson_correlation(v1, v2):\n","    # Handling cases where vectors are too short or contain constant values\n","    if len(v1) > 1 and len(v2) > 1 and np.std(v1) * np.std(v2) != 0:\n","        return np.corrcoef(v1, v2)[0, 1]\n","    else:\n","        return None  # Returning None for cases where correlation is not defined\n","\n","import pandas as pd\n","\n","def compute_pearson_correlation_matrix_df(representations):\n","    user_ids = list(representations.keys())\n","    n = len(user_ids)\n","    correlation_matrix = np.zeros((n, n))\n","\n","    for i in range(n):\n","        for j in range(i, n):  # Compute only for one triangle and mirror it as the matrix is symmetric\n","            if i != j:\n","                corr = compute_pearson_correlation(representations[user_ids[i]], representations[user_ids[j]])\n","                correlation_matrix[i, j] = corr if corr is not None else 0\n","                correlation_matrix[j, i] = correlation_matrix[i, j]  # Mirror the value\n","            else:\n","                correlation_matrix[i, j] = 1  # Self-correlation is always 1\n","\n","    # Convert to DataFrame for better usability\n","    correlation_df = pd.DataFrame(correlation_matrix, index=user_ids, columns=user_ids)\n","    return correlation_df\n","\n","similarities_hourly_JPL = compute_pearson_correlation_matrix_df(concatenated_hourly_JPL)\n","similarities_half_hourly_JPL = compute_pearson_correlation_matrix_df(concatenated_half_hourly_JPL)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pYVRigKn0xSo"},"outputs":[],"source":["from scipy.stats import spearmanr\n","import pandas as pd\n","\n","def data_to_dataframe(data):\n","    return pd.DataFrame.from_dict(data, orient='index')\n","\n","def calculate_spearman_correlation(df):\n","    # Transpose the DataFrame to make users (rows) into columns\n","    df_transposed = df.T\n","\n","    # Initialize an empty DataFrame to store the results\n","    spearman_corr_df = pd.DataFrame(index=df.index, columns=df.index)\n","\n","    # Calculate Spearman correlation for each pair of users (now columns)\n","    for user1 in df_transposed.columns:\n","        for user2 in df_transposed.columns:\n","            # Calculate Spearman correlation\n","            corr, _ = spearmanr(df_transposed[user1], df_transposed[user2])\n","            spearman_corr_df.loc[user1, user2] = corr\n","\n","    # Convert all values to numeric\n","    return spearman_corr_df.apply(pd.to_numeric)\n","\n","# Compute Spearman correlation for each concatenated dataset\n","df_hourly_JPL = data_to_dataframe(concatenated_hourly_JPL)\n","similarities_hourly_JPL = calculate_spearman_correlation(df_hourly_JPL)\n","\n","df_half_hourly_JPL = data_to_dataframe(concatenated_half_hourly_JPL)\n","similarities_half_hourly_JPL = calculate_spearman_correlation(df_half_hourly_JPL)\n"]},{"cell_type":"markdown","metadata":{"id":"nKBtKfZWOEXk"},"source":["#Calculated most correlated users (using raw values)\n","Run one of the two cells as the results have the same name - raw values perform much better"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xD2V-KFHWosG"},"outputs":[],"source":["#Using threshold\n","def most_correlated_users(matrix,cap,threshold=0.65):\n","    \"\"\"\n","    For each user, find the top correlated users with correlations above 0.5.\n","\n","    :param matrix: Cosine similarity matrix.\n","    :param threshold: Minimum correlation threshold for considering a user as correlated.\n","    :return: Dictionary with keys being user IDs and values being lists of top correlated user IDs.\n","    \"\"\"\n","    result = {}\n","\n","    for user_index in matrix.index:\n","        # Subtract self-similarity value for the current user\n","        user_similarities = matrix.loc[user_index].drop(user_index)\n","        # Filter users with correlation above the threshold\n","        filtered_users = user_similarities[abs(user_similarities) > threshold]\n","\n","        # Sort by descending correlation\n","        sorted_users = filtered_users.sort_values(ascending=False)\n","\n","        # Append users above the threshold to the result\n","        result[user_index] = sorted_users.index.tolist()\n","\n","    return result\n","\n","# Assuming `similarities_hourly_cal` is your cosine similarity matrix for the entire dataset\n","\n","# Similarly for JPL data\n","cap=5\n","most_corr_users_hourly_JPL = most_correlated_users(similarities_hourly_JPL,cap)\n","most_corr_users_half_hourly_JPL = most_correlated_users(similarities_half_hourly_JPL,cap)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":503},"executionInfo":{"elapsed":170,"status":"ok","timestamp":1715572783114,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"5wj0F-APeVv1","outputId":"5e294e1c-6ac7-4a6a-dd93-a2251f3e1e06"},"outputs":[],"source":["similarities_hourly_JPL"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1715572785141,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"o1Lml8060GaP","outputId":"1a7fd4e0-5adb-4649-d8c2-c6f5f68ddcb2"},"outputs":[],"source":["most_corr_users_hourly_JPL"]},{"cell_type":"markdown","metadata":{"id":"AGkdPoFWZPcM"},"source":["#Mode - Stay duration prediction (no correlation)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jjs3DlxMZSp8"},"outputs":[],"source":["#Mode calculation with linear interpolation\n","from collections import Counter\n","def compute_mode_per_user_per_interval(data):\n","    \"\"\"Compute the mode per user per time interval with linear interpolation.\"\"\"\n","    user_groups = {}\n","    mode_per_user_per_interval = {}\n","\n","    # Group by user\n","    for row in data:\n","        if row[2] not in user_groups:\n","            user_groups[row[2]] = []\n","        user_groups[row[2]].append(row)\n","\n","    # Calculate modes per user\n","    for user, entries in user_groups.items():\n","        time_intervals = sorted(list(set([entry[0] for entry in entries])))\n","        modes = {}\n","        for interval in time_intervals:\n","            session_values = [entry[3] for entry in entries if entry[0] == interval]\n","            counts = Counter(session_values)\n","            highest_freq = max(counts.values())\n","            common_vals = [key for key, val in counts.items() if val == highest_freq]\n","\n","            # If there's a single mode, use it; else, average them\n","            modes[interval] = sum(common_vals) / len(common_vals)\n","\n","        # Linearly interpolate for missing modes\n","        all_intervals = sorted(list(set(data[:, 0])))\n","        for i, interval in enumerate(all_intervals):\n","            if interval not in modes:\n","                # Find previous and next known modes\n","                prev_mode = next((modes[prev_int] for prev_int in reversed(all_intervals[:i]) if prev_int in modes), None)\n","                next_mode = next((modes[next_int] for next_int in all_intervals[i+1:] if next_int in modes), None)\n","\n","                # If both previous and next modes exist, interpolate\n","                if prev_mode is not None and next_mode is not None:\n","                    gap_size = all_intervals[i+1:].index(next((next_int for next_int in all_intervals[i+1:] if next_int in modes))) + 1\n","                    increment = (next_mode - prev_mode) / (gap_size + 1)\n","                    modes[interval] = prev_mode + increment\n","\n","                # If no next mode exists, keep the mode same as the previous mode\n","                elif prev_mode is not None:\n","                    modes[interval] = prev_mode\n","\n","                # If no previous mode exists, keep the mode same as the next mode\n","                elif next_mode is not None:\n","                    modes[interval] = next_mode\n","\n","        mode_per_user_per_interval[user] = modes\n","\n","    return mode_per_user_per_interval\n","\n","# Update mode_per_user_per_interval using the new function\n","mode_per_user_per_interval_JPL_interpolate = compute_mode_per_user_per_interval(sorted_data_JPL_training)"]},{"cell_type":"markdown","metadata":{"id":"QIHn62MOuwqz"},"source":["#MLR Stay Duration Prediction for all users (no correlation)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Dta-nKctzVj"},"outputs":[],"source":["import numpy as np\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","\n","# Sample training data\n","import numpy as np\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","\n","\n","def mlr_model(train_data, test_data, user_id):\n","    # Filter training and testing data for the specific user\n","    user_train_data = train_data[train_data[:, 3] == user_id]\n","    user_test_data = test_data[test_data[:, 3] == user_id]\n","\n","    # Independent variables are arrival time, day of the week\n","    X_train = user_train_data[:, :2]\n","    X_test = user_test_data[:, :2]\n","\n","    # Dependent variable is the energy\n","    y_train = user_train_data[:, 4]\n","    y_test = user_test_data[:, 4]\n","\n","    # Train the model\n","    model = LinearRegression().fit(X_train, y_train)\n","\n","    # Predict on the test set\n","    y_pred = model.predict(X_test)\n","\n","    # Calculate user SMAPE\n","    n = len(y_test)\n","    smape_val = (1/ n) * np.sum(np.abs(y_test - y_pred) / (np.abs(y_test+y_pred)))*100\n","\n","    return model\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uxP2njavuRcQ"},"outputs":[],"source":["MLR_user_models={}\n","user_ids_JPL = np.unique(np.concatenate((JPL_train[:, 3], JPL_test[:, 3])))\n","\n","# Loop through each user ID in the JPL dataset\n","for user_id in user_ids_JPL:\n","    model = mlr_model(JPL_train, JPL_test, user_id)\n","    MLR_user_models[user_id]=model\n"]},{"cell_type":"markdown","metadata":{"id":"keUtdWxOpLtm"},"source":["#Train DKDE models for all users"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6861,"status":"ok","timestamp":1715572801814,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"m0Jt5_L-pc1T","outputId":"e9a02f32-6256-4208-e7d3-f7349cb3ca8c"},"outputs":[],"source":["! pip install KDE-diffusion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N-0a4AmtpRZv"},"outputs":[],"source":["import numpy as np\n","from kde_diffusion import kde2d\n","user_ids_JPL = np.unique(np.concatenate((JPL_train[:, 3], JPL_test[:, 3])))\n","\n","class KDEModel:\n","    def __init__(self, grid, density):\n","        self.grid = grid\n","        self.density = density\n","\n","    def predict(self, X):\n","        predictions = [self._predict_y_given_x(x) for x in X]\n","        return predictions\n","\n","    def _predict_y_given_x(self, new_x):\n","        # Find the closest x index\n","        x_idx = np.argmin(np.abs(self.grid[0] - new_x))\n","\n","        # Get the y values and their corresponding densities for the given x\n","        y_values = self.grid[1]\n","        y_densities = self.density[x_idx]\n","\n","        # Find the y with the maximum density\n","        predicted_y = y_values[np.argmax(y_densities)]\n","\n","        return predicted_y\n","\n","def dkde_model(train_data, user_id):\n","    # Filter training data for the specific user\n","    user_train_data = train_data[train_data[:, 3] == user_id]\n","\n","    # Extract columns for stay duration and energy consumption\n","    X_train = user_train_data[:, 0]  # Stay duration\n","    Y_train = user_train_data[:, 4]  # Energy consumption\n","\n","    # Perform KDE\n","    density, grid, bandwidth = kde2d(X_train, Y_train, n=64, limits=None)\n","\n","    # Create and return a KDE model instance\n","    kde_model = KDEModel(grid, density)\n","    return kde_model\n","\n","DKDE_user_models = {user_id: dkde_model(JPL_train, user_id) for user_id in user_ids_JPL}\n"]},{"cell_type":"markdown","metadata":{"id":"lrdOzurx--A-"},"source":["#Train Decision Tree model for all users"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TA2X-H94_A-A"},"outputs":[],"source":["import numpy as np\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.preprocessing import StandardScaler\n","\n","def train_user_model(train_data, user_id,depth_range=(1, 21), min_samples_split_range=(2, 11), cv_folds=5):\n","    user_train_data = train_data[train_data[:, 3] == user_id]\n","    X_train = user_train_data[:, 0].reshape(-1, 1)  # Arrival time\n","    y_train = user_train_data[:, 4]  # Stay duration\n","\n","    # Set up grid search with cross-validation for Decision Tree\n","    param_grid = {\n","        'max_depth': list(range(depth_range[0], depth_range[1])),\n","        'min_samples_split': list(range(min_samples_split_range[0], min_samples_split_range[1]))\n","    }\n","    dt = DecisionTreeRegressor()\n","    grid_search = GridSearchCV(dt, param_grid, cv=cv_folds, scoring='neg_mean_squared_error', return_train_score=True)\n","    grid_search.fit(X_train, y_train)\n","\n","    # Train the model using the optimal k\n","    model = grid_search.best_estimator_\n","\n","\n","    return model\n","\n","DT_user_models = {user_id: train_user_model(JPL_train, user_id) for user_id in user_ids_JPL}"]},{"cell_type":"markdown","metadata":{"id":"875NcYigGBxC"},"source":["#Train SVR models for all users"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uX1o1Eu_F3eW"},"outputs":[],"source":["from sklearn.svm import SVR\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import mean_squared_error\n","def train_user_model(train_data, user_id, k=5):\n","    user_train_data = train_data[train_data[:, 3] == user_id]\n","    X_train = user_train_data[:, 0].reshape(-1, 1)  # Arrival time\n","    y_train = user_train_data[:, 4]  # Stay duration\n","\n","   # Define the hyperparameters to be optimized\n","    param_grid = {\n","        'C': [0.1, 1, 10, 100],\n","        'epsilon': [0.001, 0.01, 0.1, 1],\n","        'gamma': ['scale', 'auto', 0.1, 1, 10]\n","    }\n","\n","    # Initialize SVR with RBF kernel\n","    svr_rbf = SVR(kernel='rbf')\n","\n","    # Initialize GridSearchCV with k-fold cross-validation\n","    grid_search = GridSearchCV(estimator=svr_rbf, param_grid=param_grid, cv=k, scoring='neg_mean_squared_error', n_jobs=-1)\n","\n","    # Fit the model to the training data\n","    grid_search.fit(X_train, y_train)\n","\n","    model = grid_search.best_estimator_\n","\n","    return model\n","svr_models_JPL = {user_id: train_user_model(JPL_train, user_id) for user_id in user_ids_JPL}"]},{"cell_type":"markdown","metadata":{"id":"SdhWQhbbO12-"},"source":["#Enseble ML Algorithm Stay Duration (with correlations)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IzcTbEsKsv54"},"outputs":[],"source":["R_SD={'169': 2.2551554266668488,\n"," '171': 3.7531759242694362,\n"," '176': 3.033506931058353,\n"," '220': 4.157056240805807,\n"," '322': 2.929926450352931,\n"," '334': 1.5403402896510696,\n"," '335': 4.182440356147159,\n"," '346': 4.63088280381524,\n"," '365': 4.612058546331858,\n"," '368': 3.8637314436567745,\n"," '372': 3.4779871035895753,\n"," '374': 3.7627475450103334,\n"," '378': 3.61759981491824,\n"," '382': 4.358454020839778,\n"," '404': 4.285879042160351,\n"," '405': 4.760574930778141,\n"," '406': 1.1510712960433436,\n"," '409': 3.3539069505133905,\n"," '410': 4.301005502712871,\n"," '416': 4.199233283013298,\n"," '436': 4.091891479434165,\n"," '444': 3.529125348909349,\n"," '458': 4.060001379997106,\n"," '467': 4.019928954004856,\n"," '474': 3.2450469879409285,\n"," '476': 4.393288833959378,\n"," '481': 4.429827240449028,\n"," '483': 3.079068807362642,\n"," '507': 4.336149365824373,\n"," '526': 2.4205249888839466,\n"," '531': 3.9517493146670986,\n"," '537': 3.1479021761196098,\n"," '551': 4.881572582872202,\n"," '553': 3.0305502893037675,\n"," '576': 3.628599820757222,\n"," '577': 4.319531561288392,\n"," '581': 3.4796093044547454,\n"," '592': 3.1009000797824084,\n"," '607': 4.831651402991986,\n"," '651': 3.6013651820057837,\n"," '726': 3.7379880870161575,\n"," '742': 4.150297519178871,\n"," '826': 3.710092899316145,\n"," '933': 4.41718644718856}\n","\n","R_SD = {int(key): value for key, value in R_SD.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQ-88mdRsEyv"},"outputs":[],"source":["def ensemble_algorithm_with_correlation(train_data, test_data,test_data_mode, user_id,most_corr_users, mlr_user_model, DT_user_model,similarity_matrix, R_SD):\n","    # Filter training and testing data for the specific user\n","    user_test_data = test_data[test_data[:, 3] == user_id]\n","    user_test_data_mode = test_data_mode[test_data_mode[:, 2] == user_id]\n","\n","    X_test_mlr = user_test_data[:, :2]\n","    y_test = user_test_data[:, 4]\n","    #y_test=user_test_data_mode[:, 3]\n","    X_test_dt = user_test_data[:, 0].reshape(-1, 1)\n","\n","    # Choose and use the model based on the R_SD ratio for the main user\n","    if R_SD.get(user_id, 0) < 4.5:\n","        user_model =mlr_user_model[user_id]\n","        y_pred = user_model.predict(X_test_mlr)\n","    else:\n","        user_model = DT_user_model[user_id]\n","        y_pred = user_model.predict(X_test_dt)\n","\n","    # Convert y_pred to a NumPy array if it is a list\n","    y_pred = np.array(y_pred) if isinstance(y_pred, list) else y_pred\n","\n","    # Adjust the prediction based on the most correlated users\n","    total_correlation = 1\n","    if user_id in most_corr_users:\n","        correlated_users = most_corr_users[user_id]\n","\n","        for other_user_id in correlated_users:\n","            if other_user_id in mlr_user_model or other_user_id in DT_user_model:\n","                correlation = similarity_matrix.loc[user_id, other_user_id]\n","\n","                # Choose the model for the correlated user based on R_SD\n","                if R_SD.get(other_user_id, 0) < 4.5:\n","                    other_user_model = mlr_user_model[other_user_id]\n","                    other_user_pred = other_user_model.predict(X_test_mlr)\n","                else:\n","                    other_user_model = DT_user_model[other_user_id]\n","                    other_user_pred = other_user_model.predict(X_test_dt)\n","\n","                # Convert other_user_pred to a NumPy array if it's a list\n","                other_user_pred = np.array(other_user_pred) if isinstance(other_user_pred, list) else other_user_pred\n","\n","                # Ensure y_pred is an array before the operation\n","                if not isinstance(y_pred, np.ndarray):\n","                    y_pred = np.array(y_pred)\n","\n","                y_pred += other_user_pred * correlation\n","                total_correlation += correlation\n","\n","    # Ensure y_pred is an array before the division\n","    if not isinstance(y_pred, np.ndarray):\n","        y_pred = np.array(y_pred)\n","\n","    if total_correlation != 0:\n","        y_pred /= total_correlation\n","\n","    # Calculate user SMAPE\n","    n = len(y_test)\n","    smape_val = (1 / n) * np.sum(np.abs(y_test - y_pred) / (np.abs(y_test) + np.abs(y_pred))) * 100\n","\n","    return smape_val\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"executionInfo":{"elapsed":465,"status":"error","timestamp":1715524077314,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"5-nYe30czfKN","outputId":"67230998-bc1f-44e4-b80e-fccf03b0c665"},"outputs":[],"source":["user_ids_JPL = np.unique(np.concatenate((JPL_train[:, 3], JPL_test[:, 3])))\n","\n","smape_list_JPL=[]\n","\n","for user_id in user_ids_JPL:\n","    smape = ensemble_algorithm_with_correlation(JPL_train, JPL_test,sorted_data_JPL_testing, user_id, most_corr_users_hourly_JPL, MLR_user_models,DT_user_models,similarities_hourly_JPL,R_SD)\n","    smape_list_JPL.append(smape)\n","\n","# Calculate the average SMAPE\n","average_smape = np.mean(smape_list_JPL) if smape_list_JPL else 0\n","\n","print(f\"Average SMAPE for the dataset: {average_smape:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1713302306463,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"Emc2aFCIsPUR","outputId":"beac9722-8985-47ee-ba3a-c36201813a2b"},"outputs":[],"source":["import numpy as np\n","\n","# Assuming you already have the variables and functions defined as per your code snippet\n","user_ids_JPL = np.unique(np.concatenate((JPL_train[:, 3], JPL_test[:, 3])))\n","\n","# Initialize an empty dictionary to store userID as key and SMAPE as value\n","smape_dict_JPL = {}\n","\n","for user_id in user_ids_JPL:\n","    smape = ensemble_algorithm_with_correlation(JPL_train, JPL_test,sorted_data_JPL_testing, user_id, most_corr_users_hourly_JPL, MLR_user_models,DT_user_models,similarities_hourly_JPL,R_SD)\n","    smape_dict_JPL[user_id] = smape  # Store the SMAPE in the dictionary with user_id as key\n","\n","# Calculate the average SMAPE using the dictionary values\n","average_smape = np.mean(list(smape_dict_JPL.values())) if smape_dict_JPL else 0\n","\n","print(f\"Average SMAPE for the dataset: {average_smape:.2f}%\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1713302306464,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"v4Xqug5iskN3","outputId":"3e035e30-6b01-411a-bab1-86fd1c0f4233"},"outputs":[],"source":["import numpy as np\n","\n","# Assuming userIDs are at index 3 and no_sessions is at the correct index (let's assume it's at index -1 for this example)\n","user_ids = JPL_train[:, 3]\n","no_sessions = JPL_train[:, -1]  # Replace -1 with the actual index of no_sessions\n","\n","# Filter userIDs where no_sessions is greater than 30\n","# Note: Ensure no_sessions is of type int if not, convert it using no_sessions.astype(int)\n","user_ids_more_than_30_sessions = user_ids[no_sessions.astype(int) >= 30]\n","\n","# Since user_ids can have duplicates, get unique userIDs\n","unique_user_ids_more_than_30_sessions = np.unique(user_ids_more_than_30_sessions)\n","\n","# Convert to list if needed\n","unique_user_ids_more_than_30_sessions_list = unique_user_ids_more_than_30_sessions.tolist()\n","\n","# Now unique_user_ids_more_than_30_sessions_list contains the userIDs with more than 30 sessions\n","print(unique_user_ids_more_than_30_sessions_list)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":187,"status":"ok","timestamp":1713302306646,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"ug9Lb8bcspa2","outputId":"59d10eec-f266-4f7d-ac1c-523775351389"},"outputs":[],"source":["# Assuming smape_dict_JPL is the dictionary with userID as key and SMAPE as value\n","# And unique_user_ids_more_than_30_sessions_list contains the list of userIDs\n","\n","# Initialize an empty list to store SMAPE values for users with more than 30 sessions\n","smape_values_for_selected_users = []\n","\n","# Loop through each userID in the list and get their SMAPE from the dictionary\n","for user_id in unique_user_ids_more_than_30_sessions_list:\n","    if user_id in smape_dict_JPL:\n","        smape_values_for_selected_users.append(smape_dict_JPL[user_id])\n","    else:\n","        print(f\"No SMAPE found for userID {user_id}\")\n","\n","# If you want to calculate the average SMAPE for these users\n","if smape_values_for_selected_users:\n","    average_smape_selected_users = np.mean(smape_values_for_selected_users)\n","    print(f\"Average SMAPE for users with more than 30 sessions: {average_smape_selected_users:.2f}%\")\n","else:\n","    print(\"No SMAPE values found for the selected users.\")\n"]},{"cell_type":"markdown","metadata":{"id":"nKtsFeOdg2GM"},"source":["#Ensemble machine learning with mode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8AWCh82Ng5IL"},"outputs":[],"source":["# for user_id, time_slots in mode_per_user_per_interval_JPL_interpolate.items():\n","#     for time_slot, mode_value in time_slots.items():\n","#         # Divide each mode value by 60 and update the dictionary\n","#         mode_per_user_per_interval_JPL_interpolate[user_id][time_slot] = mode_value\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3J3QfSgguOej"},"outputs":[],"source":["import datetime\n","def generate_15min_intervals():\n","    intervals = []\n","    for hour in range(24):\n","        for minute in [0, 15, 30, 45]:\n","            intervals.append(f'{hour:02d}:{minute:02d}')\n","    return intervals\n","\n","all_intervals = generate_15min_intervals()\n","\n","def interpolate_modes(user_modes, all_intervals):\n","    existing_times = [datetime.datetime.strptime(time, '%H:%M') for time in user_modes.keys()]\n","    existing_values = list(user_modes.values())\n","\n","    # Convert times to minutes since midnight for interpolation\n","    existing_minutes = [(time.hour * 60 + time.minute) for time in existing_times]\n","    all_minutes = [(datetime.datetime.strptime(time, '%H:%M').hour * 60 + datetime.datetime.strptime(time, '%H:%M').minute) for time in all_intervals]\n","\n","    interpolated_values = np.interp(all_minutes, existing_minutes, existing_values)\n","\n","    # Map interpolated values back to time slots\n","    return dict(zip(all_intervals, interpolated_values))\n","\n","# Interpolate for each user\n","for user_id, user_modes in mode_per_user_per_interval_JPL_interpolate.items():\n","    mode_per_user_per_interval_JPL_interpolate[user_id] = interpolate_modes(user_modes, all_intervals)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HD0SZ4DzAxwu"},"outputs":[],"source":["import numpy as np\n","import datetime\n","\n","def interpolate_modes_full(user_modes, all_intervals):\n","    if not user_modes:  # Check if user_modes is empty\n","        return dict.fromkeys(all_intervals, 0)  # Return a dictionary with 0 for all intervals if no modes\n","\n","    # Parse the times and modes from the user data\n","    existing_times = [datetime.datetime.strptime(time, '%H:%M') for time in user_modes.keys()]\n","    existing_values = list(user_modes.values())\n","\n","    # Find the earliest and latest times for the user\n","    earliest_time = min(existing_times)\n","    latest_time = max(existing_times)\n","\n","    # Initialize dictionaries to hold the full set of modes\n","    full_modes = {}\n","\n","    # Repeat the earliest real value for times before the first observation\n","    for interval in all_intervals:\n","        interval_time = datetime.datetime.strptime(interval, '%H:%M')\n","        if interval_time < earliest_time:\n","            full_modes[interval] = existing_values[0]  # Use the first (earliest) mode value\n","        elif interval_time > latest_time:\n","            full_modes[interval] = existing_values[-1]  # Use the last (latest) mode value\n","\n","    # Interpolate between the first and last observations\n","    existing_minutes = [(time.hour * 60 + time.minute) for time in existing_times]\n","    all_minutes = [(datetime.datetime.strptime(time, '%H:%M').hour * 60 + datetime.datetime.strptime(time, '%H:%M').minute) for time in all_intervals if datetime.datetime.strptime(time, '%H:%M') >= earliest_time and datetime.datetime.strptime(time, '%H:%M') <= latest_time]\n","\n","    interpolated_values = np.interp(all_minutes, existing_minutes, existing_values)\n","\n","    # Map interpolated values to their corresponding time slots\n","    for time, value in zip(all_intervals, interpolated_values):\n","        if datetime.datetime.strptime(time, '%H:%M') >= earliest_time and datetime.datetime.strptime(time, '%H:%M') <= latest_time:\n","            full_modes[time] = value\n","\n","    return full_modes\n","\n","# Interpolate for each user, assuming mode_per_user_per_interval_JPL_interpolate is defined\n","for user_id, user_modes in mode_per_user_per_interval_JPL_interpolate.items():\n","    mode_per_user_per_interval_JPL_interpolate[user_id] = interpolate_modes_full(user_modes, all_intervals)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ga3Bv9cfhNR6"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","# Assuming mode_per_user_per_interval_caltech_interpolate is your prediction dataset\n","# and correlation_matrix is your correlation matrix\n","\n","def weighted_average_prediction(user_id, time_slot, mode_per_user_per_interval, most_corr_users, correlation_matrix):\n","    # Initialize weighted predictions with the user's own prediction or a default value\n","    user_prediction = mode_per_user_per_interval.get(user_id, {}).get(time_slot, 0)  # You can change 0 to a default value if needed\n","    weighted_predictions = user_prediction\n","    total_correlation=1\n","    # Add weighted predictions of the most correlated users\n","    for corr_user_id in most_corr_users[user_id]:\n","        corr_value = correlation_matrix.loc[user_id, corr_user_id]\n","        corr_user_prediction = mode_per_user_per_interval.get(corr_user_id, {}).get(time_slot, 0)  # You can change 0 to a default value if needed\n","\n","        # Add to weighted predictions\n","        weighted_predictions += corr_user_prediction * corr_value\n","        total_correlation+=corr_value\n","    weighted_predictions=weighted_predictions/total_correlation\n","    return weighted_predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":342,"status":"ok","timestamp":1715572823944,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"NNm0KX5uhPd0","outputId":"ac2a6f72-daf5-4575-83bb-a0c22a68ad6b"},"outputs":[],"source":["from collections import defaultdict\n","\n","# Initialize dictionaries to store true and predicted values for each user\n","y_true_by_user = defaultdict(list)\n","y_pred_by_user = defaultdict(list)\n","\n","# Function to calculate SMAPE\n","def calculate_smape(y_true, y_pred):\n","    \"\"\"Compute the SMAPE\"\"\"\n","    n = len(y_true)\n","    if n == 0:\n","        return np.nan  # Return NaN if there are no observations\n","    smape_val = (1/n) * np.sum(np.abs(y_pred - y_true) / (np.abs(y_true + y_pred))) * 100\n","    return smape_val\n","\n","# Iterate over the testing data to fill the dictionaries\n","for row in sorted_data_JPL_testing:\n","    user_id = row[2]  # Assuming user_id is at index 2\n","    time_slot = row[0]  # Assuming time_slot is at index 0\n","\n","    if time_slot in mode_per_user_per_interval_JPL_interpolate.get(user_id, {}):\n","        y_true_by_user[user_id].append(row[3])  # true_value is assumed to be at index 3\n","\n","        # Calculate the weighted prediction\n","        weighted_pred = weighted_average_prediction(user_id, time_slot, mode_per_user_per_interval_JPL_interpolate, most_corr_users_hourly_JPL, similarities_hourly_JPL)\n","\n","        # If weighted_pred is 0 (i.e., no data available), use user_prediction as a fallback\n","        user_prediction = mode_per_user_per_interval_JPL_interpolate[user_id][time_slot]\n","        y_pred_by_user[user_id].append(weighted_pred if weighted_pred != 0 else user_prediction)\n","\n","# Calculate SMAPE for each user\n","smape_values = []\n","for user_id in y_true_by_user:\n","    y_true = np.array(y_true_by_user[user_id], dtype=float)\n","    print(len(y_true))\n","    y_pred = np.array(y_pred_by_user[user_id], dtype=float)\n","    print(len(y_pred))\n","    user_smape = calculate_smape(y_true, y_pred)\n","    if not np.isnan(user_smape):  # Exclude users without predictions\n","        smape_values.append(user_smape)\n","        print(f'User {user_id} SMAPE: {user_smape:.2f}%')\n","\n","# Calculate and print the average SMAPE across users\n","if smape_values:\n","    average_smape = np.mean(smape_values)\n","    print(f'Average SMAPE across users: {average_smape:.2f}%')\n","else:\n","    print('No SMAPE values calculated.')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":141,"status":"ok","timestamp":1715572833796,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"JSBDHKU29fOE","outputId":"23c043b0-25bf-465a-c533-ffaf66c6c7bc"},"outputs":[],"source":["# Given array\n","array = JPL_test\n","\n","\n","# Function to convert hours to HH:MM format, rounding down to the nearest 15 minutes\n","def hours_to_hhmm_rounded_down(hours):\n","    total_minutes = hours * 60\n","    rounded_minutes = total_minutes - (total_minutes % 15)  # Round down to the nearest 15 minutes\n","    hours_part = int(rounded_minutes // 60)\n","    minutes_part = int(rounded_minutes % 60)\n","    return f\"{hours_part:02d}:{minutes_part:02d}\"\n","\n","# Apply the function to the first column of the array and create a new column with the results\n","hhmm_column = np.array([hours_to_hhmm_rounded_down(hours) for hours in array[:, 0]])\n","\n","array_str = array.astype(str)\n","hhmm_column = hhmm_column.reshape(-1, 1)  # Reshape for concatenation\n","\n","# Concatenate the original array with the new HH:MM column\n","modified_JPL_test= np.concatenate((array_str, hhmm_column), axis=1)\n","\n","modified_JPL_test\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQROB2-pgDmG"},"outputs":[],"source":["def ensemble_algorithm_with_correlation(train_data, test_data, test_data_mode, user_id, most_corr_users, dkde_user_model, mode_per_user_per_interval_JPL_interpolate, similarity_matrix, R_SD):\n","    # Filter training and testing data for the specific user\n","    user_test_data = test_data[test_data[:, 3] == user_id]\n","    user_test_data_mode = test_data_mode[test_data_mode[:, 2] == user_id]\n","\n","    time_slots = list(mode_per_user_per_interval_JPL_interpolate[user_id].keys())\n","    X_test_dkde_full=[]\n","    for row in user_test_data:\n","      if row[-1] in time_slots:\n","        X_test_dkde_full.append(row)\n","    #print(X_test_dkde_full)\n","    # X_test_dkde=[]\n","    # for row in test_data_mode:\n","    #   time_slot = row[0]  # Assuming time_slot is at index 0\n","    #   if time_slot in mode_per_user_per_interval_JPL_interpolate.get(user_id, {}):\n","    #     if row[2]==user_id:\n","    #       X_test_dkde.append(row[0])  # true_value is assumed to be at index 3\n","    X_test_dkde = np.array([row[0] for row in X_test_dkde_full],dtype='float64')\n","    #X_test_dkde=X_test_dkde_full[:, 0]\n","    X_test_mlr=user_test_data[:, :2]\n","    y_test = user_test_data[:, 4]\n","    y_test_1=user_test_data_mode[:,3]\n","    y_pred_list = []  # Initialize as a list\n","    # Choose and use the model based on the R_SD ratio for the main user\n","    if R_SD.get(user_id, 0) <2:\n","        user_model = dkde_user_model[user_id]\n","        y_pred = user_model.predict(X_test_dkde)\n","        y_pred = [x * 60 for x in y_pred]\n","        y_pred_list.extend(y_pred)  # Add predictions to the list\n","    else:\n","        for row in user_test_data_mode:\n","            time_slot = row[0]\n","            if time_slot in mode_per_user_per_interval_JPL_interpolate.get(user_id, {}):\n","                user_prediction = mode_per_user_per_interval_JPL_interpolate[user_id][time_slot]\n","                y_pred_list.append(user_prediction)  # Add mode prediction to the list\n","    y_pred = np.array(y_pred_list)  # Convert list to NumPy array\n","    #print(len(y_pred))\n","    # Adjust the prediction based on the most correlated users\n","    total_correlation = 1\n","    if user_id in most_corr_users:\n","        for other_user_id in most_corr_users[user_id]:\n","            other_user_pred_list = []  # Initialize as a list\n","\n","            if other_user_id in dkde_user_model:\n","                correlation = similarity_matrix.loc[user_id, other_user_id]\n","                if R_SD.get(other_user_id, 0) < 2:\n","                    other_user_model = dkde_user_model[other_user_id]\n","                    other_user_pred = other_user_model.predict(X_test_dkde)\n","                    other_user_pred = [x * 60 for x in other_user_pred]\n","                    other_user_pred_list.extend(other_user_pred)\n","                else:\n","                    for row in user_test_data_mode:\n","                        time_slot = row[0]\n","                        if time_slot in mode_per_user_per_interval_JPL_interpolate.get(other_user_id, {}):\n","                            other_user_prediction = mode_per_user_per_interval_JPL_interpolate[other_user_id][time_slot]\n","                            other_user_pred_list.append(other_user_prediction)\n","                other_user_pred = np.array(other_user_pred_list)  # Convert list to NumPy array\n","                y_pred += other_user_pred * correlation\n","                total_correlation += correlation\n","\n","    if total_correlation != 0:\n","        y_pred /= total_correlation\n","\n","    # Calculate user SMAPE\n","    #print(\"Pred size\",len(y_pred))\n","    y_true=[]\n","    for row in test_data_mode:\n","      time_slot = row[0]  # Assuming time_slot is at index 0\n","      if time_slot in mode_per_user_per_interval_JPL_interpolate.get(user_id, {}):\n","        if row[2]==user_id:\n","          y_true.append(row[3])  # true_value is assumed to be at index 3\n","    #print(\"True Size\",len(y_true))\n","    n = len(y_true)\n","    print(len(y_true))\n","    print(len(y_pred))\n","    # if R_SD.get(user_id, 0) <3:\n","    #   smape_val = (1 / n) * np.sum(np.abs(y_test- y_pred) / (np.abs(y_test) + np.abs(y_pred))) * 100\n","    # else:\n","    #   smape_val = (1 / n) * np.sum(np.abs(y_test_1- y_pred) / (np.abs(y_test_1 + y_pred))) * 100\n","    smape_val = (1 / n) * np.sum(np.abs(y_true - y_pred) / (np.abs(y_true+y_pred))) * 100\n","    return smape_val,total_correlation,y_pred,y_true\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1woKgVZrtfUw"},"outputs":[],"source":["user_test_data = sorted_data_JPL_testing[sorted_data_JPL_testing[:, 2] == 365]\n","\n","time_slots = list(mode_per_user_per_interval_JPL_interpolate[365].keys())\n","X_test_dkde_full=[]\n","for row in user_test_data:\n","  if row[0] in time_slots:\n","    X_test_dkde_full.append(row)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":160,"status":"ok","timestamp":1715524126694,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"sazOM2_iVAWj","outputId":"247c28d2-ac84-45de-db06-0a10d90ae161"},"outputs":[],"source":["len(X_test_dkde_full)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yyPYmgq3YqeI"},"outputs":[],"source":["converted_array = np.array([\n","    [float(value) if index < len(row) - 1 else value\n","     for index, value in enumerate(row)] for row in modified_JPL_test\n","], dtype=object)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":281,"status":"ok","timestamp":1715572855198,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"S3Xzk_F1DGX_","outputId":"9d6a2585-40c6-45ea-9563-4153edb25b46"},"outputs":[],"source":["user_ids_JPL = np.unique(np.concatenate((JPL_train[:, 3], JPL_test[:, 3])))\n","\n","smape_list_JPL=[]\n","total_corr=[]\n","#predicted_values=[]\n","# true_values=[]\n","results={}\n","for user_id in user_ids_JPL:\n","    smape,corr,predicted_value,true_value=ensemble_algorithm_with_correlation(JPL_train, converted_array, sorted_data_JPL_testing, user_id, most_corr_users_hourly_JPL,DKDE_user_models,mode_per_user_per_interval_JPL_interpolate,similarities_hourly_JPL,R_SD)\n","    print(f'User {user_id} SMAPE: {smape:.2f}%')\n","    # predicted_values.append(predicted_value)\n","    # true_values.append(true_value)\n","    # smape_list_JPL.append(smape)\n","    # total_corr.append(corr)\n","    # Storing the true value and predicted value as a tuple in the dictionary with user_id as the key\n","    results[user_id] = predicted_value\n","\n","    smape_list_JPL.append(smape)\n","    total_corr.append(corr)\n","# Calculate the average SMAPE\n","average_smape = np.mean(smape_list_JPL) if smape_list_JPL else 0\n","\n","print(f\"Average SMAPE for the dataset: {average_smape:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":149,"status":"ok","timestamp":1715524693267,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"ls5bC1SlRQAu","outputId":"08f6cfe7-77a8-4aa2-939f-68a64be258e6"},"outputs":[],"source":["converted_array[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":182,"status":"ok","timestamp":1715524695234,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"IcK0XtteuLMJ","outputId":"e0c18b61-4dd7-4be4-ab99-411965f53abe"},"outputs":[],"source":["print(len(converted_array[converted_array[:, 3] == 531.0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":380,"status":"ok","timestamp":1715579890503,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"5jUwP9zLtyoj","outputId":"607c22a1-9375-4c1c-bc6d-f8f4013fa9d3"},"outputs":[],"source":["user_id=405\n","energy=5.822\n","\n","user_test_data = sorted_data_JPL_testing[sorted_data_JPL_testing[:, 2] == user_id]\n","\n","time_slots = list(mode_per_user_per_interval_JPL_interpolate[user_id].keys())\n","X_test_dkde_full=[]\n","for row in user_test_data:\n","  if row[0] in time_slots:\n","    X_test_dkde_full.append(row)\n","for index, array in enumerate(X_test_dkde_full):\n","    if energy in array:\n","        print(f\"The value 7.795 is found in the array at index {index}.\")\n","        break\n","else:\n","    print(\"The value 7.795 was not found in any array.\")\n","\n","value_at_index = results[user_id][index]\n","\n","print(f\"The value at index {index} is {value_at_index}.\")\n","intervals=value_at_index//5+1\n","intervals"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":168,"status":"ok","timestamp":1715551421854,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"r-CikwOc0IhK","outputId":"1ba19678-1960-4dc2-b362-6f503c0a6458"},"outputs":[],"source":["X_test_dkde_full"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":151,"status":"ok","timestamp":1715551442177,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"dqImwPpbz1kH","outputId":"afa4cf73-9317-4b3d-e6c8-094419041a2e"},"outputs":[],"source":["for index, array in enumerate(X_test_dkde_full):\n","    if energy in array:\n","        print(index)\n","        break\n","else:\n","    print(\"The value 7.795 was not found in any array.\")\n","\n","value_at_index = results[user_id][index]\n","\n","print(f\"The value at index {index} is {value_at_index}.\")\n","intervals=value_at_index//5+1\n","intervals"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyP0HkfsWFvQLFqhrlFH9dFS","provenance":[{"file_id":"1fv6i7yEHynIX6DpbNqY03UFyf-WT7u5T","timestamp":1700694907139},{"file_id":"19aOvXXPl1Rucdu0Z88HqTTP8RmiM_ipp","timestamp":1700632673978},{"file_id":"1cHdtNhA0ZofPXHdaPZsmHLX2gZDj3dJu","timestamp":1700006144358},{"file_id":"1M7ugd_F2UMQ796HlUfsXUl_Ucqj4nf54","timestamp":1697595791414}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
