{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23685,"status":"ok","timestamp":1713367984032,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"DL0k10RoUPJW","outputId":"3a63a5c9-1a70-46a0-8eda-78bfc959b1ee"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M1Ot-FcdUzts"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","# Define the file paths\n","file_path1 = '/content/drive/MyDrive/SuperUROP /Data Analysis/caltech_training_data.csv'\n","file_path2 = '/content/drive/MyDrive/SuperUROP /Data Analysis/caltech_testing_data.csv'\n","file_path3 = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_training_data.csv'\n","file_path4  = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_testing_data.csv'\n","# Use pandas to read the CSV files and then convert them to NumPy arrays\n","caltech_train = pd.read_csv(file_path1).values\n","caltech_test = pd.read_csv(file_path2).values\n","\n","JPL_train = pd.read_csv(file_path3).values\n","JPL_test=pd.read_csv(file_path4).values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27--VKb_k94F"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","# Define the file paths\n","file_path1 = '/content/drive/MyDrive/SuperUROP /Data Analysis/caltech_training_data_full.csv'\n","file_path3 = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_training_data.csv'\n","file_path4 = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_testing_data.csv'\n","\n","# Use pandas to read the CSV files\n","caltech_full_df = pd.read_csv(file_path1)\n","\n","JPL_train_df = pd.read_csv(file_path3)\n","JPL_test_df = pd.read_csv(file_path4)\n","\n","# Filter the caltech_full DataFrame\n","caltech_full_df = caltech_full_df[caltech_full_df['duration'] <= 24]\n","\n","caltech_train_df = caltech_full_df[(caltech_full_df['connectionTime'] >= '2021-03-01') & (caltech_full_df['connectionTime'] <= '2021-05-31')]\n","caltech_train_df= caltech_train_df[caltech_train_df['no_sessions'] >= 50]\n","caltech_test_df = caltech_full_df[(caltech_full_df['connectionTime'] >= '2021-06-01') & (caltech_full_df['connectionTime'] <= '2021-06-30')]\n","\n","JPL_train_df=JPL_train_df[JPL_train_df['no_sessions'] >= 30]\n","\n","# Convert to NumPy arrays if necessary\n","caltech_train = caltech_train_df.values\n","caltech_test = caltech_test_df.values\n","JPL_train = JPL_train_df.values\n","JPL_test = JPL_test_df.values\n"]},{"cell_type":"markdown","metadata":{"id":"T-LH42jEdjoD"},"source":["#Data Processing for MLR & DKDE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbbS_zZRVPX4"},"outputs":[],"source":["#Remove row number (in 1st column)\n","caltech_train=caltech_train[:,1:]\n","caltech_test=caltech_test[:,1:]\n","\n","JPL_train=JPL_train[:,1:]\n","JPL_test=JPL_test[:,1:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8P5wsz9EVgXL"},"outputs":[],"source":["#Remove departure time (2nd column)\n","# Convert arrival date to hour and find day of the week\n","from datetime import datetime\n","\n","def convert_time_and_day(data_array):\n","    \"\"\"\n","    Converts the time from HH:MM to HH.XX format and appends the day of the week to it.\n","    Also, removes the second column.\n","    \"\"\"\n","    transformed_data = []\n","    for row in data_array:\n","        # Convert the arrival time to HH.XX format\n","        time_obj = datetime.strptime(row[0], '%Y-%m-%d %H:%M:%S')\n","        new_time = time_obj.hour + (time_obj.minute / 60.0)\n","\n","        # Convert the date to a day of the week\n","        day_of_week = time_obj.strftime('%A')\n","        new_time = str(new_time) + \" \" + day_of_week\n","\n","        # Create a new row excluding the second column\n","        new_row = [new_time] + list(row[2:])\n","        transformed_data.append(new_row)\n","\n","    return np.array(transformed_data)\n","\n","caltech_train=convert_time_and_day(caltech_train)\n","caltech_test=convert_time_and_day(caltech_test)\n","JPL_train=convert_time_and_day(JPL_train)\n","JPL_test=convert_time_and_day(JPL_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFJTpBVqd-jm"},"outputs":[],"source":["def day_to_number(day):\n","    \"\"\"Converts a day of the week to its corresponding discrete value.\"\"\"\n","    days = {\n","        'Monday': 1,\n","        'Tuesday': 2,\n","        'Wednesday': 3,\n","        'Thursday': 4,\n","        'Friday': 5,\n","        'Saturday': 6,\n","        'Sunday': 7\n","    }\n","    return days[day]\n","\n","def separate_time_and_day(data_array):\n","    \"\"\"\n","    Separates the time and day in the given column,\n","    and converts the day into a discrete value between 1 and 7.\n","\n","    \"\"\"\n","    transformed_data = []\n","    for row in data_array:\n","        time_day_str = row[0]\n","        time, day = time_day_str.split()\n","        time = float(time)\n","        day_num = day_to_number(day)\n","\n","        # Create a new row with separated time and day number\n","        new_row = [time, day_num] + list(row[1:])\n","        transformed_data.append(new_row)\n","\n","    return np.array(transformed_data)\n","\n","caltech_train=separate_time_and_day(caltech_train)\n","caltech_test=separate_time_and_day(caltech_test)\n","JPL_train=separate_time_and_day(JPL_train)\n","JPL_test=separate_time_and_day(JPL_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OYcBnwTisZ8n"},"outputs":[],"source":["#Make training and testing set have the same user IDs\n","users_from_training_caltech = set(caltech_train[:, 3])\n","mask_caltech = np.isin(caltech_test[:, 3], list(users_from_training_caltech))\n","caltech_test = caltech_test[mask_caltech]\n","users_from_testing_caltech = set(caltech_test[:, 3])\n","mask_caltech = np.isin(caltech_train[:, 3], list(users_from_testing_caltech))\n","caltech_train = caltech_train[mask_caltech]\n","\n","users_from_training = set(JPL_train[:, 3])\n","mask = np.isin(JPL_test[:, 3], list(users_from_training))\n","JPL_test = JPL_test[mask]\n","users_from_testing = set(JPL_test[:, 3])\n","mask = np.isin(JPL_train[:, 3], list(users_from_testing))\n","JPL_train = JPL_train[mask]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eORUVi8wemPe"},"outputs":[],"source":["caltech_train = np.array(caltech_train, dtype='float')\n","caltech_test = np.array(caltech_test, dtype='float')\n","JPL_train = np.array(JPL_train, dtype='float')\n","JPL_test = np.array(JPL_test, dtype='float')"]},{"cell_type":"markdown","metadata":{"id":"yELKBYaHSvPO"},"source":["#Data Processing for correlation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hBlAwEPNSyjh"},"outputs":[],"source":["from collections import defaultdict\n","\n","# Organize the arrival times by user ID for the entire dataset\n","user_times_cal = defaultdict(list)\n","user_times_JPL = defaultdict(list)\n","\n","# Accumulate all arrival times for each user in caltech_train\n","for row in caltech_train:\n","    arrival_time = float(row[0])\n","    user_id = row[3]\n","    user_times_cal[user_id].append(arrival_time)\n","\n","# Accumulate all arrival times for each user in JPL_train\n","for row in JPL_train:\n","    arrival_time = float(row[0])\n","    user_id = row[3]\n","    user_times_JPL[user_id].append(arrival_time)\n","\n","# Define the functions for hourly and half-hourly representations\n","def create_hourly_representation(times):\n","    hourly_vector = [0] * 24\n","    for time in times:\n","        hour = int(float(time))\n","        hourly_vector[hour] += 1\n","    return hourly_vector\n","\n","def create_half_hourly_representation(times):\n","    half_hourly_vector = [0] * 48\n","    for time in times:\n","        interval = int(float(time) * 2)  # Multiply by 2 for half-hourly intervals\n","        half_hourly_vector[interval] += 1\n","    return half_hourly_vector\n","\n","# Compute the hourly and half-hourly representations for each user\n","hourly_representations_cal = {}\n","half_hourly_representations_cal = {}\n","hourly_representations_JPL = {}\n","half_hourly_representations_JPL = {}\n","\n","for user_id, times in user_times_cal.items():\n","    hourly_representations_cal[user_id] = create_hourly_representation(times)\n","    half_hourly_representations_cal[user_id] = create_half_hourly_representation(times)\n","\n","for user_id, times in user_times_JPL.items():\n","    hourly_representations_JPL[user_id] = create_hourly_representation(times)\n","    half_hourly_representations_JPL[user_id] = create_half_hourly_representation(times)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RbYqnzpoTJ7n"},"outputs":[],"source":["max_energy_caltech= int(np.round(caltech_train[:, 2].astype(float).max()))\n","max_energy_JPL= int(np.round(JPL_train[:, 2].astype(float).max()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ORxUMNMwTKb6"},"outputs":[],"source":["import numpy as np\n","from collections import defaultdict\n","\n","# Organize the energy consumption durations by user ID for the entire dataset\n","user_durations_cal = defaultdict(list)\n","user_durations_JPL = defaultdict(list)\n","\n","# Accumulate all energy consumption durations for each user in caltech_train\n","for row in caltech_train:\n","    energy_consumption = float(row[2])\n","    user_id = row[3]\n","    user_durations_cal[user_id].append(energy_consumption)\n","\n","# Accumulate all energy consumption durations for each user in JPL_train\n","for row in JPL_train:\n","    energy_consumption = float(row[2])\n","    user_id = row[3]\n","    user_durations_JPL[user_id].append(energy_consumption)\n","\n","# Define functions for hourly and half-hourly representations\n","def create_hourly_representation_duration(durations, max_energy):\n","    hourly_vector = [0] * (max_energy + 1)\n","    for duration in durations:\n","        index = int(float(duration))\n","        hourly_vector[index] += 1\n","    return hourly_vector\n","\n","def create_half_hourly_representation_duration(durations, max_energy):\n","    half_hourly_vector = [0] * ((max_energy + 1) * 2)\n","    for duration in durations:\n","        interval = int(float(duration) * 2)\n","        half_hourly_vector[interval] += 1\n","    return half_hourly_vector\n","\n","# Compute the hourly and half-hourly representations for each user\n","hourly_representations_duration_cal = {}\n","half_hourly_representations_duration_cal = {}\n","hourly_representations_duration_JPL = {}\n","half_hourly_representations_duration_JPL = {}\n","\n","for user_id, durations in user_durations_cal.items():\n","    hourly_representations_duration_cal[user_id] = create_hourly_representation_duration(durations, max_energy_caltech)\n","    half_hourly_representations_duration_cal[user_id] = create_half_hourly_representation_duration(durations, max_energy_caltech)\n","\n","for user_id, durations in user_durations_JPL.items():\n","    hourly_representations_duration_JPL[user_id] = create_hourly_representation_duration(durations, max_energy_JPL)\n","    half_hourly_representations_duration_JPL[user_id] = create_half_hourly_representation_duration(durations, max_energy_JPL)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ea-OFUnwTpG-"},"outputs":[],"source":["def concatenate_representations(dict1, dict2):\n","    concatenated = {}\n","    for key in set(dict1.keys()) | set(dict2.keys()):  # union of keys from both dicts\n","        list1 = dict1.get(key, [])\n","        list2 = dict2.get(key, [])\n","\n","        # Concatenate the lists\n","        concatenated_list = list1 + list2\n","\n","        concatenated[key] = concatenated_list\n","\n","    return concatenated\n","\n","# Example usage\n","concatenated_hourly_cal = concatenate_representations(hourly_representations_cal, hourly_representations_duration_cal)\n","concatenated_half_hourly_cal = concatenate_representations(half_hourly_representations_cal, half_hourly_representations_duration_cal)\n","\n","concatenated_hourly_JPL = concatenate_representations(hourly_representations_JPL, hourly_representations_duration_JPL)\n","concatenated_half_hourly_JPL = concatenate_representations(half_hourly_representations_JPL, half_hourly_representations_duration_JPL)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NT77SWE7N5xN"},"source":["#Calculate correlations - choose one of the two\n","Run one of the two cells as the results have the same name"]},{"cell_type":"markdown","metadata":{"id":"KFbalDPVN7vE"},"source":["##Calculate cosine similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tkQQql47Ultv"},"outputs":[],"source":["from sklearn.metrics import pairwise_distances\n","import pandas as pd\n","\n","def data_to_dataframe(data):\n","    return pd.DataFrame.from_dict(data, orient='index')\n","\n","def compute_cosine_similarity(df):\n","    cosine_distance = pairwise_distances(df, metric='cosine')\n","    cosine_similarity = 1 - cosine_distance\n","\n","    # Set the user IDs as row and column names to preserve them\n","    cosine_similarity_df = pd.DataFrame(cosine_similarity, index=df.index, columns=df.index)\n","    return cosine_similarity_df\n","\n","# Compute cosine similarity for each concatenated dataset\n","df_hourly_cal = data_to_dataframe(concatenated_hourly_cal)\n","similarities_hourly_cal = compute_cosine_similarity(df_hourly_cal)\n","\n","df_half_hourly_cal = data_to_dataframe(concatenated_half_hourly_cal)\n","similarities_half_hourly_cal = compute_cosine_similarity(df_half_hourly_cal)\n","\n","df_hourly_JPL = data_to_dataframe(concatenated_hourly_JPL)\n","similarities_hourly_JPL = compute_cosine_similarity(df_hourly_JPL)\n","\n","df_half_hourly_JPL = data_to_dataframe(concatenated_half_hourly_JPL)\n","similarities_half_hourly_JPL = compute_cosine_similarity(df_half_hourly_JPL)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_4dA-332OAB1"},"source":["##Calculate Pearson correlation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ZUpqfjarhtj"},"outputs":[],"source":["import numpy as np\n","\n","def compute_pearson_correlation(v1, v2):\n","    # Handling cases where vectors are too short or contain constant values\n","    if len(v1) > 1 and len(v2) > 1 and np.std(v1) * np.std(v2) != 0:\n","        return np.corrcoef(v1, v2)[0, 1]\n","    else:\n","        return None  # Returning None for cases where correlation is not defined\n","\n","import pandas as pd\n","\n","def compute_pearson_correlation_matrix_df(representations):\n","    user_ids = list(representations.keys())\n","    n = len(user_ids)\n","    correlation_matrix = np.zeros((n, n))\n","\n","    for i in range(n):\n","        for j in range(i, n):  # Compute only for one triangle and mirror it as the matrix is symmetric\n","            if i != j:\n","                corr = compute_pearson_correlation(representations[user_ids[i]], representations[user_ids[j]])\n","                correlation_matrix[i, j] = corr if corr is not None else 0\n","                correlation_matrix[j, i] = correlation_matrix[i, j]  # Mirror the value\n","            else:\n","                correlation_matrix[i, j] = 1  # Self-correlation is always 1\n","\n","    # Convert to DataFrame for better usability\n","    correlation_df = pd.DataFrame(correlation_matrix, index=user_ids, columns=user_ids)\n","    return correlation_df\n","\n","similarities_hourly_cal = compute_pearson_correlation_matrix_df(concatenated_hourly_cal)\n","similarities_half_hourly_cal = compute_pearson_correlation_matrix_df(concatenated_half_hourly_cal)\n","\n","similarities_hourly_JPL = compute_pearson_correlation_matrix_df(concatenated_hourly_JPL)\n","similarities_half_hourly_JPL = compute_pearson_correlation_matrix_df(concatenated_half_hourly_JPL)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pYVRigKn0xSo"},"outputs":[],"source":["from scipy.stats import spearmanr\n","import pandas as pd\n","\n","def data_to_dataframe(data):\n","    return pd.DataFrame.from_dict(data, orient='index')\n","\n","def calculate_spearman_correlation(df):\n","    # Transpose the DataFrame to make users (rows) into columns\n","    df_transposed = df.T\n","\n","    # Initialize an empty DataFrame to store the results\n","    spearman_corr_df = pd.DataFrame(index=df.index, columns=df.index)\n","\n","    # Calculate Spearman correlation for each pair of users (now columns)\n","    for user1 in df_transposed.columns:\n","        for user2 in df_transposed.columns:\n","            # Calculate Spearman correlation\n","            corr, _ = spearmanr(df_transposed[user1], df_transposed[user2])\n","            spearman_corr_df.loc[user1, user2] = corr\n","\n","    # Convert all values to numeric\n","    return spearman_corr_df.apply(pd.to_numeric)\n","\n","# Compute Spearman correlation for each concatenated dataset\n","df_hourly_cal = data_to_dataframe(concatenated_hourly_cal)\n","similarities_hourly_cal = calculate_spearman_correlation(df_hourly_cal)\n","\n","df_half_hourly_cal = data_to_dataframe(concatenated_half_hourly_cal)\n","similarities_half_hourly_cal = calculate_spearman_correlation(df_half_hourly_cal)\n","\n","df_hourly_JPL = data_to_dataframe(concatenated_hourly_JPL)\n","similarities_hourly_JPL = calculate_spearman_correlation(df_hourly_JPL)\n","\n","df_half_hourly_JPL = data_to_dataframe(concatenated_half_hourly_JPL)\n","similarities_half_hourly_JPL = calculate_spearman_correlation(df_half_hourly_JPL)\n"]},{"cell_type":"markdown","metadata":{"id":"nKBtKfZWOEXk"},"source":["#Calculated most correlated users (using raw values)\n","Run one of the two cells as the results have the same name - raw values perform much better"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xD2V-KFHWosG"},"outputs":[],"source":["#Using threshold\n","def most_correlated_users(matrix, threshold=0.82):\n","    \"\"\"\n","    For each user, find the top correlated users with correlations above 0.5.\n","\n","    :param matrix: Cosine similarity matrix.\n","    :param threshold: Minimum correlation threshold for considering a user as correlated.\n","    :return: Dictionary with keys being user IDs and values being lists of top correlated user IDs.\n","    \"\"\"\n","    result = {}\n","\n","    for user_index in matrix.index:\n","        # Subtract self-similarity value for the current user\n","        user_similarities = matrix.loc[user_index].drop(user_index)\n","\n","        # Filter users with correlation above the threshold\n","        filtered_users = user_similarities[abs(user_similarities) > threshold]\n","\n","        # Sort by descending correlation\n","        sorted_users = filtered_users.sort_values(ascending=False)\n","\n","        # Append users above the threshold to the result\n","        result[user_index] = sorted_users.index.tolist()\n","\n","    return result\n","\n","# Assuming `similarities_hourly_cal` is your cosine similarity matrix for the entire dataset\n","most_corr_users_hourly_cal = most_correlated_users(similarities_hourly_cal)\n","most_corr_users_half_hourly_cal = most_correlated_users(similarities_half_hourly_cal)\n","\n","# Similarly for JPL data\n","most_corr_users_hourly_JPL = most_correlated_users(similarities_hourly_JPL)\n","most_corr_users_half_hourly_JPL = most_correlated_users(similarities_half_hourly_JPL)"]},{"cell_type":"markdown","metadata":{"id":"QIHn62MOuwqz"},"source":["#MLR Energy Prediction for all users (no correlation)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Dta-nKctzVj"},"outputs":[],"source":["import numpy as np\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","\n","# Sample training data\n","import numpy as np\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","\n","\n","def mlr_model(train_data, test_data, user_id):\n","    # Filter training and testing data for the specific user\n","    user_train_data = train_data[train_data[:, 3] == user_id]\n","    user_test_data = test_data[test_data[:, 3] == user_id]\n","\n","    # Independent variables are arrival time, day of the week and estimated duration\n","    X_train = user_train_data[:, [0,1,4]]\n","    X_test = user_test_data[:, [0,1,4]]\n","\n","    # Dependent variable is the energy\n","    y_train = user_train_data[:, 2]\n","    y_test = user_test_data[:, 2]\n","\n","    # Train the model\n","    model = LinearRegression().fit(X_train, y_train)\n","\n","    # Predict on the test set\n","    y_pred = model.predict(X_test)\n","\n","    # Calculate user SMAPE\n","    n = len(y_test)\n","    smape_val = (1/ n) * np.sum(np.abs(y_test - y_pred) / (np.abs(y_test+y_pred)))*100\n","\n","    return model\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uxP2njavuRcQ"},"outputs":[],"source":["MLR_user_models={}\n","user_ids_JPL = np.unique(np.concatenate((JPL_train[:, 3], JPL_test[:, 3])))\n","\n","# Loop through each user ID in the JPL dataset\n","for user_id in user_ids_JPL:\n","    model = mlr_model(JPL_train, JPL_test, user_id)\n","    MLR_user_models[user_id]=model\n"]},{"cell_type":"markdown","metadata":{"id":"keUtdWxOpLtm"},"source":["#Train DKDE models for all users"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9687,"status":"ok","timestamp":1713368039674,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"m0Jt5_L-pc1T","outputId":"f78c7920-6bd4-48e2-faa1-eaa187024455"},"outputs":[],"source":["! pip install KDE-diffusion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N-0a4AmtpRZv"},"outputs":[],"source":["import numpy as np\n","from kde_diffusion import kde2d\n","user_ids_JPL = np.unique(np.concatenate((JPL_train[:, 3], JPL_test[:, 3])))\n","\n","class KDEModel:\n","    def __init__(self, grid, density):\n","        self.grid = grid\n","        self.density = density\n","\n","    def predict(self, X):\n","        predictions = [self._predict_y_given_x(x) for x in X]\n","        return predictions\n","\n","    def _predict_y_given_x(self, new_x):\n","        # Find the closest x index\n","        x_idx = np.argmin(np.abs(self.grid[0] - new_x))\n","\n","        # Get the y values and their corresponding densities for the given x\n","        y_values = self.grid[1]\n","        y_densities = self.density[x_idx]\n","\n","        # Find the y with the maximum density\n","        predicted_y = y_values[np.argmax(y_densities)]\n","\n","        return predicted_y\n","\n","def dkde_model(train_data, user_id):\n","    # Filter training data for the specific user\n","    user_train_data = train_data[train_data[:, 3] == user_id]\n","\n","    # Extract columns for stay duration and energy consumption\n","    X_train = user_train_data[:, 4]  # Stay duration\n","    Y_train = user_train_data[:, 2]  # Energy consumption\n","\n","    # Perform KDE\n","    density, grid, bandwidth = kde2d(X_train, Y_train, n=64, limits=None)\n","\n","    # Create and return a KDE model instance\n","    kde_model = KDEModel(grid, density)\n","    return kde_model\n","\n","DKDE_user_models = {user_id: dkde_model(JPL_train, user_id) for user_id in user_ids_JPL}\n"]},{"cell_type":"markdown","metadata":{"id":"lrdOzurx--A-"},"source":["#Train Decision Tree model for all users"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TA2X-H94_A-A"},"outputs":[],"source":["import numpy as np\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.preprocessing import StandardScaler\n","\n","def train_user_model(train_data, user_id,depth_range=(1, 21), min_samples_split_range=(2, 11), cv_folds=5):\n","    user_train_data = train_data[train_data[:, 3] == user_id]\n","    X_train = user_train_data[:, 4].reshape(-1, 1)  # Arrival time\n","    y_train = user_train_data[:, 2]  # Stay duration\n","\n","    # Set up grid search with cross-validation for Decision Tree\n","    param_grid = {\n","        'max_depth': list(range(depth_range[0], depth_range[1])),\n","        'min_samples_split': list(range(min_samples_split_range[0], min_samples_split_range[1]))\n","    }\n","    dt = DecisionTreeRegressor()\n","    grid_search = GridSearchCV(dt, param_grid, cv=cv_folds, scoring='neg_mean_squared_error', return_train_score=True)\n","    grid_search.fit(X_train, y_train)\n","\n","    # Train the model using the optimal k\n","    model = grid_search.best_estimator_\n","\n","\n","    return model\n","\n","DT_user_models = {user_id: train_user_model(JPL_train, user_id) for user_id in user_ids_JPL}"]},{"cell_type":"markdown","metadata":{"id":"875NcYigGBxC"},"source":["#Train SVR models for all users"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uX1o1Eu_F3eW"},"outputs":[],"source":["from sklearn.svm import SVR\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import mean_squared_error\n","def train_user_model(train_data, user_id, k=5):\n","    user_train_data = train_data[train_data[:, 3] == user_id]\n","    X_train = user_train_data[:, 4].reshape(-1, 1)  # Arrival time\n","    y_train = user_train_data[:, 2]  # Stay duration\n","\n","   # Define the hyperparameters to be optimized\n","    param_grid = {\n","        'C': [0.1, 1, 10, 100],\n","        'epsilon': [0.001, 0.01, 0.1, 1],\n","        'gamma': ['scale', 'auto', 0.1, 1, 10]\n","    }\n","\n","    # Initialize SVR with RBF kernel\n","    svr_rbf = SVR(kernel='rbf')\n","\n","    # Initialize GridSearchCV with k-fold cross-validation\n","    grid_search = GridSearchCV(estimator=svr_rbf, param_grid=param_grid, cv=k, scoring='neg_mean_squared_error', n_jobs=-1)\n","\n","    # Fit the model to the training data\n","    grid_search.fit(X_train, y_train)\n","\n","    model = grid_search.best_estimator_\n","\n","    return model\n","svr_models_JPL = {user_id: train_user_model(JPL_train, user_id) for user_id in user_ids_JPL}"]},{"cell_type":"markdown","metadata":{"id":"SdhWQhbbO12-"},"source":["#Enseble ML Algorithm Energy Prediction (with correlations)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IzcTbEsKsv54"},"outputs":[],"source":["R_DE={'169': 2.119068507176728,\n"," '171': 3.7115405017002776,\n"," '176': 2.9977726188370353,\n"," '220': 4.170165871125364,\n"," '322': 2.974319275358279,\n"," '334': 1.54796573662954,\n"," '335': 3.9915670302142447,\n"," '346': 4.687498008228514,\n"," '365': 4.400963644915041,\n"," '368': 3.4312414172978474,\n"," '372': 2.9575939245001077,\n"," '374': 3.839538311235034,\n"," '378': 3.1515844041039993,\n"," '382': 4.148262279562261,\n"," '404': 3.097855936467447,\n"," '405': 4.732202935285684,\n"," '406': 1.158211936837657,\n"," '409': 2.886901319079918,\n"," '410': 4.280915725084575,\n"," '416': 4.148262279562261,\n"," '436': 3.3808089672729613,\n"," '444': 3.0622185077071102,\n"," '458': 4.148262279562261,\n"," '467': 3.777008295296614,\n"," '474': 2.6165283654578775,\n"," '476': 3.7356597962280196,\n"," '481': 4.045488808054726,\n"," '483': 2.755038189921663,\n"," '507': 3.7804325624313817,\n"," '526': 2.4478241428939165,\n"," '531': 3.5032226226713927,\n"," '537': 3.1955976636365735,\n"," '551': 4.767567980735159,\n"," '553': 2.555583224030839,\n"," '576': 3.380624050295972,\n"," '577': 4.031441936975631,\n"," '581': 2.9304059447845647,\n"," '592': 3.056683078202939,\n"," '607': 4.625780698041485,\n"," '651': 3.492080210083478,\n"," '726': 3.2841952906487855,\n"," '742': 3.9023067062499264,\n"," '826': 3.375181253576597,\n"," '933': 3.9394875755880188}\n","\n","R_DE = {int(key): value for key, value in R_DE.items()}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eeuRxA3dsuL3"},"outputs":[],"source":["def ensemble_algorithm_with_correlation(train_data, test_data, user_id, most_corr_users, mlr_user_model, DKDE_user_model,DT_user_model,svr_user_model,similarity_matrix, R_DE):\n","    # Filter training and testing data for the specific user\n","    user_train_data = train_data[train_data[:, 3] == user_id]\n","    user_test_data = test_data[test_data[:, 3] == user_id]\n","\n","    X_test_mlr = user_test_data[:, [0, 1, 4]]\n","    X_test_dkde = user_test_data[:, 4]\n","    X_test_dt = user_test_data[:, 4].reshape(-1, 1)\n","    X_test_svr = user_test_data[:, 4].reshape(-1, 1)\n","    y_test = user_test_data[:, 2]\n","\n","    # Choose and use the model based on the R_SD ratio for the main user\n","    if R_DE.get(user_id, 0) < 3.5:\n","        user_model =mlr_user_model[user_id]\n","        y_pred = user_model.predict(X_test_mlr)\n","    else:\n","        user_model = DKDE_user_model[user_id]\n","        y_pred = user_model.predict(X_test_dkde)\n","\n","    # Convert y_pred to a NumPy array if it is a list\n","    y_pred = np.array(y_pred) if isinstance(y_pred, list) else y_pred\n","\n","    # Adjust the prediction based on the most correlated users\n","    total_correlation = 1\n","    if user_id in most_corr_users:\n","        correlated_users = most_corr_users[user_id]\n","\n","        for other_user_id in correlated_users:\n","            if other_user_id in mlr_user_model or other_user_id in DKDE_user_model:\n","                correlation = similarity_matrix.loc[user_id, other_user_id]\n","\n","                # Choose the model for the correlated user based on R_SD\n","                if R_DE.get(other_user_id, 0) < 3.5:\n","                    other_user_model = mlr_user_model[other_user_id]\n","                    other_user_pred = other_user_model.predict(X_test_mlr)\n","                else:\n","                    other_user_model = DKDE_user_model[other_user_id]\n","                    other_user_pred = other_user_model.predict(X_test_dkde)\n","\n","                # Convert other_user_pred to a NumPy array if it's a list\n","                other_user_pred = np.array(other_user_pred) if isinstance(other_user_pred, list) else other_user_pred\n","\n","                # Ensure y_pred is an array before the operation\n","                if not isinstance(y_pred, np.ndarray):\n","                    y_pred = np.array(y_pred)\n","\n","                y_pred += other_user_pred * correlation\n","                total_correlation += correlation\n","\n","    # Ensure y_pred is an array before the division\n","    if not isinstance(y_pred, np.ndarray):\n","        y_pred = np.array(y_pred)\n","\n","    if total_correlation != 0:\n","        y_pred /= total_correlation\n","\n","    # Calculate user SMAPE\n","    n = len(y_test)\n","    smape_val = (1 / n) * np.sum(np.abs(y_test - y_pred) / (np.abs(y_test + y_pred))) * 100\n","\n","    return smape_val\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":149,"status":"ok","timestamp":1713368121007,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"fepqJRoPbzdq","outputId":"c29adaf1-7eb7-47ea-9ab5-e77b709a6a43"},"outputs":[],"source":["user_ids_JPL = np.unique(np.concatenate((JPL_train[:, 3], JPL_test[:, 3])))\n","\n","smape_list_JPL=[]\n","\n","for user_id in user_ids_JPL:\n","    smape = ensemble_algorithm_with_correlation(JPL_train, JPL_test, user_id, most_corr_users_half_hourly_JPL, MLR_user_models, DKDE_user_models,DT_user_models,svr_models_JPL,similarities_half_hourly_JPL,R_DE)\n","    smape_list_JPL.append(smape)\n","\n","# Calculate the average SMAPE\n","average_smape = np.mean(smape_list_JPL) if smape_list_JPL else 0\n","\n","print(f\"Average SMAPE for the dataset: {average_smape:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wM_YCZl4AnbL"},"outputs":[],"source":["def ensemble_algorithm_with_correlation(train_data, test_data, user_id, most_corr_users, mlr_user_model, DKDE_user_model,DT_user_model,svr_user_model,similarity_matrix, R_DE):\n","    # Filter training and testing data for the specific user\n","    user_train_data = train_data[train_data[:, 3] == user_id]\n","    user_test_data = test_data[test_data[:, 3] == user_id]\n","\n","    X_test_mlr = user_test_data[:, [0, 1, 4]]\n","    X_test_dkde = user_test_data[:, 4]\n","    X_test_dt = user_test_data[:, 4].reshape(-1, 1)\n","    X_test_svr = user_test_data[:, 4].reshape(-1, 1)\n","    y_test = user_test_data[:, 2]\n","\n","    # Choose and use the model based on the R_SD ratio for the main user\n","    if R_DE.get(user_id, 0) < 3.5:\n","        user_model =mlr_user_model[user_id]\n","        y_pred = user_model.predict(X_test_mlr)\n","    else:\n","        user_model = DKDE_user_model[user_id]\n","        y_pred = user_model.predict(X_test_dkde)\n","\n","    # Convert y_pred to a NumPy array if it is a list\n","    y_pred = np.array(y_pred) if isinstance(y_pred, list) else y_pred\n","\n","    # Adjust the prediction based on the most correlated users\n","    total_correlation = 1\n","    if user_id in most_corr_users:\n","        correlated_users = most_corr_users[user_id]\n","\n","        for other_user_id in correlated_users:\n","            if other_user_id in mlr_user_model or other_user_id in DKDE_user_model:\n","                correlation = similarity_matrix.loc[user_id, other_user_id]\n","\n","                # Choose the model for the correlated user based on R_SD\n","                if R_DE.get(other_user_id, 0) < 3.5:\n","                    other_user_model = mlr_user_model[other_user_id]\n","                    other_user_pred = other_user_model.predict(X_test_mlr)\n","                else:\n","                    other_user_model = DKDE_user_model[other_user_id]\n","                    other_user_pred = other_user_model.predict(X_test_dkde)\n","\n","                # Convert other_user_pred to a NumPy array if it's a list\n","                other_user_pred = np.array(other_user_pred) if isinstance(other_user_pred, list) else other_user_pred\n","\n","                # Ensure y_pred is an array before the operation\n","                if not isinstance(y_pred, np.ndarray):\n","                    y_pred = np.array(y_pred)\n","\n","                y_pred += other_user_pred * correlation\n","                total_correlation += correlation\n","\n","    # Ensure y_pred is an array before the division\n","    if not isinstance(y_pred, np.ndarray):\n","        y_pred = np.array(y_pred)\n","\n","    if total_correlation != 0:\n","        y_pred /= total_correlation\n","\n","    # Calculate user SMAPE\n","    n = len(y_test)\n","    smape_val = (1 / n) * np.sum(np.abs(y_test - y_pred) / (np.abs(y_test + y_pred))) * 100\n","\n","    return smape_val\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMWyyyrYKlE/IPFY7bn5qsk","provenance":[{"file_id":"19aOvXXPl1Rucdu0Z88HqTTP8RmiM_ipp","timestamp":1700632673978},{"file_id":"1cHdtNhA0ZofPXHdaPZsmHLX2gZDj3dJu","timestamp":1700006144358},{"file_id":"1M7ugd_F2UMQ796HlUfsXUl_Ucqj4nf54","timestamp":1697595791414}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
