{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":102377,"status":"ok","timestamp":1700626399950,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"DL0k10RoUPJW","outputId":"7d116f68-b277-49e1-a4cf-4edb14906e83"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M1Ot-FcdUzts"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","# Define the file paths\n","file_path1 = '/content/drive/MyDrive/SuperUROP /Data Analysis/caltech_training_data.csv'\n","file_path2 = '/content/drive/MyDrive/SuperUROP /Data Analysis/caltech_testing_data.csv'\n","file_path3 = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_training_data.csv'\n","file_path4  = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_testing_data.csv'\n","# Use pandas to read the CSV files and then convert them to NumPy arrays\n","caltech_train = pd.read_csv(file_path1).values\n","caltech_test = pd.read_csv(file_path2).values\n","\n","JPL_train = pd.read_csv(file_path3).values\n","JPL_test=pd.read_csv(file_path4).values"]},{"cell_type":"markdown","metadata":{"id":"T-LH42jEdjoD"},"source":["#Data Processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbbS_zZRVPX4"},"outputs":[],"source":["#Remove row number (in 1st column)\n","caltech_train=caltech_train[:,1:]\n","caltech_test=caltech_test[:,1:]\n","\n","JPL_train=JPL_train[:,1:]\n","JPL_test=JPL_test[:,1:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8P5wsz9EVgXL"},"outputs":[],"source":["#Remove departure time (2nd column)\n","# Convert arrival date to hour and find day of the week\n","from datetime import datetime\n","\n","def convert_time_and_day(data_array):\n","    \"\"\"\n","    Converts the time from HH:MM to HH.XX format and appends the day of the week to it.\n","    Also, removes the second column.\n","    \"\"\"\n","    transformed_data = []\n","    for row in data_array:\n","        # Convert the arrival time to HH.XX format\n","        time_obj = datetime.strptime(row[0], '%Y-%m-%d %H:%M:%S')\n","        new_time = time_obj.hour + (time_obj.minute / 60.0)\n","\n","        # Convert the date to a day of the week\n","        day_of_week = time_obj.strftime('%A')\n","        new_time = str(new_time) + \" \" + day_of_week\n","\n","        # Create a new row excluding the second column\n","        new_row = [new_time] + list(row[2:])\n","        transformed_data.append(new_row)\n","\n","    return np.array(transformed_data)\n","\n","caltech_train=convert_time_and_day(caltech_train)\n","caltech_test=convert_time_and_day(caltech_test)\n","JPL_train=convert_time_and_day(JPL_train)\n","JPL_test=convert_time_and_day(JPL_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFJTpBVqd-jm"},"outputs":[],"source":["def day_to_number(day):\n","    \"\"\"Converts a day of the week to its corresponding discrete value.\"\"\"\n","    days = {\n","        'Monday': 1,\n","        'Tuesday': 2,\n","        'Wednesday': 3,\n","        'Thursday': 4,\n","        'Friday': 5,\n","        'Saturday': 6,\n","        'Sunday': 7\n","    }\n","    return days[day]\n","\n","def separate_time_and_day(data_array):\n","    \"\"\"\n","    Separates the time and day in the given column,\n","    and converts the day into a discrete value between 1 and 7.\n","\n","    \"\"\"\n","    transformed_data = []\n","    for row in data_array:\n","        time_day_str = row[0]\n","        time, day = time_day_str.split()\n","        time = float(time)\n","        day_num = day_to_number(day)\n","\n","        # Create a new row with separated time and day number\n","        new_row = [time, day_num] + list(row[1:])\n","        transformed_data.append(new_row)\n","\n","    return np.array(transformed_data)\n","\n","caltech_train=separate_time_and_day(caltech_train)\n","caltech_test=separate_time_and_day(caltech_test)\n","JPL_train=separate_time_and_day(JPL_train)\n","JPL_test=separate_time_and_day(JPL_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OYcBnwTisZ8n"},"outputs":[],"source":["#Make training and testing set have the same user IDs\n","users_from_training_caltech = set(caltech_train[:, 3])\n","mask_caltech = np.isin(caltech_test[:, 3], list(users_from_training_caltech))\n","caltech_test = caltech_test[mask_caltech]\n","users_from_testing_caltech = set(caltech_test[:, 3])\n","mask_caltech = np.isin(caltech_train[:, 3], list(users_from_testing_caltech))\n","caltech_train = caltech_train[mask_caltech]\n","\n","users_from_training = set(JPL_train[:, 3])\n","mask = np.isin(JPL_test[:, 3], list(users_from_training))\n","JPL_test = JPL_test[mask]\n","users_from_testing = set(JPL_test[:, 3])\n","mask = np.isin(JPL_train[:, 3], list(users_from_testing))\n","JPL_train = JPL_train[mask]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eORUVi8wemPe"},"outputs":[],"source":["caltech_train = np.array(caltech_train, dtype='float')\n","caltech_test = np.array(caltech_test, dtype='float')\n","JPL_train = np.array(JPL_train, dtype='float')\n","JPL_test = np.array(JPL_test, dtype='float')"]},{"cell_type":"markdown","metadata":{"id":"QIHn62MOuwqz"},"source":["#MLR Energy Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Dta-nKctzVj"},"outputs":[],"source":["import numpy as np\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","\n","# Sample training data\n","\n","def mlr_model(train_data, test_data, user_id):\n","    # Filter training and testing data for the specific user\n","    user_train_data = train_data[train_data[:, 3] == user_id]\n","    user_test_data = test_data[test_data[:, 3] == user_id]\n","\n","    # Independent variables are arrival time, day of the week and estimated duration\n","    X_train = user_train_data[:, [0,1,4]]\n","    X_test = user_test_data[:, [0,1,4]]\n","\n","    # Dependent variable is the energy\n","    y_train = user_train_data[:, 2]\n","    y_test = user_test_data[:, 2]\n","\n","    # Train the model\n","    model = LinearRegression().fit(X_train, y_train)\n","\n","    # Predict on the test set\n","    y_pred = model.predict(X_test)\n","\n","    # Calculate user SMAPE\n","    n = len(y_test)\n","    smape_val = (1/ n) * np.sum(np.abs(y_test - y_pred) / (np.abs(y_test+y_pred)))*100\n","\n","    return model.coef_, smape_val\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":125,"status":"ok","timestamp":1700626420585,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"uxP2njavuRcQ","outputId":"052a4d7d-b433-4293-9072-3d6ed84853d6"},"outputs":[],"source":["# Test the function\n","user_ids_JPL = np.unique(np.concatenate((JPL_train[:, 3], JPL_test[:, 3])))\n","smape_list_JPL=[]\n","\n","for user_id in user_ids_JPL:\n","    model, smape = mlr_model(JPL_train, JPL_test, user_id)\n","    smape_list_JPL.append(smape)\n","\n","#Calculate average SMAPE for JPL\n","no_JPL_users=len(user_ids_JPL)\n","JPL_smape=sum(smape_list_JPL)/no_JPL_users\n","print(f\"Average SMAPE for JPL dataset using DKDE: {JPL_smape}\")"]},{"cell_type":"markdown","metadata":{"id":"ywd0tIKMroY_"},"source":["#DKDE Energy Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7724,"status":"ok","timestamp":1700626430688,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"hdbCMVPBrvuX","outputId":"3e57a3a6-5a3b-454f-891f-85a6ea70aba5"},"outputs":[],"source":["! pip install KDE-diffusion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2VfkEkPrrdR"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","from kde_diffusion import kde2d\n","def dkde_model(train_data,test_data,user_id):\n","    # Filter training and testing data for the specific user\n","    user_train_data = train_data[train_data[:, 3] == user_id]\n","    user_test_data = test_data[test_data[:, 3] == user_id]\n","\n","    # Extract columns for stay duration and energy consumption for both train and test sets\n","    X_train = user_train_data[:, 4]  # Stay duration\n","    Y_train = user_train_data[:, 2] # Energy consumption\n","\n","    X_test = user_test_data[:, 4]\n","    y_test = user_test_data[:, 2]\n","\n","\n","    (density, grid, bandwidth) = kde2d(X_train, Y_train, n=128, limits=None)\n","\n","    def predict_y_given_x(new_x, grid, density):\n","        # Find the closest x index\n","        x_idx = np.argmin(np.abs(grid[0] - new_x))\n","\n","        # Get the y values and their corresponding densities for the given x\n","        y_values = grid[1]\n","        y_densities = density[x_idx]\n","\n","        # Find the y with the maximum density\n","        predicted_y = y_values[np.argmax(y_densities)]\n","\n","        return predicted_y\n","\n","    # Predict y values for X_test using the density estimate\n","    y_pred = [predict_y_given_x(x_val, grid, density) for x_val in X_test]\n","\n","    # Calculate user SMAPE\n","    n = len(y_test)\n","    smape_val = (1/ n) * np.sum(np.abs(y_test - y_pred) / (np.abs(y_test+y_pred)))*100\n","\n","    return smape_val,density,grid\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6411,"status":"ok","timestamp":1700626445757,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"75USAyZ4r3Pr","outputId":"5f3c64f7-d9e6-415e-c613-17a833ebe1cc"},"outputs":[],"source":["# Initialize dictionaries to store user-wise SMAPE values\n","user_smape_JPL = {}\n","\n","# Loop through each user ID in the JPL dataset\n","for user_id in user_ids_JPL:\n","    smape, density, grid = dkde_model(JPL_train, JPL_test, user_id)\n","    user_smape_JPL[user_id] = smape  # Store the SMAPE in the dictionary\n","\n","average_smape_JPL = np.mean(list(user_smape_JPL.values()))\n","\n","print(f\"Average SMAPE for JPL dataset using DKDE: {average_smape_JPL:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"izPfXDrqQmpn"},"source":["#Decision Tree energy prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5lfMloPbQydz"},"outputs":[],"source":["import numpy as np\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt\n","\n","def decision_tree_model(train_data, test_data, used_id, depth_range=(1, 21), min_samples_split_range=(2, 11), cv_folds=5):\n","    \"\"\"\n","    Perform grid search with cross-validation to find optimal hyperparameters for Decision Tree regression.\n","\n","    Parameters:\n","    - train_data: Training dataset with the format [sample, features]\n","    - test_data: Testing dataset with the same format as train_data\n","    - depth_range: Tuple representing the range of max_depth values to test (default is (1, 21))\n","    - min_samples_split_range: Tuple representing the range of min_samples_split values to test (default is (2, 11))\n","    - cv_folds: Number of cross-validation folds (default is 5)\n","\n","    Returns:\n","    - y_pred: Predictions on the test set\n","    - best_params: Best hyperparameters found\n","    \"\"\"\n","\n","    # Filter training and testing data for the specific user\n","    user_train_data = train_data[train_data[:, 3] == user_id]\n","    user_test_data = test_data[test_data[:, 3] == user_id]\n","\n","    # Extract columns for stay duration and energy consumption for both train and test sets\n","    X_train = user_train_data[:, 4].reshape(-1, 1)  # Stay Duration\n","    y_train = user_train_data[:, 2]  # Energy Consumption\n","\n","    X_test = user_test_data[:, 4].reshape(-1, 1)\n","    y_test = user_test_data[:, 2]\n","\n","    # Set up grid search with cross-validation for Decision Tree\n","    param_grid = {\n","        'max_depth': list(range(depth_range[0], depth_range[1])),\n","        'min_samples_split': list(range(min_samples_split_range[0], min_samples_split_range[1]))\n","    }\n","    dt = DecisionTreeRegressor()\n","    grid_search = GridSearchCV(dt, param_grid, cv=cv_folds, scoring='neg_mean_squared_error', return_train_score=True)\n","    grid_search.fit(X_train, y_train)\n","\n","    # Train the model using the optimal hyperparameters\n","    best_dt = grid_search.best_estimator_\n","\n","    # Predict on the test set\n","    y_pred = best_dt.predict(X_test)\n","\n","    # Best hyperparameters\n","    best_params = grid_search.best_params_\n","\n","    # Calculate user SMAPE\n","    n = len(y_test)\n","    smape_val = (1/ n) * np.sum(np.abs(y_test - y_pred) / (np.abs(y_test+y_pred)))*100\n","\n","    return best_params,smape_val\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":74524,"status":"ok","timestamp":1700627570650,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"8ZyN2yp5Q3bY","outputId":"7d08d1d7-c01e-401a-ffad-568a33d875f1"},"outputs":[],"source":["# Initialize dictionaries to store user-wise SMAPE values\n","user_smape_JPL = {}\n","\n","# Loop through each user ID in the JPL dataset\n","for user_id in user_ids_JPL:\n","    best_params,smape = decision_tree_model(JPL_train, JPL_test, user_id)\n","    user_smape_JPL[user_id] = smape  # Store the SMAPE in the dictionary\n","\n","average_smape_JPL = np.mean(list(user_smape_JPL.values()))\n","\n","print(f\"Average SMAPE for JPL dataset using Decision Tree: {average_smape_JPL:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Zgr7Q0VSOx0"},"outputs":[],"source":["from sklearn.svm import SVR\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import mean_squared_error\n","\n","def svr_rbf_model(train_data, test_data, user_id,k=5):\n","\n","    # Filter training and testing data for the specific user\n","    user_train_data = train_data[train_data[:, 3] == user_id]\n","    user_test_data = test_data[test_data[:, 3] == user_id]\n","\n","    # Extract columns for stay duration and energy consumption for both train and test sets\n","    X_train = user_train_data[:,4].reshape(-1, 1)  # Stay Duration\n","    y_train = user_train_data[:,2]  # Energy Consumption\n","\n","    X_test = user_test_data[:,4].reshape(-1, 1)\n","    y_test = user_test_data[:,2]\n","\n","    # Define the hyperparameters to be optimized\n","    param_grid = {\n","        'C': [0.1, 1, 10, 100],\n","        'epsilon': [0.001, 0.01, 0.1, 1],\n","        'gamma': ['scale', 'auto', 0.1, 1, 10]\n","    }\n","\n","    # Initialize SVR with RBF kernel\n","    svr_rbf = SVR(kernel='rbf')\n","\n","    # Initialize GridSearchCV with k-fold cross-validation\n","    grid_search = GridSearchCV(estimator=svr_rbf, param_grid=param_grid, cv=k, scoring='neg_mean_squared_error', n_jobs=-1)\n","\n","    # Fit the model to the training data\n","    grid_search.fit(X_train, y_train)\n","\n","    # Use the best estimator to predict on the test data\n","    test_predictions = grid_search.best_estimator_.predict(X_test)\n","\n","    # Calculate MSE on test data\n","    mse = mean_squared_error(y_test, test_predictions)\n","\n","    # Calculate user SMAPE\n","    n = len(y_test)\n","    smape_val = (1/ n) * np.sum(np.abs(y_test - test_predictions) / (np.abs(y_test+test_predictions)))*100\n","\n","    return grid_search.best_params_, smape_val\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32604,"status":"ok","timestamp":1700627686752,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"yeSLsWGhSS10","outputId":"ca46a3d7-a9db-42d9-b58b-869e4b85fd96"},"outputs":[],"source":["# Initialize dictionaries to store user-wise SMAPE values\n","user_smape_JPL = {}\n","\n","# Loop through each user ID in the JPL dataset\n","for user_id in user_ids_JPL:\n","    best_params,smape = svr_rbf_model(JPL_train, JPL_test, user_id)\n","    user_smape_JPL[user_id] = smape  # Store the SMAPE in the dictionary\n","\n","average_smape_JPL = np.mean(list(user_smape_JPL.values()))\n","\n","print(f\"Average SMAPE for JPL dataset using SVR: {average_smape_JPL:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"Joamn3QTC8LA"},"source":["#Ensemble ML energy prediction with different thresholds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rm7dx72dC_12"},"outputs":[],"source":["R_DE={'169': 2.119068507176728,\n"," '171': 3.7115405017002776,\n"," '176': 2.9977726188370353,\n"," '220': 4.170165871125364,\n"," '322': 2.974319275358279,\n"," '334': 1.54796573662954,\n"," '335': 3.9915670302142447,\n"," '346': 4.687498008228514,\n"," '365': 4.400963644915041,\n"," '368': 3.4312414172978474,\n"," '372': 2.9575939245001077,\n"," '374': 3.839538311235034,\n"," '378': 3.1515844041039993,\n"," '382': 4.148262279562261,\n"," '404': 3.097855936467447,\n"," '405': 4.732202935285684,\n"," '406': 1.158211936837657,\n"," '409': 2.886901319079918,\n"," '410': 4.280915725084575,\n"," '416': 4.148262279562261,\n"," '436': 3.3808089672729613,\n"," '444': 3.0622185077071102,\n"," '458': 4.148262279562261,\n"," '467': 3.777008295296614,\n"," '474': 2.6165283654578775,\n"," '476': 3.7356597962280196,\n"," '481': 4.045488808054726,\n"," '483': 2.755038189921663,\n"," '507': 3.7804325624313817,\n"," '526': 2.4478241428939165,\n"," '531': 3.5032226226713927,\n"," '537': 3.1955976636365735,\n"," '551': 4.767567980735159,\n"," '553': 2.555583224030839,\n"," '576': 3.380624050295972,\n"," '577': 4.031441936975631,\n"," '581': 2.9304059447845647,\n"," '592': 3.056683078202939,\n"," '607': 4.625780698041485,\n"," '651': 3.492080210083478,\n"," '726': 3.2841952906487855,\n"," '742': 3.9023067062499264,\n"," '826': 3.375181253576597,\n"," '933': 3.9394875755880188}\n","\n","R_DE = {int(key): value for key, value in R_DE.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2277,"status":"ok","timestamp":1700626491718,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"WEqHiW1aEbUJ","outputId":"6d13b681-815e-46a8-cca9-4cf494a00e8e"},"outputs":[],"source":["import numpy as np\n","\n","# Assuming R_DE, JPL_train, JPL_test, mlr_model, and dkde_model are already defined\n","\n","user_smape_combined = {}  # Dictionary to store SMAPE values for all users\n","\n","# Loop through each user ID in the JPL dataset\n","for user_id in user_ids_JPL:\n","    if R_DE.get(user_id, 0) < 3.5:\n","        # Use mlr_model for users with R_DE < 3.5\n","        model, smape = mlr_model(JPL_train, JPL_test, user_id)\n","    else:\n","        # Use dkde_model for users with R_DE >= 3.5\n","        smape, density, grid = dkde_model(JPL_train, JPL_test, user_id)\n","\n","    user_smape_combined[user_id] = smape  # Store the SMAPE in the dictionary\n","\n","# Calculate the overall average SMAPE for the entire dataset\n","average_smape_combined = np.mean(list(user_smape_combined.values()))\n","\n","print(f\"Average SMAPE for JPL dataset using combined approach with threshold of 3.5: {average_smape_combined:.2f}%\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":664,"status":"ok","timestamp":1700627211627,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"04VzNYhGPPxn","outputId":"ce20c5e7-1aa2-458b-e56d-a4f239e2b938"},"outputs":[],"source":["import numpy as np\n","\n","# Assuming R_DE, JPL_train, JPL_test, mlr_model, and dkde_model are already defined\n","\n","user_smape_combined = {}  # Dictionary to store SMAPE values for all users\n","\n","# Loop through each user ID in the JPL dataset\n","for user_id in user_ids_JPL:\n","    if R_DE.get(user_id, 0) < 4:\n","        # Use mlr_model for users with R_DE < 4\n","        model, smape = mlr_model(JPL_train, JPL_test, user_id)\n","    else:\n","        # Use dkde_model for users with R_DE >= 4\n","        smape, density, grid = dkde_model(JPL_train, JPL_test, user_id)\n","\n","    user_smape_combined[user_id] = smape  # Store the SMAPE in the dictionary\n","\n","# Calculate the overall average SMAPE for the entire dataset\n","average_smape_combined = np.mean(list(user_smape_combined.values()))\n","\n","print(f\"Average SMAPE for JPL dataset using combined approach with threshold of 4: {average_smape_combined:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"Bn0k44CwPY84"},"source":["#Ensemble ML energy prediction (threshold of 4.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":69113,"status":"ok","timestamp":1700627763763,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"_KxhXk4iRYCP","outputId":"4a9c4ff6-979c-46b7-f89a-de1c0e547594"},"outputs":[],"source":["import numpy as np\n","\n","# Assuming R_DE, JPL_train, JPL_test, mlr_model, and dkde_model are already defined\n","\n","user_smape_combined = {}  # Dictionary to store SMAPE values for all users\n","\n","# Loop through each user ID in the JPL dataset\n","for user_id in user_ids_JPL:\n","    if R_DE.get(user_id, 0) < 4.5:\n","        # Use DT for users with R_DE < 4.5\n","        best_params,smape = decision_tree_model(JPL_train, JPL_test, user_id)\n","    else:\n","        # Use dkde_model for users with R_DE >= 4.5\n","        smape, density, grid = dkde_model(JPL_train, JPL_test, user_id)\n","\n","    user_smape_combined[user_id] = smape  # Store the SMAPE in the dictionary\n","\n","# Calculate the overall average SMAPE for the entire dataset\n","average_smape_combined = np.mean(list(user_smape_combined.values()))\n","\n","print(f\"Average SMAPE for JPL dataset using combined approach with threshold of 4.5: {average_smape_combined:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"qzatCV6PSAZC"},"source":["#Ensemble ML energy prediction (threshold of 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53059,"status":"ok","timestamp":1700628037206,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"jMwsh0XASD96","outputId":"1284e56b-c596-477e-ef1e-e65331a9642c"},"outputs":[],"source":["import numpy as np\n","\n","# Assuming R_DE, JPL_train, JPL_test, mlr_model, and dkde_model are already defined\n","\n","user_smape_combined = {}  # Dictionary to store SMAPE values for all users\n","\n","# Loop through each user ID in the JPL dataset\n","for user_id in user_ids_JPL:\n","    if R_DE.get(user_id, 0) < 3:\n","        # Use SVR for users with R_DE < 3\n","        best_params,smape = svr_rbf_model(JPL_train, JPL_test, user_id)\n","    else:\n","        # Use DT for users with R_DE >= 3\n","        best_params,smape = decision_tree_model(JPL_train, JPL_test, user_id)\n","\n","    user_smape_combined[user_id] = smape  # Store the SMAPE in the dictionary\n","\n","# Calculate the overall average SMAPE for the entire dataset\n","average_smape_combined = np.mean(list(user_smape_combined.values()))\n","\n","print(f\"Average SMAPE for JPL dataset using combined approach with threshold of 3: {average_smape_combined:.2f}%\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOyiYOoPvqeresR64h7u6aN","provenance":[{"file_id":"1CznyjFbSYMF94TkTpJq62eAMMmxgaEJN","timestamp":1700600054990},{"file_id":"1cHdtNhA0ZofPXHdaPZsmHLX2gZDj3dJu","timestamp":1700589157779},{"file_id":"1M7ugd_F2UMQ796HlUfsXUl_Ucqj4nf54","timestamp":1697595791414}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
