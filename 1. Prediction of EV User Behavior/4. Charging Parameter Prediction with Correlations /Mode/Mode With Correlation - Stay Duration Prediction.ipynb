{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":460549,"status":"ok","timestamp":1713071191492,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"SEPSs_wa7AGx","outputId":"f5e62ef7-e335-4c67-85d8-ec79b6935de4"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"x3oloIR5h--R"},"source":["#Data Processing for mode"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2616,"status":"ok","timestamp":1713071195979,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"4MaMlUVr7VG-"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","# Define the file paths\n","file_path1 = '/content/drive/MyDrive/SuperUROP /Data Analysis/caltech_training_data.csv'\n","file_path2 = '/content/drive/MyDrive/SuperUROP /Data Analysis/caltech_testing_data.csv'\n","file_path3 = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_training_data.csv'\n","file_path4  = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_testing_data.csv'\n","# Use pandas to read the CSV files and then convert them to NumPy arrays\n","Caltech_data_training = pd.read_csv(file_path1).values\n","Caltech_data_testing = pd.read_csv(file_path2).values\n","\n","JPL_training_data = pd.read_csv(file_path3).values\n","JPL_testing_data=pd.read_csv(file_path4).values"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":632,"status":"ok","timestamp":1713071201189,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"0ltyzwm8fhmz"},"outputs":[],"source":["import pandas as pd\n","\n","# Define the file paths\n","file_path1 = '/content/drive/MyDrive/SuperUROP /Data Analysis/caltech_training_data_full.csv'\n","file_path3 = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_training_data.csv'\n","file_path4 = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_testing_data.csv'\n","\n","# Use pandas to read the CSV files\n","caltech_full_df = pd.read_csv(file_path1)\n","\n","JPL_train_df = pd.read_csv(file_path3)\n","JPL_test_df = pd.read_csv(file_path4)\n","\n","# Filter the caltech_full DataFrame\n","caltech_full_df = caltech_full_df[caltech_full_df['duration'] <= 24]\n","\n","caltech_train_df = caltech_full_df[(caltech_full_df['connectionTime'] >= '2021-03-01') & (caltech_full_df['connectionTime'] <= '2021-05-31')]\n","caltech_train_df= caltech_train_df[caltech_train_df['no_sessions'] >= 50]\n","caltech_test_df = caltech_full_df[(caltech_full_df['connectionTime'] >= '2021-06-01') & (caltech_full_df['connectionTime'] <= '2021-06-30')]\n","\n","JPL_train_df=JPL_train_df[JPL_train_df['no_sessions'] >= 30]\n","\n","# Convert to NumPy arrays if necessary\n","Caltech_data_training = caltech_train_df.values\n","Caltech_data_testing = caltech_test_df.values\n","JPL_training_data = JPL_train_df.values\n","JPL_testing_data = JPL_test_df.values\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":151,"status":"ok","timestamp":1713071370465,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"nqNBVVmQ7xLt","outputId":"bfd59fce-a4f8-4319-9d5f-8ba21014bb41"},"outputs":[],"source":["JPL_testing_data"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":170,"status":"ok","timestamp":1713071210838,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"obL0PJo67d1C"},"outputs":[],"source":["Caltech_data_training=Caltech_data_training[:,1:]\n","Caltech_data_testing=Caltech_data_testing[:,1:]\n","\n","JPL_training_data=JPL_training_data[:,1:]\n","JPL_testing_data=JPL_testing_data[:,1:]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":132,"status":"ok","timestamp":1713071233829,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"4aLgtnZQ73qq"},"outputs":[],"source":["sorted_caltech_training = sorted(Caltech_data_training, key=lambda x: x[3])\n","sorted_caltech_training=np.delete(sorted_caltech_training, [1], axis=1)\n","sorted_caltech_testing = sorted(Caltech_data_testing, key=lambda x: x[3])\n","sorted_caltech_testing=np.delete(sorted_caltech_testing, [1], axis=1)\n","\n","\n","sorted_JPL_training = sorted(JPL_training_data, key=lambda x: x[3])\n","sorted_JPL_training=np.delete(sorted_JPL_training, [1], axis=1)\n","sorted_JPL_testing = sorted(JPL_testing_data, key=lambda x: x[3])\n","sorted_JPL_testing=np.delete(sorted_JPL_testing, [1], axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":148,"status":"ok","timestamp":1713071235140,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"oYdRkjFB-JJa","outputId":"fd961c04-74dd-43a1-9c90-e502f0c66eb1"},"outputs":[],"source":["sorted_JPL_testing"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":336,"status":"ok","timestamp":1713070120289,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"cauK-Fj48e--"},"outputs":[],"source":["def convert_to_interval(datetime_str):\n","    # Extract time string from the datetime string\n","    time_str = datetime_str.split()[1]\n","\n","    # Convert time string into hours, minutes, and seconds\n","    hours, minutes, seconds = map(int, time_str.split(':'))\n","\n","    # Calculate total minutes\n","    total_minutes = hours * 60 + minutes\n","\n","    # Find the nearest lower multiple of 15 for the minutes\n","    interval_minutes = (total_minutes // 15) * 15\n","\n","    # Convert back to hour and minute\n","    interval_hour, interval_minute = divmod(interval_minutes, 60)\n","\n","    # Return as a formatted string\n","    return \"{:02d}:{:02d}\".format(interval_hour, interval_minute)\n","\n","# Apply the conversion to the data\n","sorted_caltech_training[:, 0] = np.vectorize(convert_to_interval)(sorted_caltech_training[:, 0])\n","sorted_caltech_testing[:, 0] = np.vectorize(convert_to_interval)(sorted_caltech_testing[:, 0])\n","\n","sorted_JPL_training[:, 0] = np.vectorize(convert_to_interval)(sorted_JPL_training[:, 0])\n","sorted_JPL_testing[:, 0] = np.vectorize(convert_to_interval)(sorted_JPL_testing[:, 0])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":318,"status":"ok","timestamp":1713070124414,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"8mow8R_-Clt5"},"outputs":[],"source":["sorted_caltech_training[:, 3] = sorted_caltech_training[:, 3].astype(float) * 60\n","sorted_caltech_training[:, 3] = np.round(sorted_caltech_training[:, 3].astype(float) / 10) * 10\n","sorted_caltech_testing[:, 3] = sorted_caltech_testing[:, 3].astype(float) * 60\n","sorted_caltech_testing[:, 3] = np.round(sorted_caltech_testing[:, 3].astype(float) / 10) * 10\n","\n","sorted_JPL_training[:, 3] = sorted_JPL_training[:, 3].astype(float) * 60\n","sorted_JPL_training[:, 3] = np.round(sorted_JPL_training[:, 3].astype(float) / 10) * 10\n","sorted_JPL_testing[:, 3] = sorted_JPL_testing[:, 3].astype(float) * 60\n","sorted_JPL_testing[:, 3] = np.round(sorted_JPL_testing[:, 3].astype(float) / 10) * 10"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":128,"status":"ok","timestamp":1713071284635,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"Lup2q-2PIn3B"},"outputs":[],"source":["#Make training and testing set have the same user IDs\n","users_from_training_caltech = set(sorted_caltech_training[:, 2])\n","mask_caltech = np.isin(sorted_caltech_testing[:, 2], list(users_from_training_caltech))\n","sorted_caltech_testing = sorted_caltech_testing[mask_caltech]\n","users_from_testing_caltech = set(sorted_caltech_testing[:, 2])\n","mask_caltech = np.isin(sorted_caltech_training[:, 2], list(users_from_testing_caltech))\n","sorted_caltech_training = sorted_caltech_training[mask_caltech]\n","\n","users_from_training = set(sorted_JPL_training[:, 2])\n","mask = np.isin(sorted_JPL_testing[:, 2], list(users_from_training))\n","sorted_JPL_testing = sorted_JPL_testing[mask]\n","\n","users_from_testing = set(sorted_JPL_testing[:, 2])\n","mask = np.isin(sorted_JPL_training[:, 2], list(users_from_testing))\n","sorted_JPL_training = sorted_JPL_training[mask]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1713071286590,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"HDJ-orzzY6o1","outputId":"3838e8e6-dfba-434b-c0fe-5afb0e04577c"},"outputs":[],"source":["#User IDs in training set\n","unique_elements_cal = np.unique(sorted_caltech_training[:, 2])\n","print(len(unique_elements_cal))\n","\n","unique_elements = np.unique(sorted_JPL_training[:, 2])\n","print(len(unique_elements))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":161,"status":"ok","timestamp":1713071300783,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"tP5pOFZIAXBW","outputId":"f86afa9d-f1be-4039-8124-c1dfa2cb1dbd"},"outputs":[],"source":["sorted_JPL_testing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":158,"status":"ok","timestamp":1713071902622,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"GcN1zcYUBi9t","outputId":"610cb4c3-4bf7-4745-a218-28f9c2d2c02a"},"outputs":[],"source":["# Since we encountered a TypeError due to using a Series as a subset, let's correct this.\n","# We'll create a new column with just the date component of the datetime to use in drop_duplicates.\n","df = pd.DataFrame(sorted_JPL_testing, columns=['datetime', 'value1', 'userID', 'value2', 'value3'])\n","\n","# Convert the 'datetime' column to datetime objects\n","df['datetime'] = pd.to_datetime(df['datetime'])\n","\n","# Create a new column with just the date component\n","df['date'] = df['datetime'].dt.date\n","\n","# Sort the DataFrame based on userID and datetime to ensure the earliest dates come first\n","df = df.sort_values(by=['userID', 'datetime'])\n","\n","# Remove duplicates by userID and date, keeping the first (which is the earliest due to the sort)\n","df_unique = df.drop_duplicates(subset=['userID', 'date'], keep='first')\n","\n","df_unique.reset_index(drop=True, inplace=True)\n","#df_unique.drop(columns='date', inplace=True)  # Drop the date column as it was just for processing\n","df_unique\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":158,"status":"ok","timestamp":1713072130606,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"QeRjBgpVCdw9","outputId":"d6897fe2-2297-4da5-a53e-ff5d4bbe5eb0"},"outputs":[],"source":["# To find the most common date, we will use the 'date' column which we previously created.\n","# We will count the occurrences of each date and then find the most common one.\n","\n","# Count the occurrences of each date\n","date_counts = df_unique['date'].value_counts()\n","\n","# Find the most common date\n","most_common_date = date_counts.idxmax()\n","most_common_count = date_counts.max()\n","\n","most_common_date, most_common_count\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":557},"executionInfo":{"elapsed":152,"status":"ok","timestamp":1713072132021,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"ywG9pz7tDPce","outputId":"853dbf56-6dcd-425e-ec07-9dd87913974f"},"outputs":[],"source":["# Filtering the DataFrame to get the rows with the most common date\n","rows_with_most_common_date = df_unique[df_unique['date'] == most_common_date]\n","\n","rows_with_most_common_date.drop(columns='date', inplace=True)  # Drop the date column as it was just for processing\n","rows_with_most_common_date.reset_index(drop=True, inplace=True)\n","rows_with_most_common_date\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":174,"status":"ok","timestamp":1713071605397,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"OcOxbN_lY6HV","outputId":"82de9a10-ff0e-42c7-924e-277272bb069c"},"outputs":[],"source":["#User IDs in testing set\n","unique_elements_cal = np.unique(sorted_caltech_testing[:, 2])\n","print(len(unique_elements_cal))\n","\n","unique_elements = np.unique(sorted_JPL_testing[:, 2])\n","print(len(unique_elements))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":274,"status":"ok","timestamp":1713070137247,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"FMmyF6rrDmuV"},"outputs":[],"source":["# Convert time strings to total minutes for sorting\n","total_minutes_caltech_training = np.array([int(time.split(':')[0])*60 + int(time.split(':')[1]) for time in sorted_caltech_training[:, 0]])\n","total_minutes_caltech_testing = np.array([int(time.split(':')[0])*60 + int(time.split(':')[1]) for time in sorted_caltech_testing[:, 0]])\n","\n","total_minutes_JPL_training = np.array([int(time.split(':')[0])*60 + int(time.split(':')[1]) for time in sorted_JPL_training[:, 0]])\n","total_minutes_JPL_testing = np.array([int(time.split(':')[0])*60 + int(time.split(':')[1]) for time in sorted_JPL_testing[:, 0]])\n","\n","# Argsort first by the 3rd column and then by total_minutes\n","indices_caltech_training = np.lexsort((total_minutes_caltech_training, sorted_caltech_training[:, 2].astype(int)))\n","indices_caltech_testing = np.lexsort((total_minutes_caltech_testing, sorted_caltech_testing[:, 2].astype(int)))\n","\n","indices_JPL_training = np.lexsort((total_minutes_JPL_training, sorted_JPL_training[:, 2].astype(int)))\n","indices_JPL_testing = np.lexsort((total_minutes_JPL_testing, sorted_JPL_testing[:, 2].astype(int)))\n","# Use the sorted indices to reorder the array\n","sorted_data_caltech_training = sorted_caltech_training[indices_caltech_training]\n","sorted_data_caltech_testing = sorted_caltech_testing[indices_caltech_testing]\n","\n","sorted_data_JPL_training = sorted_JPL_training[indices_JPL_training]\n","sorted_data_JPL_testing = sorted_JPL_testing[indices_JPL_testing]"]},{"cell_type":"markdown","metadata":{"id":"Dj_z3mUiiZ7k"},"source":["#Data Processing for Correlation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bv-yX86viel6"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","# Define the file paths\n","file_path1 = '/content/drive/MyDrive/SuperUROP /Data Analysis/caltech_training_data.csv'\n","file_path2 = '/content/drive/MyDrive/SuperUROP /Data Analysis/caltech_testing_data.csv'\n","file_path3 = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_training_data.csv'\n","file_path4  = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_testing_data.csv'\n","# Use pandas to read the CSV files and then convert them to NumPy arrays\n","caltech_train = pd.read_csv(file_path1).values\n","caltech_test = pd.read_csv(file_path2).values\n","\n","JPL_train = pd.read_csv(file_path3).values\n","JPL_test=pd.read_csv(file_path4).values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6M_jEKbSf7Aw"},"outputs":[],"source":["import pandas as pd\n","\n","# Define the file paths\n","file_path1 = '/content/drive/MyDrive/SuperUROP /Data Analysis/caltech_training_data_full.csv'\n","file_path3 = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_training_data.csv'\n","file_path4 = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_testing_data.csv'\n","\n","# Use pandas to read the CSV files\n","caltech_full_df = pd.read_csv(file_path1)\n","\n","JPL_train_df = pd.read_csv(file_path3)\n","JPL_test_df = pd.read_csv(file_path4)\n","\n","# Filter the caltech_full DataFrame\n","caltech_full_df = caltech_full_df[caltech_full_df['duration'] <= 24]\n","caltech_train_df = caltech_full_df[(caltech_full_df['connectionTime'] >= '2021-03-01') & (caltech_full_df['connectionTime'] <= '2021-05-31')]\n","caltech_train_df= caltech_train_df[caltech_train_df['no_sessions'] >= 50]\n","caltech_test_df = caltech_full_df[(caltech_full_df['connectionTime'] >= '2021-06-01') & (caltech_full_df['connectionTime'] <= '2021-06-30')]\n","\n","JPL_train_df= JPL_train_df[JPL_train_df['no_sessions'] >= 30]\n","\n","# Convert to NumPy arrays if necessary\n","caltech_train = caltech_train_df.values\n","caltech_test = caltech_test_df.values\n","JPL_train = JPL_train_df.values\n","JPL_test = JPL_test_df.values\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ryH1EV4Tivfw"},"outputs":[],"source":["#Remove row number (in 1st column)\n","caltech_train=caltech_train[:,1:]\n","caltech_test=caltech_test[:,1:]\n","\n","JPL_train=JPL_train[:,1:]\n","JPL_test=JPL_test[:,1:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yqR_BKSEiwG1"},"outputs":[],"source":["#Remove departure time (2nd column)\n","# Convert arrival date to hour and find day of the week\n","from datetime import datetime\n","\n","def convert_time_and_day(data_array):\n","    \"\"\"\n","    Converts the time from HH:MM to HH.XX format and appends the day of the week to it.\n","    Also, removes the second column.\n","    \"\"\"\n","    transformed_data = []\n","    for row in data_array:\n","        # Convert the arrival time to HH.XX format\n","        time_obj = datetime.strptime(row[0], '%Y-%m-%d %H:%M:%S')\n","        new_time = time_obj.hour + (time_obj.minute / 60.0)\n","\n","        # Convert the date to a day of the week\n","        day_of_week = time_obj.strftime('%A')\n","        new_time = str(new_time) + \" \" + day_of_week\n","\n","        # Create a new row excluding the second column\n","        new_row = [new_time] + list(row[2:])\n","        transformed_data.append(new_row)\n","\n","    return np.array(transformed_data)\n","\n","caltech_train=convert_time_and_day(caltech_train)\n","caltech_test=convert_time_and_day(caltech_test)\n","JPL_train=convert_time_and_day(JPL_train)\n","JPL_test=convert_time_and_day(JPL_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0gPvRN6Ji0CH"},"outputs":[],"source":["def day_to_number(day):\n","    \"\"\"Converts a day of the week to its corresponding discrete value.\"\"\"\n","    days = {\n","        'Monday': 1,\n","        'Tuesday': 2,\n","        'Wednesday': 3,\n","        'Thursday': 4,\n","        'Friday': 5,\n","        'Saturday': 6,\n","        'Sunday': 7\n","    }\n","    return days[day]\n","\n","def separate_time_and_day(data_array):\n","    \"\"\"\n","    Separates the time and day in the given column,\n","    and converts the day into a discrete value between 1 and 7.\n","\n","    \"\"\"\n","    transformed_data = []\n","    for row in data_array:\n","        time_day_str = row[0]\n","        time, day = time_day_str.split()\n","        time = float(time)\n","        day_num = day_to_number(day)\n","\n","        # Create a new row with separated time and day number\n","        new_row = [time, day_num] + list(row[1:])\n","        transformed_data.append(new_row)\n","\n","    return np.array(transformed_data)\n","\n","caltech_train=separate_time_and_day(caltech_train)\n","caltech_test=separate_time_and_day(caltech_test)\n","JPL_train=separate_time_and_day(JPL_train)\n","JPL_test=separate_time_and_day(JPL_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jCp72xubi25z"},"outputs":[],"source":["#Make training and testing set have the same user IDs\n","users_from_training_caltech = set(caltech_train[:, 3])\n","mask_caltech = np.isin(caltech_test[:, 3], list(users_from_training_caltech))\n","caltech_test = caltech_test[mask_caltech]\n","users_from_testing_caltech = set(caltech_test[:, 3])\n","mask_caltech = np.isin(caltech_train[:, 3], list(users_from_testing_caltech))\n","caltech_train = caltech_train[mask_caltech]\n","\n","users_from_training = set(JPL_train[:, 3])\n","mask = np.isin(JPL_test[:, 3], list(users_from_training))\n","JPL_test = JPL_test[mask]\n","users_from_testing = set(JPL_test[:, 3])\n","mask = np.isin(JPL_train[:, 3], list(users_from_testing))\n","JPL_train = JPL_train[mask]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tK1LdodKi4tq"},"outputs":[],"source":["caltech_train = np.array(caltech_train, dtype='float')\n","caltech_test = np.array(caltech_test, dtype='float')\n","JPL_train = np.array(JPL_train, dtype='float')\n","JPL_test = np.array(JPL_test, dtype='float')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VFX6dYNCi5ZS"},"outputs":[],"source":["from collections import defaultdict\n","\n","# Organize the arrival times by user ID for the entire dataset\n","user_times_cal = defaultdict(list)\n","user_times_JPL = defaultdict(list)\n","\n","# Accumulate all arrival times for each user in caltech_train\n","for row in caltech_train:\n","    arrival_time = float(row[0])\n","    user_id = row[3]\n","    user_times_cal[user_id].append(arrival_time)\n","\n","# Accumulate all arrival times for each user in JPL_train\n","for row in JPL_train:\n","    arrival_time = float(row[0])\n","    user_id = row[3]\n","    user_times_JPL[user_id].append(arrival_time)\n","\n","# Define the functions for hourly and half-hourly representations\n","def create_hourly_representation(times):\n","    hourly_vector = [0] * 24\n","    for time in times:\n","        hour = int(float(time))\n","        hourly_vector[hour] += 1\n","    return hourly_vector\n","\n","def create_half_hourly_representation(times):\n","    half_hourly_vector = [0] * 48\n","    for time in times:\n","        interval = int(float(time) * 2)  # Multiply by 2 for half-hourly intervals\n","        half_hourly_vector[interval] += 1\n","    return half_hourly_vector\n","\n","# Compute the hourly and half-hourly representations for each user\n","hourly_representations_cal = {}\n","half_hourly_representations_cal = {}\n","hourly_representations_JPL = {}\n","half_hourly_representations_JPL = {}\n","\n","for user_id, times in user_times_cal.items():\n","    hourly_representations_cal[user_id] = create_hourly_representation(times)\n","    half_hourly_representations_cal[user_id] = create_half_hourly_representation(times)\n","\n","for user_id, times in user_times_JPL.items():\n","    hourly_representations_JPL[user_id] = create_hourly_representation(times)\n","    half_hourly_representations_JPL[user_id] = create_half_hourly_representation(times)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GXrAaEx5i7aC"},"outputs":[],"source":["import numpy as np\n","from collections import defaultdict\n","\n","# Organize the energy consumption durations by user ID for the entire dataset\n","user_durations_cal = defaultdict(list)\n","user_durations_JPL = defaultdict(list)\n","\n","# Accumulate all energy consumption durations for each user in caltech_train\n","for row in caltech_train:\n","    stay_duration = float(row[4])\n","    user_id = row[3]\n","    user_durations_cal[user_id].append(stay_duration)\n","\n","# Accumulate all energy consumption durations for each user in JPL_train\n","for row in JPL_train:\n","    stay_duration = float(row[4])\n","    user_id = row[3]\n","    user_durations_JPL[user_id].append(stay_duration)\n","\n","# Define functions for hourly and half-hourly representations\n","def create_hourly_representation_duration(durations):\n","    hourly_vector = [0] * 24\n","    for duration in durations:\n","        index = int(float(duration))\n","        hourly_vector[index] += 1\n","    return hourly_vector\n","\n","def create_half_hourly_representation_duration(durations):\n","    half_hourly_vector = [0] * 48\n","    for duration in durations:\n","        interval = int(float(duration) * 2)\n","        half_hourly_vector[interval] += 1\n","    return half_hourly_vector\n","\n","# Compute the hourly and half-hourly representations for each user\n","hourly_representations_duration_cal = {}\n","half_hourly_representations_duration_cal = {}\n","hourly_representations_duration_JPL = {}\n","half_hourly_representations_duration_JPL = {}\n","\n","for user_id, durations in user_durations_cal.items():\n","    hourly_representations_duration_cal[user_id] = create_hourly_representation_duration(durations)\n","    half_hourly_representations_duration_cal[user_id] = create_half_hourly_representation_duration(durations)\n","\n","for user_id, durations in user_durations_JPL.items():\n","    hourly_representations_duration_JPL[user_id] = create_hourly_representation_duration(durations)\n","    half_hourly_representations_duration_JPL[user_id] = create_half_hourly_representation_duration(durations)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iarxO4zzi_rN"},"outputs":[],"source":["def concatenate_representations(dict1, dict2):\n","    concatenated = {}\n","    for key in set(dict1.keys()) | set(dict2.keys()):  # union of keys from both dicts\n","        list1 = dict1.get(key, [])\n","        list2 = dict2.get(key, [])\n","\n","        # Concatenate the lists\n","        concatenated_list = list1 + list2\n","\n","        concatenated[key] = concatenated_list\n","\n","    return concatenated\n","\n","# Example usage\n","concatenated_hourly_cal = concatenate_representations(hourly_representations_cal, hourly_representations_duration_cal)\n","concatenated_half_hourly_cal = concatenate_representations(half_hourly_representations_cal, half_hourly_representations_duration_cal)\n","\n","concatenated_hourly_JPL = concatenate_representations(hourly_representations_JPL, hourly_representations_duration_JPL)\n","concatenated_half_hourly_JPL = concatenate_representations(half_hourly_representations_JPL, half_hourly_representations_duration_JPL)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1zMxvl7Xtu4Y"},"source":["#Calculate correlations"]},{"cell_type":"markdown","metadata":{"id":"M5_76BGktw--"},"source":["##Cosine similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bCSqIIIrjAOw"},"outputs":[],"source":["from sklearn.metrics import pairwise_distances\n","import pandas as pd\n","\n","def data_to_dataframe(data):\n","    return pd.DataFrame.from_dict(data, orient='index')\n","\n","def compute_cosine_similarity(df):\n","    cosine_distance = pairwise_distances(df, metric='cosine')\n","    cosine_similarity = 1 - cosine_distance\n","\n","    # Set the user IDs as row and column names to preserve them\n","    cosine_similarity_df = pd.DataFrame(cosine_similarity, index=df.index, columns=df.index)\n","    return cosine_similarity_df\n","\n","# Compute cosine similarity for each concatenated dataset\n","df_hourly_cal = data_to_dataframe(concatenated_hourly_cal)\n","similarities_hourly_cal = compute_cosine_similarity(df_hourly_cal)\n","\n","df_half_hourly_cal = data_to_dataframe(concatenated_half_hourly_cal)\n","similarities_half_hourly_cal = compute_cosine_similarity(df_half_hourly_cal)\n","\n","df_hourly_JPL = data_to_dataframe(concatenated_hourly_JPL)\n","similarities_hourly_JPL = compute_cosine_similarity(df_hourly_JPL)\n","\n","df_half_hourly_JPL = data_to_dataframe(concatenated_half_hourly_JPL)\n","similarities_half_hourly_JPL = compute_cosine_similarity(df_half_hourly_JPL)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"D_nLixUut0Ai"},"source":["#Pearson correlation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-n2efXNjDuj"},"outputs":[],"source":["import numpy as np\n","\n","def compute_pearson_correlation(v1, v2):\n","    # Handling cases where vectors are too short or contain constant values\n","    if len(v1) > 1 and len(v2) > 1 and np.std(v1) * np.std(v2) != 0:\n","        return np.corrcoef(v1, v2)[0, 1]\n","    else:\n","        return None  # Returning None for cases where correlation is not defined\n","\n","import pandas as pd\n","\n","def compute_pearson_correlation_matrix_df(representations):\n","    user_ids = list(representations.keys())\n","    n = len(user_ids)\n","    correlation_matrix = np.zeros((n, n))\n","\n","    for i in range(n):\n","        for j in range(i, n):  # Compute only for one triangle and mirror it as the matrix is symmetric\n","            if i != j:\n","                corr = compute_pearson_correlation(representations[user_ids[i]], representations[user_ids[j]])\n","                correlation_matrix[i, j] = corr if corr is not None else 0\n","                correlation_matrix[j, i] = correlation_matrix[i, j]  # Mirror the value\n","            else:\n","                correlation_matrix[i, j] = 1  # Self-correlation is always 1\n","\n","    # Convert to DataFrame for better usability\n","    correlation_df = pd.DataFrame(correlation_matrix, index=user_ids, columns=user_ids)\n","    return correlation_df\n","\n","similarities_hourly_cal = compute_pearson_correlation_matrix_df(concatenated_hourly_cal)\n","similarities_half_hourly_cal = compute_pearson_correlation_matrix_df(concatenated_half_hourly_cal)\n","\n","similarities_hourly_JPL = compute_pearson_correlation_matrix_df(concatenated_hourly_JPL)\n","similarities_half_hourly_JPL = compute_pearson_correlation_matrix_df(concatenated_half_hourly_JPL)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DyXSQv2zsh4f"},"source":["#Spearman Correlation  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FTCqePtdslKi"},"outputs":[],"source":["from scipy.stats import spearmanr\n","import pandas as pd\n","\n","def data_to_dataframe(data):\n","    return pd.DataFrame.from_dict(data, orient='index')\n","\n","def calculate_spearman_correlation(df):\n","    # Transpose the DataFrame to make users (rows) into columns\n","    df_transposed = df.T\n","\n","    # Initialize an empty DataFrame to store the results\n","    spearman_corr_df = pd.DataFrame(index=df.index, columns=df.index)\n","\n","    # Calculate Spearman correlation for each pair of users (now columns)\n","    for user1 in df_transposed.columns:\n","        for user2 in df_transposed.columns:\n","            # Calculate Spearman correlation\n","            corr, _ = spearmanr(df_transposed[user1], df_transposed[user2])\n","            spearman_corr_df.loc[user1, user2] = corr\n","\n","    # Convert all values to numeric\n","    return spearman_corr_df.apply(pd.to_numeric)\n","\n","# Compute Spearman correlation for each concatenated dataset\n","df_hourly_cal = data_to_dataframe(concatenated_hourly_cal)\n","similarities_hourly_cal = calculate_spearman_correlation(df_hourly_cal)\n","\n","df_half_hourly_cal = data_to_dataframe(concatenated_half_hourly_cal)\n","similarities_half_hourly_cal = calculate_spearman_correlation(df_half_hourly_cal)\n","\n","df_hourly_JPL = data_to_dataframe(concatenated_hourly_JPL)\n","similarities_hourly_JPL = calculate_spearman_correlation(df_hourly_JPL)\n","\n","df_half_hourly_JPL = data_to_dataframe(concatenated_half_hourly_JPL)\n","similarities_half_hourly_JPL = calculate_spearman_correlation(df_half_hourly_JPL)\n"]},{"cell_type":"markdown","metadata":{"id":"qqR6YlRM3RDg"},"source":["#Manhattan Distance (converted to similarity)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n1uFEtPE3V4H"},"outputs":[],"source":["import pandas as pd\n","from sklearn.metrics import pairwise_distances\n","\n","def data_to_dataframe(data):\n","    return pd.DataFrame.from_dict(data, orient='index')\n","\n","def manhattan_similarity(df):\n","    # Calculate the Manhattan distances\n","    manhattan_distances = pairwise_distances(df, metric='manhattan')\n","\n","    # Normalize the distances\n","    max_distance = manhattan_distances.max()\n","    normalized_distances = manhattan_distances / max_distance\n","\n","    # Convert distances to similarity scores\n","    similarity_scores = 1 - normalized_distances\n","\n","    # Convert to a DataFrame for easier interpretation\n","    similarity_df = pd.DataFrame(similarity_scores, index=df.index, columns=df.index)\n","    return similarity_df\n","\n","# Compute Spearman correlation for each concatenated dataset\n","df_hourly_cal = data_to_dataframe(concatenated_hourly_cal)\n","similarities_hourly_cal = manhattan_similarity(df_hourly_cal)\n","\n","df_half_hourly_cal = data_to_dataframe(concatenated_half_hourly_cal)\n","similarities_half_hourly_cal = manhattan_similarity(df_half_hourly_cal)\n","\n","df_hourly_JPL = data_to_dataframe(concatenated_hourly_JPL)\n","similarities_hourly_JPL = manhattan_similarity(df_hourly_JPL)\n","\n","df_half_hourly_JPL = data_to_dataframe(concatenated_half_hourly_JPL)\n","similarities_half_hourly_JPL = manhattan_similarity(df_half_hourly_JPL)"]},{"cell_type":"markdown","metadata":{"id":"DHWES8XuuAWV"},"source":["#Find most correlated users (using raw values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bXDvL8QyjHFK"},"outputs":[],"source":["#Using threshold\n","def most_correlated_users(matrix, threshold=0.65):\n","    \"\"\"\n","    For each user, find the top correlated users with correlations above 0.5.\n","\n","    :param matrix: Cosine similarity matrix.\n","    :param threshold: Minimum correlation threshold for considering a user as correlated.\n","    :return: Dictionary with keys being user IDs and values being lists of top correlated user IDs.\n","    \"\"\"\n","    result = {}\n","\n","    for user_index in matrix.index:\n","        # Subtract self-similarity value for the current user\n","        user_similarities = matrix.loc[user_index].drop(user_index)\n","\n","        # Filter users with correlation above the threshold\n","        filtered_users = user_similarities[abs(user_similarities) > threshold]\n","\n","        # Sort by descending correlation\n","        sorted_users = filtered_users.sort_values(ascending=False)\n","\n","        # Append users above the threshold to the result\n","        result[user_index] = sorted_users.index.tolist()\n","\n","    return result\n","\n","# Assuming `similarities_hourly_cal` is your cosine similarity matrix for the entire dataset\n","most_corr_users_hourly_cal = most_correlated_users(similarities_hourly_cal)\n","most_corr_users_half_hourly_cal = most_correlated_users(similarities_half_hourly_cal)\n","\n","# Similarly for JPL data\n","most_corr_users_hourly_JPL = most_correlated_users(similarities_hourly_JPL)\n","most_corr_users_half_hourly_JPL = most_correlated_users(similarities_half_hourly_JPL)"]},{"cell_type":"markdown","metadata":{"id":"kFojJvImuCjw"},"source":["#Calculate mode of each user (using linear interpolation)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3EuXTgXhaOuF"},"outputs":[],"source":["#Mode calculation with linear interpolation\n","from collections import Counter\n","def compute_mode_per_user_per_interval(data):\n","    \"\"\"Compute the mode per user per time interval with linear interpolation.\"\"\"\n","    user_groups = {}\n","    mode_per_user_per_interval = {}\n","\n","    # Group by user\n","    for row in data:\n","        if row[2] not in user_groups:\n","            user_groups[row[2]] = []\n","        user_groups[row[2]].append(row)\n","\n","    # Calculate modes per user\n","    for user, entries in user_groups.items():\n","        time_intervals = sorted(list(set([entry[0] for entry in entries])))\n","        modes = {}\n","        for interval in time_intervals:\n","            session_values = [entry[3] for entry in entries if entry[0] == interval]\n","            counts = Counter(session_values)\n","            highest_freq = max(counts.values())\n","            common_vals = [key for key, val in counts.items() if val == highest_freq]\n","\n","            # If there's a single mode, use it; else, average them\n","            modes[interval] = sum(common_vals) / len(common_vals)\n","\n","        # Linearly interpolate for missing modes\n","        all_intervals = sorted(list(set(data[:, 0])))\n","        for i, interval in enumerate(all_intervals):\n","            if interval not in modes:\n","                # Find previous and next known modes\n","                prev_mode = next((modes[prev_int] for prev_int in reversed(all_intervals[:i]) if prev_int in modes), None)\n","                next_mode = next((modes[next_int] for next_int in all_intervals[i+1:] if next_int in modes), None)\n","\n","                # If both previous and next modes exist, interpolate\n","                if prev_mode is not None and next_mode is not None:\n","                    gap_size = all_intervals[i+1:].index(next((next_int for next_int in all_intervals[i+1:] if next_int in modes))) + 1\n","                    increment = (next_mode - prev_mode) / (gap_size + 1)\n","                    modes[interval] = prev_mode + increment\n","\n","                # If no next mode exists, keep the mode same as the previous mode\n","                elif prev_mode is not None:\n","                    modes[interval] = prev_mode\n","\n","                # If no previous mode exists, keep the mode same as the next mode\n","                elif next_mode is not None:\n","                    modes[interval] = next_mode\n","\n","        mode_per_user_per_interval[user] = modes\n","\n","    return mode_per_user_per_interval\n","\n","# Update mode_per_user_per_interval using the new function\n","mode_per_user_per_interval_caltech_interpolate = compute_mode_per_user_per_interval(sorted_data_caltech_training)\n","mode_per_user_per_interval_JPL_interpolate = compute_mode_per_user_per_interval(sorted_data_JPL_training)"]},{"cell_type":"markdown","metadata":{"id":"iwJ0NamGuMkn"},"source":["#Prediciction using comination of own user's and correlated users modes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E24-gAWkj470"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","# Assuming mode_per_user_per_interval_caltech_interpolate is your prediction dataset\n","# and correlation_matrix is your correlation matrix\n","\n","def weighted_average_prediction(user_id, time_slot, mode_per_user_per_interval, most_corr_users, correlation_matrix):\n","    # Initialize weighted predictions with the user's own prediction or a default value\n","    user_prediction = mode_per_user_per_interval.get(user_id, {}).get(time_slot, 0)  # You can change 0 to a default value if needed\n","    weighted_predictions = user_prediction\n","    total_correlation=1\n","    # Add weighted predictions of the most correlated users\n","    for corr_user_id in most_corr_users[user_id]:\n","        corr_value = correlation_matrix.loc[user_id, corr_user_id]\n","        corr_user_prediction = mode_per_user_per_interval.get(corr_user_id, {}).get(time_slot, 0)  # You can change 0 to a default value if needed\n","\n","        # Add to weighted predictions\n","        weighted_predictions += corr_user_prediction * corr_value\n","        total_correlation+=corr_value\n","    weighted_predictions=weighted_predictions/total_correlation\n","    return weighted_predictions"]},{"cell_type":"markdown","metadata":{"id":"Amw6xeVKueIo"},"source":["#Calculate SMAPE"]},{"cell_type":"markdown","metadata":{"id":"y13pSq0_uc5f"},"source":["##SMAPE Caltech"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":201,"status":"ok","timestamp":1712811096870,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"nHgHkowMmhEt","outputId":"14fd696e-f888-4624-eae5-159f891280f8"},"outputs":[],"source":["y_pred_caltech_interpolate_list = []\n","y_true_caltech_interpolate_list = []\n","\n","def calculate_smape(y_true, y_pred):\n","    \"\"\"Compute the SMAPE\"\"\"\n","    n = len(y_true)\n","    smape_val = (1/ n) * np.sum(np.abs(y_true - y_pred) / (np.abs(y_true+y_pred)))*100\n","    return smape_val\n","\n","for row in sorted_data_caltech_testing:\n","    user_id = row[2]  # Assuming user_id is at index 2\n","    time_slot = row[0]  # Assuming time_slot is at index 0\n","    true_value=row[3]\n","    # Get user prediction\n","    if row[0] in mode_per_user_per_interval_caltech_interpolate.get(row[2], {}):\n","        y_true_caltech_interpolate_list.append(row[3])\n","        user_prediction=mode_per_user_per_interval_caltech_interpolate[row[2]][row[0]]\n","\n","    # Calculate weighted prediction using the function\n","        weighted_pred = weighted_average_prediction(user_id, time_slot, mode_per_user_per_interval_caltech_interpolate, most_corr_users_hourly_cal, similarities_hourly_cal)\n","\n","    # If weighted_pred is 0 (i.e., no data available), use user_prediction as a fallback\n","        y_pred_caltech_interpolate_list.append(weighted_pred if weighted_pred != 0 else user_prediction)\n","\n","\n","# Convert lists to numpy arrays for calculation\n","y_true_caltech_interpolate = np.array(y_true_caltech_interpolate_list, dtype=float)\n","y_pred_caltech_interpolate = np.array(y_pred_caltech_interpolate_list, dtype=float)\n","# Calculate SMAPE\n","smape_caltech_interpolate = calculate_smape(y_true_caltech_interpolate, y_pred_caltech_interpolate)\n","print(f'Individual SMAPE Caltech (interpolate): {smape_caltech_interpolate:.2f}%')\n"]},{"cell_type":"markdown","metadata":{"id":"Qbdj8KBjuaXb"},"source":["##SMAPE JPL"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":153,"status":"ok","timestamp":1712811098476,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"g--bNICarWoY","outputId":"eb12d0fa-3433-4d8b-fd93-647876eec869"},"outputs":[],"source":["y_pred_JPL_interpolate_list = []\n","y_true_JPL_interpolate_list = []\n","\n","def calculate_smape(y_true, y_pred):\n","    \"\"\"Compute the SMAPE\"\"\"\n","    n = len(y_true)\n","    smape_val = (1/ n) * np.sum(np.abs(y_true - y_pred) / (np.abs(y_true+y_pred)))*100\n","    return smape_val\n","\n","for row in sorted_data_JPL_testing:\n","    user_id = row[2]  # Assuming user_id is at index 2\n","    time_slot = row[0]  # Assuming time_slot is at index 0\n","    true_value=row[3]\n","    # Get user prediction\n","    if row[0] in mode_per_user_per_interval_JPL_interpolate.get(row[2], {}):\n","        y_true_JPL_interpolate_list.append(row[3])\n","        user_prediction=mode_per_user_per_interval_JPL_interpolate[row[2]][row[0]]\n","\n","    # Calculate weighted prediction using the function\n","        weighted_pred = weighted_average_prediction(user_id, time_slot, mode_per_user_per_interval_JPL_interpolate, most_corr_users_hourly_JPL, similarities_hourly_JPL)\n","\n","    # If weighted_pred is 0 (i.e., no data available), use user_prediction as a fallback\n","        y_pred_JPL_interpolate_list.append(weighted_pred if weighted_pred != 0 else user_prediction)\n","\n","\n","# Convert lists to numpy arrays for calculation\n","y_true_JPL_interpolate = np.array(y_true_JPL_interpolate_list, dtype=float)\n","y_pred_JPL_interpolate = np.array(y_pred_JPL_interpolate_list, dtype=float)\n","# Calculate SMAPE\n","smape_JPL_interpolate = calculate_smape(y_true_JPL_interpolate, y_pred_JPL_interpolate)\n","print(f'Individual SMAPE JPL (interpolate): {smape_JPL_interpolate:.2f}%')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":153,"status":"ok","timestamp":1712812071021,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":240},"id":"cEbiorwCii8J","outputId":"b0aeea5f-50c1-4613-a706-c869297bc56e"},"outputs":[],"source":["from collections import defaultdict\n","\n","# Initialize dictionaries to store true and predicted values for each user\n","y_true_by_user = defaultdict(list)\n","y_pred_by_user = defaultdict(list)\n","\n","# Function to calculate SMAPE\n","def calculate_smape(y_true, y_pred):\n","    \"\"\"Compute the SMAPE\"\"\"\n","    n = len(y_true)\n","    if n == 0:\n","        return np.nan  # Return NaN if there are no observations\n","    smape_val = (1/n) * np.sum(np.abs(y_pred - y_true) / (np.abs(y_true + y_pred))) * 100\n","    return smape_val\n","\n","# Iterate over the testing data to fill the dictionaries\n","for row in sorted_data_JPL_testing:\n","    user_id = row[2]  # Assuming user_id is at index 2\n","    time_slot = row[0]  # Assuming time_slot is at index 0\n","\n","    if time_slot in mode_per_user_per_interval_JPL_interpolate.get(user_id, {}):\n","        y_true_by_user[user_id].append(row[3])  # true_value is assumed to be at index 3\n","\n","        # Calculate the weighted prediction\n","        weighted_pred = weighted_average_prediction(user_id, time_slot, mode_per_user_per_interval_JPL_interpolate, most_corr_users_hourly_JPL, similarities_hourly_JPL)\n","\n","        # If weighted_pred is 0 (i.e., no data available), use user_prediction as a fallback\n","        user_prediction = mode_per_user_per_interval_JPL_interpolate[user_id][time_slot]\n","        y_pred_by_user[user_id].append(weighted_pred if weighted_pred != 0 else user_prediction)\n","\n","# Calculate SMAPE for each user\n","smape_values = []\n","for user_id in y_true_by_user:\n","    y_true = np.array(y_true_by_user[user_id], dtype=float)\n","    #print(len(y_true))\n","    y_pred = np.array(y_pred_by_user[user_id], dtype=float)\n","    #print(y_pred)\n","    user_smape = calculate_smape(y_true, y_pred)\n","    if not np.isnan(user_smape):  # Exclude users without predictions\n","        smape_values.append(user_smape)\n","        print(f'User {user_id} SMAPE: {user_smape:.2f}%')\n","\n","# Calculate and print the average SMAPE across users\n","if smape_values:\n","    average_smape = np.mean(smape_values)\n","    print(f'Average SMAPE across users: {average_smape:.2f}%')\n","else:\n","    print('No SMAPE values calculated.')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pjVqIS3Lf8I4"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOrr4N9pVuLCv08ME6B8n7Y","provenance":[{"file_id":"14nk9wv0SF1eX18j4ApXmHS6_LuV0xMq3","timestamp":1700094001992}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
