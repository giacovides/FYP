{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17035,"status":"ok","timestamp":1700101488762,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"DL0k10RoUPJW","outputId":"1fddee7a-5418-42ac-c89d-1bcfadfc46ed"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2809,"status":"ok","timestamp":1700101491564,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"M1Ot-FcdUzts"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","# Define the file paths\n","file_path1 = '/content/drive/MyDrive/SuperUROP /Data Analysis/caltech_training_data.csv'\n","file_path2 = '/content/drive/MyDrive/SuperUROP /Data Analysis/caltech_testing_data.csv'\n","file_path3 = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_training_data.csv'\n","file_path4  = '/content/drive/MyDrive/SuperUROP /Data Analysis/JPL_testing_data.csv'\n","# Use pandas to read the CSV files and then convert them to NumPy arrays\n","caltech_train = pd.read_csv(file_path1).values\n","caltech_test = pd.read_csv(file_path2).values\n","\n","JPL_train = pd.read_csv(file_path3).values\n","JPL_test=pd.read_csv(file_path4).values"]},{"cell_type":"markdown","metadata":{"id":"T-LH42jEdjoD"},"source":["#Data Processing for Random Forest\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1700101491565,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"WbbS_zZRVPX4"},"outputs":[],"source":["#Remove row number (in 1st column)\n","caltech_train=caltech_train[:,1:]\n","caltech_test=caltech_test[:,1:]\n","\n","JPL_train=JPL_train[:,1:]\n","JPL_test=JPL_test[:,1:]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":560,"status":"ok","timestamp":1700101492117,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"8P5wsz9EVgXL"},"outputs":[],"source":["#Remove departure time (2nd column)\n","# Convert arrival date to hour and find day of the week\n","from datetime import datetime\n","\n","def convert_time_and_day(data_array):\n","    \"\"\"\n","    Converts the time from HH:MM to HH.XX format and appends the day of the week to it.\n","    Also, removes the second column.\n","    \"\"\"\n","    transformed_data = []\n","    for row in data_array:\n","        # Convert the arrival time to HH.XX format\n","        time_obj = datetime.strptime(row[0], '%Y-%m-%d %H:%M:%S')\n","        new_time = time_obj.hour + (time_obj.minute / 60.0)\n","\n","        # Convert the date to a day of the week\n","        day_of_week = time_obj.strftime('%A')\n","        new_time = str(new_time) + \" \" + day_of_week\n","\n","        # Create a new row excluding the second column\n","        new_row = [new_time] + list(row[2:])\n","        transformed_data.append(new_row)\n","\n","    return np.array(transformed_data)\n","\n","caltech_train=convert_time_and_day(caltech_train)\n","caltech_test=convert_time_and_day(caltech_test)\n","JPL_train=convert_time_and_day(JPL_train)\n","JPL_test=convert_time_and_day(JPL_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1700101492117,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"KFJTpBVqd-jm"},"outputs":[],"source":["def day_to_number(day):\n","    \"\"\"Converts a day of the week to its corresponding discrete value.\"\"\"\n","    days = {\n","        'Monday': 1,\n","        'Tuesday': 2,\n","        'Wednesday': 3,\n","        'Thursday': 4,\n","        'Friday': 5,\n","        'Saturday': 6,\n","        'Sunday': 7\n","    }\n","    return days[day]\n","\n","def separate_time_and_day(data_array):\n","    \"\"\"\n","    Separates the time and day in the given column,\n","    and converts the day into a discrete value between 1 and 7.\n","\n","    \"\"\"\n","    transformed_data = []\n","    for row in data_array:\n","        time_day_str = row[0]\n","        time, day = time_day_str.split()\n","        time = float(time)\n","        day_num = day_to_number(day)\n","\n","        # Create a new row with separated time and day number\n","        new_row = [time, day_num] + list(row[1:])\n","        transformed_data.append(new_row)\n","\n","    return np.array(transformed_data)\n","\n","caltech_train=separate_time_and_day(caltech_train)\n","caltech_test=separate_time_and_day(caltech_test)\n","JPL_train=separate_time_and_day(JPL_train)\n","JPL_test=separate_time_and_day(JPL_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":273,"status":"ok","timestamp":1700101492385,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"OYcBnwTisZ8n"},"outputs":[],"source":["#Make training and testing set have the same user IDs\n","users_from_training_caltech = set(caltech_train[:, 3])\n","mask_caltech = np.isin(caltech_test[:, 3], list(users_from_training_caltech))\n","caltech_test = caltech_test[mask_caltech]\n","users_from_testing_caltech = set(caltech_test[:, 3])\n","mask_caltech = np.isin(caltech_train[:, 3], list(users_from_testing_caltech))\n","caltech_train = caltech_train[mask_caltech]\n","\n","users_from_training = set(JPL_train[:, 3])\n","mask = np.isin(JPL_test[:, 3], list(users_from_training))\n","JPL_test = JPL_test[mask]\n","users_from_testing = set(JPL_test[:, 3])\n","mask = np.isin(JPL_train[:, 3], list(users_from_testing))\n","JPL_train = JPL_train[mask]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1700101492795,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"eORUVi8wemPe"},"outputs":[],"source":["caltech_train = np.array(caltech_train, dtype='float')\n","caltech_test = np.array(caltech_test, dtype='float')\n","JPL_train = np.array(JPL_train, dtype='float')\n","JPL_test = np.array(JPL_test, dtype='float')"]},{"cell_type":"markdown","metadata":{"id":"yELKBYaHSvPO"},"source":["#Data processing for correllations (discretization)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":174,"status":"ok","timestamp":1700101493728,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"hBlAwEPNSyjh"},"outputs":[],"source":["from collections import defaultdict\n","\n","# Organize the arrival times by user ID for the entire dataset\n","user_times_cal = defaultdict(list)\n","user_times_JPL = defaultdict(list)\n","\n","# Accumulate all arrival times for each user in caltech_train\n","for row in caltech_train:\n","    arrival_time = float(row[0])\n","    user_id = row[3]\n","    user_times_cal[user_id].append(arrival_time)\n","\n","# Accumulate all arrival times for each user in JPL_train\n","for row in JPL_train:\n","    arrival_time = float(row[0])\n","    user_id = row[3]\n","    user_times_JPL[user_id].append(arrival_time)\n","\n","# Define the functions for hourly and half-hourly representations\n","def create_hourly_representation(times):\n","    hourly_vector = [0] * 24\n","    for time in times:\n","        hour = int(float(time))\n","        hourly_vector[hour] += 1\n","    return hourly_vector\n","\n","def create_half_hourly_representation(times):\n","    half_hourly_vector = [0] * 48\n","    for time in times:\n","        interval = int(float(time) * 2)  # Multiply by 2 for half-hourly intervals\n","        half_hourly_vector[interval] += 1\n","    return half_hourly_vector\n","\n","# Compute the hourly and half-hourly representations for each user\n","hourly_representations_cal = {}\n","half_hourly_representations_cal = {}\n","hourly_representations_JPL = {}\n","half_hourly_representations_JPL = {}\n","\n","for user_id, times in user_times_cal.items():\n","    hourly_representations_cal[user_id] = create_hourly_representation(times)\n","    half_hourly_representations_cal[user_id] = create_half_hourly_representation(times)\n","\n","for user_id, times in user_times_JPL.items():\n","    hourly_representations_JPL[user_id] = create_hourly_representation(times)\n","    half_hourly_representations_JPL[user_id] = create_half_hourly_representation(times)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":173,"status":"ok","timestamp":1700101496262,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"RbYqnzpoTJ7n"},"outputs":[],"source":["max_duration_caltech= int(np.round(caltech_train[:, 4].astype(float).max()))\n","max_duration_JPL= int(np.round(JPL_train[:, 4].astype(float).max()))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1700101496795,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"ORxUMNMwTKb6"},"outputs":[],"source":["import numpy as np\n","from collections import defaultdict\n","\n","# Organize the energy consumption durations by user ID for the entire dataset\n","user_durations_cal = defaultdict(list)\n","user_durations_JPL = defaultdict(list)\n","\n","# Accumulate all energy consumption durations for each user in caltech_train\n","for row in caltech_train:\n","    stay_duration = float(row[4])\n","    user_id = row[3]\n","    user_durations_cal[user_id].append(stay_duration)\n","\n","# Accumulate all energy consumption durations for each user in JPL_train\n","for row in JPL_train:\n","    stay_duration = float(row[4])\n","    user_id = row[3]\n","    user_durations_JPL[user_id].append(stay_duration)\n","\n","# Define functions for hourly and half-hourly representations\n","def create_hourly_representation_duration(durations):\n","    hourly_vector = [0] * 24\n","    for duration in durations:\n","        index = int(float(duration))\n","        hourly_vector[index] += 1\n","    return hourly_vector\n","\n","def create_half_hourly_representation_duration(durations):\n","    half_hourly_vector = [0] * 48\n","    for duration in durations:\n","        interval = int(float(duration) * 2)\n","        half_hourly_vector[interval] += 1\n","    return half_hourly_vector\n","\n","# Compute the hourly and half-hourly representations for each user\n","hourly_representations_duration_cal = {}\n","half_hourly_representations_duration_cal = {}\n","hourly_representations_duration_JPL = {}\n","half_hourly_representations_duration_JPL = {}\n","\n","for user_id, durations in user_durations_cal.items():\n","    hourly_representations_duration_cal[user_id] = create_hourly_representation_duration(durations)\n","    half_hourly_representations_duration_cal[user_id] = create_half_hourly_representation_duration(durations)\n","\n","for user_id, durations in user_durations_JPL.items():\n","    hourly_representations_duration_JPL[user_id] = create_hourly_representation_duration(durations)\n","    half_hourly_representations_duration_JPL[user_id] = create_half_hourly_representation_duration(durations)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":161,"status":"ok","timestamp":1700101503111,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"ea-OFUnwTpG-"},"outputs":[],"source":["def concatenate_representations(dict1, dict2):\n","    concatenated = {}\n","    for key in set(dict1.keys()) | set(dict2.keys()):  # union of keys from both dicts\n","        list1 = dict1.get(key, [])\n","        list2 = dict2.get(key, [])\n","\n","        # Concatenate the lists\n","        concatenated_list = list1 + list2\n","\n","        concatenated[key] = concatenated_list\n","\n","    return concatenated\n","\n","# Example usage\n","concatenated_hourly_cal = concatenate_representations(hourly_representations_cal, hourly_representations_duration_cal)\n","concatenated_half_hourly_cal = concatenate_representations(half_hourly_representations_cal, half_hourly_representations_duration_cal)\n","\n","concatenated_hourly_JPL = concatenate_representations(hourly_representations_JPL, hourly_representations_duration_JPL)\n","concatenated_half_hourly_JPL = concatenate_representations(half_hourly_representations_JPL, half_hourly_representations_duration_JPL)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"j6Ei18qKMM0P"},"source":["#Correlation calculation"]},{"cell_type":"markdown","metadata":{"id":"MpKvNIjFLwhJ"},"source":["##Cosine similarity calculation"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1345,"status":"ok","timestamp":1700101506025,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"tkQQql47Ultv"},"outputs":[],"source":["from sklearn.metrics import pairwise_distances\n","import pandas as pd\n","\n","def data_to_dataframe(data):\n","    return pd.DataFrame.from_dict(data, orient='index')\n","\n","def compute_cosine_similarity(df):\n","    cosine_distance = pairwise_distances(df, metric='cosine')\n","    cosine_similarity = 1 - cosine_distance\n","\n","    # Set the user IDs as row and column names to preserve them\n","    cosine_similarity_df = pd.DataFrame(cosine_similarity, index=df.index, columns=df.index)\n","    return cosine_similarity_df\n","\n","# Compute cosine similarity for each concatenated dataset\n","df_hourly_cal = data_to_dataframe(concatenated_hourly_cal)\n","similarities_hourly_cal = compute_cosine_similarity(df_hourly_cal)\n","\n","df_half_hourly_cal = data_to_dataframe(concatenated_half_hourly_cal)\n","similarities_half_hourly_cal = compute_cosine_similarity(df_half_hourly_cal)\n","\n","df_hourly_JPL = data_to_dataframe(concatenated_hourly_JPL)\n","similarities_hourly_JPL = compute_cosine_similarity(df_hourly_JPL)\n","\n","df_half_hourly_JPL = data_to_dataframe(concatenated_half_hourly_JPL)\n","similarities_half_hourly_JPL = compute_cosine_similarity(df_half_hourly_JPL)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"k3OsvvkML4p2"},"source":["##Pearson correlation calculation"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":567,"status":"ok","timestamp":1700109920513,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"_ZUpqfjarhtj"},"outputs":[],"source":["import numpy as np\n","\n","def compute_pearson_correlation(v1, v2):\n","    # Handling cases where vectors are too short or contain constant values\n","    if len(v1) > 1 and len(v2) > 1 and np.std(v1) * np.std(v2) != 0:\n","        return np.corrcoef(v1, v2)[0, 1]\n","    else:\n","        return None  # Returning None for cases where correlation is not defined\n","\n","import pandas as pd\n","\n","def compute_pearson_correlation_matrix_df(representations):\n","    user_ids = list(representations.keys())\n","    n = len(user_ids)\n","    correlation_matrix = np.zeros((n, n))\n","\n","    for i in range(n):\n","        for j in range(i, n):  # Compute only for one triangle and mirror it as the matrix is symmetric\n","            if i != j:\n","                corr = compute_pearson_correlation(representations[user_ids[i]], representations[user_ids[j]])\n","                correlation_matrix[i, j] = corr if corr is not None else 0\n","                correlation_matrix[j, i] = correlation_matrix[i, j]  # Mirror the value\n","            else:\n","                correlation_matrix[i, j] = 1  # Self-correlation is always 1\n","\n","    # Convert to DataFrame for better usability\n","    correlation_df = pd.DataFrame(correlation_matrix, index=user_ids, columns=user_ids)\n","    return correlation_df\n","\n","similarities_hourly_cal = compute_pearson_correlation_matrix_df(concatenated_hourly_cal)\n","similarities_half_hourly_cal = compute_pearson_correlation_matrix_df(concatenated_half_hourly_cal)\n","\n","similarities_hourly_JPL = compute_pearson_correlation_matrix_df(concatenated_hourly_JPL)\n","similarities_half_hourly_JPL = compute_pearson_correlation_matrix_df(concatenated_half_hourly_JPL)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ethVbrTjL9ta"},"source":["#Find most correlated users (using energy and raw values)\n","Raw threshold performs better (choose a threshold of 0.5/0.75 depending on the data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sjgEz85CgSCD"},"outputs":[],"source":["#Using energy\n","def most_correlated_users(matrix, threshold=0.5):\n","    \"\"\"\n","    For each user, calculate the top correlated users that cumulatively contribute to a given energy threshold.\n","\n","    :param matrix: Cosine similarity matrix.\n","    :param threshold: Energy threshold.\n","    :return: Dictionary with keys being user IDs and values being lists of top correlated user IDs.\n","    \"\"\"\n","    result = {}\n","\n","    for user_index in matrix.index:\n","        # Subtract self-similarity value for the current user\n","        user_similarities = matrix.loc[user_index].drop(user_index)\n","\n","        # Square values to get energy, then sort by descending energy\n","        sorted_users = user_similarities.map(np.square).sort_values(ascending=False)\n","\n","        # Calculate total energy\n","        total_energy = sorted_users.sum()\n","\n","        # Find the subset of users whose energy sums to the given threshold of the total energy\n","        cumulative_energy = 0\n","        selected_users = []\n","        for other_user, user_energy in sorted_users.items():\n","            cumulative_energy += user_energy\n","            selected_users.append(other_user)\n","            if cumulative_energy / total_energy >= threshold:\n","                break\n","\n","        result[user_index] = selected_users\n","\n","    return result\n","\n","# Assuming `similarities_hourly_cal` is your cosine similarity matrix for the entire dataset\n","most_corr_users_hourly_cal = most_correlated_users(similarities_hourly_cal)\n","most_corr_users_half_hourly_cal = most_correlated_users(similarities_half_hourly_cal)\n","\n","# Similarly for JPL data\n","most_corr_users_hourly_JPL = most_correlated_users(similarities_hourly_JPL)\n","most_corr_users_half_hourly_JPL = most_correlated_users(similarities_half_hourly_JPL)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":391,"status":"ok","timestamp":1700109924558,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"cAknC0g1Uw2f"},"outputs":[],"source":["#Using threshold\n","def most_correlated_users(matrix, threshold=0.75):\n","    \"\"\"\n","    For each user, find the top correlated users with correlations above 0.5.\n","\n","    :param matrix: Cosine similarity matrix.\n","    :param threshold: Minimum correlation threshold for considering a user as correlated.\n","    :return: Dictionary with keys being user IDs and values being lists of top correlated user IDs.\n","    \"\"\"\n","    result = {}\n","\n","    for user_index in matrix.index:\n","        # Subtract self-similarity value for the current user\n","        user_similarities = matrix.loc[user_index].drop(user_index)\n","\n","        # Filter users with correlation above the threshold\n","        filtered_users = user_similarities[abs(user_similarities) > threshold]\n","\n","        # Sort by descending correlation\n","        sorted_users = filtered_users.sort_values(ascending=False)\n","\n","        # Append users above the threshold to the result\n","        result[user_index] = sorted_users.index.tolist()\n","\n","    return result\n","\n","# Assuming `similarities_hourly_cal` is your cosine similarity matrix for the entire dataset\n","most_corr_users_hourly_cal = most_correlated_users(similarities_hourly_cal)\n","most_corr_users_half_hourly_cal = most_correlated_users(similarities_half_hourly_cal)\n","\n","# Similarly for JPL data\n","most_corr_users_hourly_JPL = most_correlated_users(similarities_hourly_JPL)\n","most_corr_users_half_hourly_JPL = most_correlated_users(similarities_half_hourly_JPL)"]},{"cell_type":"markdown","metadata":{"id":"QIHn62MOuwqz"},"source":["#Rabdom Forest Stay Duration (no correlations - individual user's data **only**)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":957,"status":"ok","timestamp":1700101551639,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"_Dta-nKctzVj"},"outputs":[],"source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import train_test_split, GridSearchCV, LeaveOneOut\n","from sklearn.metrics import mean_squared_error\n","import numpy as np\n","\n","def random_forest_regression(train_data,test_data,user_id, n_splits=5):\n","\n","    # Filter training and testing data for the specific user\n","    user_train_data = train_data[train_data[:, 3] == user_id]\n","    user_test_data = test_data[test_data[:, 3] == user_id]\n","\n","    # Extract columns for arrival time and stay duration for both train and test sets\n","    X_train = user_train_data[:, 0].reshape(-1, 1)  # Arrival time\n","    y_train = user_train_data[:, 4] # Stay duration\n","\n","    X_test = user_test_data[:, 0].reshape(-1, 1)\n","    y_test = user_test_data[:, 4]\n","\n","    # Define the hyperparameters and their possible values\n","    param_grid = {\n","        'n_estimators': [10, 20,50],\n","        'max_depth': [2,5,7,10,12],\n","        'min_samples_split':[2,3,5,7,10] ,\n","        'max_features': [1.0, 'sqrt']\n","    }\n","\n","    # Initialize RandomForestRegressor and GridSearchCV\n","    rf = RandomForestRegressor(random_state=42)\n","    grid_search = GridSearchCV(rf, param_grid, cv=n_splits, scoring='neg_mean_squared_error')\n","\n","    # Fit the model\n","    grid_search.fit(X_train, y_train)\n","\n","    # Predict using the best model\n","    best_rf = grid_search.best_estimator_\n","    y_pred = best_rf.predict(X_test)\n","\n","    # Calculate the performance metric (mean squared error)\n","    mse = mean_squared_error(y_test, y_pred)\n","\n","    # Calculate user SMAPE\n","    n = len(y_test)\n","    smape_val = (1/ n) * np.sum(np.abs(y_test - y_pred) / (np.abs(y_test+y_pred)))*100\n","\n","    return grid_search.best_params_,smape_val\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1590861,"status":"ok","timestamp":1700103172572,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"uxP2njavuRcQ"},"outputs":[],"source":["# Test the function\n","user_ids_caltech = np.unique(np.concatenate((caltech_train[:, 3], caltech_test[:, 3])))\n","user_ids_JPL = np.unique(np.concatenate((JPL_train[:, 3], JPL_test[:, 3])))\n","\n","smape_list_caltech=[]\n","smape_list_JPL=[]\n","best_params_caltech=[]\n","best_params_JPL=[]\n","for user_id in user_ids_caltech:\n","    best_params,smape = random_forest_regression(caltech_train, caltech_test, user_id)\n","    smape_list_caltech.append(smape)\n","    best_params_caltech.append(best_params)\n","for user_id in user_ids_JPL:\n","    best_params, smape = random_forest_regression(JPL_train, JPL_test, user_id)\n","    smape_list_JPL.append(smape)\n","    best_params_JPL.append(best_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1700103172573,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"UeMPq9qkzqY2","outputId":"f376fbc6-8760-4515-896b-84ccd3176661"},"outputs":[],"source":["#Calculate average SMAPE for each location\n","no_caltech_users=len(user_ids_caltech)\n","caltech_smape=sum(smape_list_caltech)/no_caltech_users\n","\n","no_JPL_users=len(user_ids_JPL)\n","JPL_smape=sum(smape_list_JPL)/no_JPL_users\n","\n","print(f\"Caltech SMAPE: {caltech_smape}\")\n","print(f\"JPL SMAPE: {JPL_smape}\")"]},{"cell_type":"markdown","metadata":{"id":"PIiUPuWobjtE"},"source":["#Calculate Random Forest models for all users"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1540979,"status":"ok","timestamp":1700104715444,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"Ob7jhhfzcCdO"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","\n","def train_user_model(train_data, user_id,n_splits=5):\n","    user_train_data = train_data[train_data[:, 3] == user_id]\n","    X_train = user_train_data[:, 0].reshape(-1, 1)  # Arrival time\n","    y_train = user_train_data[:, 4]  # Stay duration\n","\n","    # Define the hyperparameters and their possible values\n","    param_grid = {\n","        'n_estimators': [10, 20,50],\n","        'max_depth': [2,5,7,10,12],\n","        'min_samples_split':[2,3,5,7,10] ,\n","        'max_features': [1.0, 'sqrt']\n","    }\n","\n","    # Initialize RandomForestRegressor and GridSearchCV\n","    rf = RandomForestRegressor(random_state=42)\n","    grid_search = GridSearchCV(rf, param_grid, cv=n_splits, scoring='neg_mean_squared_error')\n","\n","    # Fit the model\n","    grid_search.fit(X_train, y_train)\n","\n","    # Predict using the best model\n","    model = grid_search.best_estimator_\n","\n","\n","    return model\n","\n","all_user_models_caltech = {user_id: train_user_model(caltech_train, user_id) for user_id in user_ids_caltech}\n","all_user_models_JPL = {user_id: train_user_model(JPL_train, user_id) for user_id in user_ids_JPL}"]},{"cell_type":"markdown","metadata":{"id":"Jl4DFIy2MtrB"},"source":["#Random Forest Stay Duration including most correlated users"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1700104715446,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"FKL74Nbubkuv"},"outputs":[],"source":["def train_and_test_user_model_with_correlation(train_data, test_data, user_id, most_corr_users, all_user_models, similarity_matrix,n_splits=5):\n","    # Filter training and testing data for the specific user\n","    user_train_data = train_data[train_data[:, 3] == user_id]\n","    user_test_data = test_data[test_data[:, 3] == user_id]\n","\n","    # Extract columns for arrival time and stay duration for both train and test sets\n","    X_train = user_train_data[:, 0].reshape(-1, 1)  # Arrival time\n","    y_train = user_train_data[:, 4]  # Stay duration\n","\n","    X_test = user_test_data[:, 0].reshape(-1, 1)\n","    y_test = user_test_data[:, 4]\n","\n","    # Define the hyperparameters and their possible values\n","    param_grid = {\n","        'n_estimators': [10, 20,50],\n","        'max_depth': [2,5,7,10,12],\n","        'min_samples_split':[2,3,5,7,10] ,\n","        'max_features': [1.0, 'sqrt']\n","    }\n","\n","    # Initialize RandomForestRegressor and GridSearchCV\n","    rf = RandomForestRegressor(random_state=42)\n","    grid_search = GridSearchCV(rf, param_grid, cv=n_splits, scoring='neg_mean_squared_error')\n","\n","    # Fit the model\n","    grid_search.fit(X_train, y_train)\n","\n","    user_model = grid_search.best_estimator_\n","    # Predict on the test set\n","    y_pred = user_model.predict(X_test)\n","\n","    # Adjust the prediction based on the most correlated users\n","    if user_id in most_corr_users:\n","      correlated_users = most_corr_users[user_id]\n","      total_correlation = 1\n","\n","    for other_user_id in correlated_users:\n","        if other_user_id in all_user_models:\n","            correlation = similarity_matrix.loc[user_id, other_user_id]\n","            other_user_model = all_user_models[other_user_id]\n","\n","            # Predict using the other user's model\n","            other_user_pred = other_user_model.predict(X_test)\n","\n","            # Weight the prediction by the correlation and add to the base prediction\n","            y_pred += other_user_pred * correlation\n","            total_correlation += correlation\n","    if total_correlation != 0:\n","      y_pred /= total_correlation\n","\n","    # Calculate user SMAPE\n","    n = len(y_test)\n","    smape_val = (1/ n) * np.sum(np.abs(y_test - y_pred) / (np.abs(y_test+y_pred)))*100\n","\n","\n","    return user_model, smape_val,total_correlation\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1339886,"status":"ok","timestamp":1700111271409,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"fepqJRoPbzdq"},"outputs":[],"source":["user_ids_caltech = np.unique(np.concatenate((caltech_train[:, 3], caltech_test[:, 3])))\n","user_ids_JPL = np.unique(np.concatenate((JPL_train[:, 3], JPL_test[:, 3])))\n","\n","smape_list_caltech=[]\n","smape_list_JPL=[]\n","sum_of_correlations_list=[]\n","# for user_id in user_ids_caltech:\n","#     model, smape,sum_of_correlations = train_and_test_user_model_with_correlation(caltech_train, caltech_test, user_id, most_corr_users_half_hourly_cal,all_user_models_caltech,similarities_half_hourly_cal)\n","#     smape_list_caltech.append(smape)\n","#     sum_of_correlations_list.append(sum_of_correlations)\n","for user_id in user_ids_JPL:\n","    model, smape,sum_of_correlations = train_and_test_user_model_with_correlation(JPL_train, JPL_test, user_id, most_corr_users_hourly_JPL,all_user_models_JPL,similarities_hourly_JPL)\n","    smape_list_JPL.append(smape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1700111271412,"user":{"displayName":"George Iacovides","userId":"10140267727913163690"},"user_tz":300},"id":"cIcBop98ebYB","outputId":"29d51377-c143-4789-89f6-3c23e6b42417"},"outputs":[],"source":["#Calculate average SMAPE for each location\n","no_caltech_users=len(user_ids_caltech)\n","caltech_smape=sum(smape_list_caltech)/no_caltech_users\n","\n","no_JPL_users=len(user_ids_JPL)\n","JPL_smape=sum(smape_list_JPL)/no_JPL_users\n","\n","print(f\"Caltech SMAPE: {caltech_smape}\")\n","print(f\"JPL SMAPE: {JPL_smape}\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPspu8TKg9n1ZeVPsvu4Jhk","provenance":[{"file_id":"1AQAcH3foS8kFBaI9CBrDIRW-C_qmNsYR","timestamp":1700101441417},{"file_id":"1_zSSH0LCUppl_Is9UdX79AayTAr5WjAa","timestamp":1700069210316},{"file_id":"1RehB3ik2AdRZku3yRoWVbsnv6vZDFuUw","timestamp":1700064728380},{"file_id":"19aOvXXPl1Rucdu0Z88HqTTP8RmiM_ipp","timestamp":1700022651684},{"file_id":"1cHdtNhA0ZofPXHdaPZsmHLX2gZDj3dJu","timestamp":1700006144358},{"file_id":"1M7ugd_F2UMQ796HlUfsXUl_Ucqj4nf54","timestamp":1697595791414}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
